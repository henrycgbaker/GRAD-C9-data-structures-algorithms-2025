\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Emergent Behaviour: Grey Walter's Tortoises}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The Machina Speculatrix}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Implications for Complex Systems}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Scrum: Harnessing Complexity}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Historical Context}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Scrum Principles}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}The Agile Manifesto}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Twelve Principles}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}The Scrum Framework}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Scrum Roles}{6}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Product Owner}{6}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Scrum Master}{7}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Development Team}{7}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Scrum Events}{7}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}The Sprint}{8}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Sprint Planning}{8}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Daily Scrum}{8}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Sprint Review}{9}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}Sprint Retrospective}{9}{subsubsection.4.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Scrum Artefacts}{10}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Product Backlog}{10}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Sprint Backlog}{10}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Product Increment}{10}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Why This Matters for Data Structures and Algorithms}{11}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Algorithms Exist in Context}{11}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Iterative Problem-Solving}{11}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Managing Complexity}{12}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Introduction to Computing}{13}{section.6}\protected@file@percent }
\newlabel{ch:intro-computing}{{6}{13}{Introduction to Computing}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}What is a Computer?}{13}{section.7}\protected@file@percent }
\newlabel{sec:what-is-computer}{{7}{13}{What is a Computer?}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Bits: The Fundamental Unit}{13}{subsection.7.1}\protected@file@percent }
\newlabel{subsec:bits}{{7.1}{13}{Bits: The Fundamental Unit}{subsection.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Logic Gates}{13}{subsection.7.2}\protected@file@percent }
\newlabel{subsec:logic-gates}{{7.2}{13}{Logic Gates}{subsection.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Truth tables for fundamental logic gates. A useful mnemonic: read ``A is 1, \textit  {[gate name]} B is 1'' to determine when the output is 1. For AND: ``A is 1 \textit  {and} B is 1''; for OR: ``A is 1 \textit  {or} B is 1''.}}{14}{figure.1}\protected@file@percent }
\newlabel{fig:truth-tables}{{1}{14}{Truth tables for fundamental logic gates. A useful mnemonic: read ``A is 1, \textit {[gate name]} B is 1'' to determine when the output is 1. For AND: ``A is 1 \textit {and} B is 1''; for OR: ``A is 1 \textit {or} B is 1''}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}The NAND Gate: A Universal Building Block}{14}{subsubsection.7.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The NAND gate symbol and truth table. NAND outputs 0 only when both inputs are 1-it is the negation of AND.}}{14}{figure.2}\protected@file@percent }
\newlabel{fig:nand-gate}{{2}{14}{The NAND gate symbol and truth table. NAND outputs 0 only when both inputs are 1-it is the negation of AND}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Hexadecimal Notation}{15}{section.8}\protected@file@percent }
\newlabel{sec:hexadecimal}{{8}{15}{Hexadecimal Notation}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Conversion Reference}{15}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Conversion Procedures}{16}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Hexadecimal to Binary}{16}{subsubsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Binary to Hexadecimal}{16}{subsubsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3}Hexadecimal to Decimal}{16}{subsubsection.8.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Memory}{16}{section.9}\protected@file@percent }
\newlabel{sec:memory}{{9}{16}{Memory}{section.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Flip-Flops: The Basis of Memory}{16}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Random Access Memory (RAM)}{17}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Memory Units and Addressing}{17}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Memory Addressing}{18}{subsection.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Data Representation}{18}{section.10}\protected@file@percent }
\newlabel{sec:representation}{{10}{18}{Data Representation}{section.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Text Representation}{18}{subsection.10.1}\protected@file@percent }
\newlabel{subsec:text-representation}{{10.1}{18}{Text Representation}{subsection.10.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.1}ASCII (American Standard Code for Information Interchange)}{18}{subsubsection.10.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.2}Unicode}{19}{subsubsection.10.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Integer Representation}{19}{subsection.10.2}\protected@file@percent }
\newlabel{subsec:integer-representation}{{10.2}{19}{Integer Representation}{subsection.10.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.1}Unsigned Binary Integers}{19}{subsubsection.10.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.2}Two's Complement for Signed Integers}{20}{subsubsection.10.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.2.3}Excess-$K$ (Biased) Notation}{21}{subsubsection.10.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Binary Arithmetic}{22}{section.11}\protected@file@percent }
\newlabel{sec:binary-arithmetic}{{11}{22}{Binary Arithmetic}{section.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Binary Addition}{22}{subsection.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Binary Subtraction via Two's Complement}{22}{subsection.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Overflow}{23}{subsection.11.3}\protected@file@percent }
\newlabel{subsec:overflow}{{11.3}{23}{Overflow}{subsection.11.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Overflow occurs when the result of an arithmetic operation exceeds the representable range. When adding two 8-bit numbers that produce a 9-bit result, the leftmost bit is truncated, leading to an incorrect answer within the 8-bit representation.}}{23}{figure.3}\protected@file@percent }
\newlabel{fig:overflow}{{3}{23}{Overflow occurs when the result of an arithmetic operation exceeds the representable range. When adding two 8-bit numbers that produce a 9-bit result, the leftmost bit is truncated, leading to an incorrect answer within the 8-bit representation}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12}Floating-Point Representation}{24}{section.12}\protected@file@percent }
\newlabel{sec:floating-point}{{12}{24}{Floating-Point Representation}{section.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}The Concept of Floating Point}{24}{subsection.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}8-Bit Floating Point (Pedagogical Example)}{25}{subsection.12.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Structure of an 8-bit floating-point format: 1 sign bit, 3 exponent bits (using excess-4 notation), and 4 mantissa bits. Real-world formats use more bits but follow the same principles.}}{25}{figure.4}\protected@file@percent }
\newlabel{fig:float-structure}{{4}{25}{Structure of an 8-bit floating-point format: 1 sign bit, 3 exponent bits (using excess-4 notation), and 4 mantissa bits. Real-world formats use more bits but follow the same principles}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.1}Normalised Form and the Hidden Bit}{25}{subsubsection.12.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.2}Conversion: Binary Floating Point to Decimal}{25}{subsubsection.12.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Examples of converting 8-bit floating-point representations to decimal values. The process involves extracting the sign, decoding the exponent (subtracting the bias), and reconstructing the mantissa with the implicit leading 1.}}{25}{figure.5}\protected@file@percent }
\newlabel{fig:float-examples}{{5}{25}{Examples of converting 8-bit floating-point representations to decimal values. The process involves extracting the sign, decoding the exponent (subtracting the bias), and reconstructing the mantissa with the implicit leading 1}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.3}Conversion: Decimal to Binary Floating Point}{26}{subsubsection.12.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}IEEE 754 Standard}{27}{subsection.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4}Floating-Point Precision Issues}{28}{subsection.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}CPU Architecture}{28}{section.13}\protected@file@percent }
\newlabel{sec:cpu}{{13}{28}{CPU Architecture}{section.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Simplified CPU architecture showing the main components: the Arithmetic Logic Unit (ALU), Control Unit, registers, and their connection to main memory via the system bus.}}{29}{figure.6}\protected@file@percent }
\newlabel{fig:cpu}{{6}{29}{Simplified CPU architecture showing the main components: the Arithmetic Logic Unit (ALU), Control Unit, registers, and their connection to main memory via the system bus}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}CPU Components}{29}{subsection.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.1}Arithmetic Logic Unit (ALU)}{29}{subsubsection.13.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.2}Control Unit}{29}{subsubsection.13.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.3}Registers}{30}{subsubsection.13.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.4}Memory Hierarchy}{30}{subsubsection.13.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}The Stored-Program Concept}{30}{subsection.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}The Machine Cycle}{31}{subsection.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Relevance to Data Structures and Algorithms}{31}{section.14}\protected@file@percent }
\newlabel{sec:relevance}{{14}{31}{Relevance to Data Structures and Algorithms}{section.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15}Summary}{32}{section.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16}Common Data Structures}{33}{section.16}\protected@file@percent }
\newlabel{ch:common-data-structures}{{16}{33}{Common Data Structures}{section.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17}Functions and Computability}{33}{section.17}\protected@file@percent }
\newlabel{sec:functions}{{17}{33}{Functions and Computability}{section.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {18}Turing Machines}{33}{section.18}\protected@file@percent }
\newlabel{sec:turing-machine}{{18}{33}{Turing Machines}{section.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Schematic of a Turing machine. The machine reads and writes symbols on an infinite tape, moves left or right, and transitions between states according to a finite set of rules.}}{34}{figure.7}\protected@file@percent }
\newlabel{fig:turing-machine}{{7}{34}{Schematic of a Turing machine. The machine reads and writes symbols on an infinite tape, moves left or right, and transitions between states according to a finite set of rules}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.1}The Church--Turing Thesis}{35}{subsection.18.1}\protected@file@percent }
\newlabel{subsec:church-turing}{{18.1}{35}{The Church--Turing Thesis}{subsection.18.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2}Computable Functions}{35}{subsection.18.2}\protected@file@percent }
\newlabel{subsec:computable-functions}{{18.2}{35}{Computable Functions}{subsection.18.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.3}Turing Completeness}{36}{subsection.18.3}\protected@file@percent }
\newlabel{subsec:turing-completeness}{{18.3}{36}{Turing Completeness}{subsection.18.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {19}The Halting Problem}{37}{section.19}\protected@file@percent }
\newlabel{sec:halting-problem}{{19}{37}{The Halting Problem}{section.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.1}Proof of Undecidability via Self-Reference}{38}{subsection.19.1}\protected@file@percent }
\newlabel{subsec:halting-proof}{{19.1}{38}{Proof of Undecidability via Self-Reference}{subsection.19.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {20}Data Types}{40}{section.20}\protected@file@percent }
\newlabel{sec:data-types}{{20}{40}{Data Types}{section.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1}Primitive Data Types}{40}{subsection.20.1}\protected@file@percent }
\newlabel{subsec:primitive-types}{{20.1}{40}{Primitive Data Types}{subsection.20.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.2}Lists}{41}{subsection.20.2}\protected@file@percent }
\newlabel{subsec:lists}{{20.2}{41}{Lists}{subsection.20.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.3}Arrays}{41}{subsection.20.3}\protected@file@percent }
\newlabel{subsec:arrays}{{20.3}{41}{Arrays}{subsection.20.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A 2D array with shape $(3, 4)$-3 rows and 4 columns. The values 1 through 12 are stored contiguously in memory, with metadata tracking the shape.}}{42}{figure.8}\protected@file@percent }
\newlabel{fig:array}{{8}{42}{A 2D array with shape $(3, 4)$-3 rows and 4 columns. The values 1 through 12 are stored contiguously in memory, with metadata tracking the shape}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.4}Hash Tables}{42}{subsection.20.4}\protected@file@percent }
\newlabel{subsec:hash-tables}{{20.4}{42}{Hash Tables}{subsection.20.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {21}Stacks and Queues}{43}{section.21}\protected@file@percent }
\newlabel{sec:stacks-queues}{{21}{43}{Stacks and Queues}{section.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1}The Stack}{43}{subsection.21.1}\protected@file@percent }
\newlabel{subsec:stack}{{21.1}{43}{The Stack}{subsection.21.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.2}Queues}{44}{subsection.21.2}\protected@file@percent }
\newlabel{subsec:queues}{{21.2}{44}{Queues}{subsection.21.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {22}Trees}{45}{section.22}\protected@file@percent }
\newlabel{sec:trees}{{22}{45}{Trees}{section.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {23}Storage and Memory}{46}{section.23}\protected@file@percent }
\newlabel{sec:storage}{{23}{46}{Storage and Memory}{section.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.1}Pointers}{46}{subsection.23.1}\protected@file@percent }
\newlabel{subsec:pointers}{{23.1}{46}{Pointers}{subsection.23.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.2}Garbage Collection}{47}{subsection.23.2}\protected@file@percent }
\newlabel{subsec:garbage-collection}{{23.2}{47}{Garbage Collection}{subsection.23.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.3}Passing by Value vs.\ Reference}{47}{subsection.23.3}\protected@file@percent }
\newlabel{subsec:pass-by-value-reference}{{23.3}{47}{Passing by Value vs.\ Reference}{subsection.23.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.4}Memory Storage Strategies}{49}{subsection.23.4}\protected@file@percent }
\newlabel{subsec:memory-storage}{{23.4}{49}{Memory Storage Strategies}{subsection.23.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {23.4.1}Contiguous Storage (Arrays)}{49}{subsubsection.23.4.1}\protected@file@percent }
\newlabel{subsubsec:contiguous-storage}{{23.4.1}{49}{Contiguous Storage (Arrays)}{subsubsection.23.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Contiguous storage: elements occupy consecutive memory addresses. Knowing the base address and element size, any element can be accessed directly by computing its address.}}{49}{figure.9}\protected@file@percent }
\newlabel{fig:contiguous-storage}{{9}{49}{Contiguous storage: elements occupy consecutive memory addresses. Knowing the base address and element size, any element can be accessed directly by computing its address}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {23.4.2}Linked Storage (Linked Lists)}{50}{subsubsection.23.4.2}\protected@file@percent }
\newlabel{subsubsec:linked-storage}{{23.4.2}{50}{Linked Storage (Linked Lists)}{subsubsection.23.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Linked storage: each node contains data and a pointer to the next node. Nodes can be anywhere in memory; the pointers connect them logically.}}{50}{figure.10}\protected@file@percent }
\newlabel{fig:linked-storage-intro}{{10}{50}{Linked storage: each node contains data and a pointer to the next node. Nodes can be anywhere in memory; the pointers connect them logically}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Insertion and deletion in a linked list. To remove a node, redirect the previous node's pointer to skip it. To insert, create a new node and update two pointers.}}{50}{figure.11}\protected@file@percent }
\newlabel{fig:linked-list-operations}{{11}{50}{Insertion and deletion in a linked list. To remove a node, redirect the previous node's pointer to skip it. To insert, create a new node and update two pointers}{figure.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {23.4.3}Storing Stacks}{51}{subsubsection.23.4.3}\protected@file@percent }
\newlabel{subsubsec:storing-stacks}{{23.4.3}{51}{Storing Stacks}{subsubsection.23.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Stack storage using an array. The stack pointer indicates the top of the stack. Push increments the pointer and writes; pop reads and decrements.}}{51}{figure.12}\protected@file@percent }
\newlabel{fig:stack-storage}{{12}{51}{Stack storage using an array. The stack pointer indicates the top of the stack. Push increments the pointer and writes; pop reads and decrements}{figure.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {23.4.4}Storing Queues}{52}{subsubsection.23.4.4}\protected@file@percent }
\newlabel{subsubsec:storing-queues}{{23.4.4}{52}{Storing Queues}{subsubsection.23.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Circular queue implementation. The array wraps around: after the last position comes the first. Head and tail pointers track the front and back of the queue.}}{52}{figure.13}\protected@file@percent }
\newlabel{fig:queue-storage}{{13}{52}{Circular queue implementation. The array wraps around: after the last position comes the first. Head and tail pointers track the front and back of the queue}{figure.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {24}Summary}{53}{section.24}\protected@file@percent }
\newlabel{sec:week3-summary}{{24}{53}{Summary}{section.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {25}Programming Paradigms}{54}{section.25}\protected@file@percent }
\newlabel{ch:programming-paradigms}{{25}{54}{Programming Paradigms}{section.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {26}Low-Level to High-Level Abstractions}{54}{section.26}\protected@file@percent }
\newlabel{sec:abstractions}{{26}{54}{Low-Level to High-Level Abstractions}{section.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {26.1}Assembly: Unitary Operations}{54}{subsection.26.1}\protected@file@percent }
\newlabel{subsec:assembler}{{26.1}{54}{Assembly: Unitary Operations}{subsection.26.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Assembly language example. Each line represents a single machine instruction: moving data between registers, performing arithmetic, or controlling program flow. The cryptic mnemonics (\texttt  {MOV}, \texttt  {ADD}, \texttt  {JMP}) map directly to processor operations.}}{54}{figure.14}\protected@file@percent }
\newlabel{fig:assembler}{{14}{54}{Assembly language example. Each line represents a single machine instruction: moving data between registers, performing arithmetic, or controlling program flow. The cryptic mnemonics (\texttt {MOV}, \texttt {ADD}, \texttt {JMP}) map directly to processor operations}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {26.2}Primitives: High-Level Instructions}{55}{subsection.26.2}\protected@file@percent }
\newlabel{subsec:primitives}{{26.2}{55}{Primitives: High-Level Instructions}{subsection.26.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {26.3}Compilers: Translation to Machine Code}{55}{subsection.26.3}\protected@file@percent }
\newlabel{subsec:compiler}{{26.3}{55}{Compilers: Translation to Machine Code}{subsection.26.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {26.4}Interpreters: Direct Execution}{56}{subsection.26.4}\protected@file@percent }
\newlabel{subsec:interpreter}{{26.4}{56}{Interpreters: Direct Execution}{subsection.26.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {27}Four Programming Paradigms}{57}{section.27}\protected@file@percent }
\newlabel{sec:paradigms}{{27}{57}{Four Programming Paradigms}{section.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The four major programming paradigms. Imperative programming specifies step-by-step instructions. Declarative programming describes the desired result. Functional programming builds programs from composable functions. Object-oriented programming models systems as interacting objects.}}{57}{figure.15}\protected@file@percent }
\newlabel{fig:paradigms}{{15}{57}{The four major programming paradigms. Imperative programming specifies step-by-step instructions. Declarative programming describes the desired result. Functional programming builds programs from composable functions. Object-oriented programming models systems as interacting objects}{figure.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {27.1}Imperative Programming}{57}{subsection.27.1}\protected@file@percent }
\newlabel{subsec:imperative}{{27.1}{57}{Imperative Programming}{subsection.27.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Fortran 77 code example. Originally written on punch cards, Fortran pioneered structured imperative programming. The numbered lines and explicit \texttt  {GOTO} statements reflect the step-by-step nature of early imperative code. Remarkably, much high-performance numerical computing still relies on Fortran libraries today.}}{58}{figure.16}\protected@file@percent }
\newlabel{fig:fortran}{{16}{58}{Fortran 77 code example. Originally written on punch cards, Fortran pioneered structured imperative programming. The numbered lines and explicit \texttt {GOTO} statements reflect the step-by-step nature of early imperative code. Remarkably, much high-performance numerical computing still relies on Fortran libraries today}{figure.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {27.2}Declarative Programming}{58}{subsection.27.2}\protected@file@percent }
\newlabel{subsec:declarative}{{27.2}{58}{Declarative Programming}{subsection.27.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces SQL query example. The query specifies desired columns, tables, join conditions, and filters-but says nothing about which indexes to use, how to order the joins, or how to scan the tables. The database engine makes these decisions.}}{59}{figure.17}\protected@file@percent }
\newlabel{fig:sql}{{17}{59}{SQL query example. The query specifies desired columns, tables, join conditions, and filters-but says nothing about which indexes to use, how to order the joins, or how to scan the tables. The database engine makes these decisions}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces dplyr example. The pipe operator (\texttt  {\%>\%}) chains declarative verbs: \texttt  {filter}, \texttt  {select}, \texttt  {mutate}, \texttt  {summarise}. You describe the transformation; dplyr handles execution.}}{59}{figure.18}\protected@file@percent }
\newlabel{fig:dplyr}{{18}{59}{dplyr example. The pipe operator (\texttt {\%>\%}) chains declarative verbs: \texttt {filter}, \texttt {select}, \texttt {mutate}, \texttt {summarise}. You describe the transformation; dplyr handles execution}{figure.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {27.3}Functional Programming}{59}{subsection.27.3}\protected@file@percent }
\newlabel{subsec:functional}{{27.3}{59}{Functional Programming}{subsection.27.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Functional approach to computing a balance. The expression \texttt  {(Find\_diff (Find\_sum Old\_balance Credits) (Find\_sum Debits))} composes three functions. Data flows through the composition: credits and debits are summed separately, then differenced with the old balance.}}{60}{figure.19}\protected@file@percent }
\newlabel{fig:functional}{{19}{60}{Functional approach to computing a balance. The expression \texttt {(Find\_diff (Find\_sum Old\_balance Credits) (Find\_sum Debits))} composes three functions. Data flows through the composition: credits and debits are summed separately, then differenced with the old balance}{figure.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Imperative approach to the same calculation. Variables are initialised, then mutated through loops. The state changes at each step, and the final result depends on the sequence of mutations.}}{60}{figure.20}\protected@file@percent }
\newlabel{fig:imperative}{{20}{60}{Imperative approach to the same calculation. Variables are initialised, then mutated through loops. The state changes at each step, and the final result depends on the sequence of mutations}{figure.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {27.3.1}Pure Functions}{60}{subsubsection.27.3.1}\protected@file@percent }
\newlabel{subsubsec:pure-functions}{{27.3.1}{60}{Pure Functions}{subsubsection.27.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {27.3.2}Loops vs Recursion}{61}{subsubsection.27.3.2}\protected@file@percent }
\newlabel{subsubsec:loops-recursion}{{27.3.2}{61}{Loops vs Recursion}{subsubsection.27.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {27.3.3}Anonymous and Higher-Order Functions}{62}{subsubsection.27.3.3}\protected@file@percent }
\newlabel{subsubsec:anonymous-functions}{{27.3.3}{62}{Anonymous and Higher-Order Functions}{subsubsection.27.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {27.4}Object-Oriented Programming}{63}{subsection.27.4}\protected@file@percent }
\newlabel{subsec:oop}{{27.4}{63}{Object-Oriented Programming}{subsection.27.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {27.4.1}Classes vs Objects}{63}{subsubsection.27.4.1}\protected@file@percent }
\newlabel{subsubsec:classes-objects}{{27.4.1}{63}{Classes vs Objects}{subsubsection.27.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {27.4.2}Classes and Methods in Python}{64}{subsubsection.27.4.2}\protected@file@percent }
\newlabel{subsubsec:python-classes}{{27.4.2}{64}{Classes and Methods in Python}{subsubsection.27.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {27.4.3}Core Properties of OOP}{65}{subsubsection.27.4.3}\protected@file@percent }
\newlabel{subsubsec:oop-properties}{{27.4.3}{65}{Core Properties of OOP}{subsubsection.27.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {28}Software Development Practices}{66}{section.28}\protected@file@percent }
\newlabel{sec:software-practices}{{28}{66}{Software Development Practices}{section.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {28.1}Literate Programming}{66}{subsection.28.1}\protected@file@percent }
\newlabel{subsec:literate-programming}{{28.1}{66}{Literate Programming}{subsection.28.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {28.2}Test-Driven Development (TDD)}{66}{subsection.28.2}\protected@file@percent }
\newlabel{subsec:tdd}{{28.2}{66}{Test-Driven Development (TDD)}{subsection.28.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces The TDD cycle: Red-Green-Refactor. First write a failing test (Red), then write minimal code to pass the test (Green), then improve the code while keeping tests passing (Refactor). This cycle repeats for each new piece of functionality.}}{67}{figure.21}\protected@file@percent }
\newlabel{fig:tdd}{{21}{67}{The TDD cycle: Red-Green-Refactor. First write a failing test (Red), then write minimal code to pass the test (Green), then improve the code while keeping tests passing (Refactor). This cycle repeats for each new piece of functionality}{figure.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces The testing pyramid. \textbf  {Unit tests} (base): Test individual functions or methods in isolation-fast, numerous, and focused. \textbf  {Integration tests} (middle): Test that multiple units work together correctly-do outputs of one module align with inputs of another? \textbf  {End-to-end (E2E) tests} (top): Test the complete application as a user would experience it-slow but comprehensive.}}{68}{figure.22}\protected@file@percent }
\newlabel{fig:testing-pyramid}{{22}{68}{The testing pyramid. \textbf {Unit tests} (base): Test individual functions or methods in isolation-fast, numerous, and focused. \textbf {Integration tests} (middle): Test that multiple units work together correctly-do outputs of one module align with inputs of another? \textbf {End-to-end (E2E) tests} (top): Test the complete application as a user would experience it-slow but comprehensive}{figure.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {28.3}Coupling}{68}{subsection.28.3}\protected@file@percent }
\newlabel{subsec:coupling}{{28.3}{68}{Coupling}{subsection.28.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.3.1}Data Coupling (Loosest)}{69}{subsubsection.28.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Data coupling: modules share only the data necessary for the task. Module A passes specific values to Module B, which processes them and returns a result. Neither module needs to know about the other's internal structure.}}{69}{figure.23}\protected@file@percent }
\newlabel{fig:data-coupling}{{23}{69}{Data coupling: modules share only the data necessary for the task. Module A passes specific values to Module B, which processes them and returns a result. Neither module needs to know about the other's internal structure}{figure.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.3.2}Stamp Coupling}{69}{subsubsection.28.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.3.3}Control Coupling}{70}{subsubsection.28.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Control coupling: Module A passes a control flag that determines how Module B behaves. Module B must interpret the flag and branch accordingly, creating tighter interdependency than pure data coupling.}}{70}{figure.24}\protected@file@percent }
\newlabel{fig:control-coupling}{{24}{70}{Control coupling: Module A passes a control flag that determines how Module B behaves. Module B must interpret the flag and branch accordingly, creating tighter interdependency than pure data coupling}{figure.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.3.4}Common Coupling}{70}{subsubsection.28.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.3.5}Content Coupling (Tightest)}{71}{subsubsection.28.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {28.4}Cohesion}{71}{subsection.28.4}\protected@file@percent }
\newlabel{subsec:cohesion}{{28.4}{71}{Cohesion}{subsection.28.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Cohesion spectrum from low (coincidental) to high (functional). Higher cohesion means the module's elements are more closely related in purpose. Aim for the right side of this spectrum.}}{72}{figure.25}\protected@file@percent }
\newlabel{fig:cohesion}{{25}{72}{Cohesion spectrum from low (coincidental) to high (functional). Higher cohesion means the module's elements are more closely related in purpose. Aim for the right side of this spectrum}{figure.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.4.1}Coincidental Cohesion (Lowest)}{72}{subsubsection.28.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.4.2}Logical Cohesion}{73}{subsubsection.28.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.4.3}Temporal Cohesion}{73}{subsubsection.28.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.4.4}Procedural Cohesion}{73}{subsubsection.28.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.4.5}Communicational Cohesion}{73}{subsubsection.28.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.4.6}Sequential Cohesion}{74}{subsubsection.28.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {28.4.7}Functional Cohesion (Highest)}{74}{subsubsection.28.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {28.5}Information Hiding}{74}{subsection.28.5}\protected@file@percent }
\newlabel{subsec:information-hiding}{{28.5}{74}{Information Hiding}{subsection.28.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {29}Connecting Paradigms to Data Structures and Algorithms}{76}{section.29}\protected@file@percent }
\newlabel{sec:paradigms-dsa}{{29}{76}{Connecting Paradigms to Data Structures and Algorithms}{section.29}{}}
\@writefile{toc}{\contentsline {section}{\numberline {30}Analysis I: Correctness \& Efficiency}{77}{section.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {30.1}Constant Time Operations}{77}{subsection.30.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {30.1.1}Arrays: Fast Random Access}{78}{subsubsection.30.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Array indexing: computing the memory address of element $i$ requires only adding the index to the base address-a constant-time operation regardless of array size.}}{78}{figure.26}\protected@file@percent }
\newlabel{fig:array-indexing}{{26}{78}{Array indexing: computing the memory address of element $i$ requires only adding the index to the base address-a constant-time operation regardless of array size}{figure.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {30.1.2}Queues: Fast Head/Tail Operations}{79}{subsubsection.30.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Queue operations: enqueue adds to the tail, dequeue removes from the head. Both operations only interact with the endpoints via pointers, making them $O(1)$ regardless of queue length.}}{79}{figure.27}\protected@file@percent }
\newlabel{fig:queue-operations}{{27}{79}{Queue operations: enqueue adds to the tail, dequeue removes from the head. Both operations only interact with the endpoints via pointers, making them $O(1)$ regardless of queue length}{figure.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {31}Analysis II: Big O Notation}{80}{section.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {31.1}Why We Drop Constants}{80}{subsection.31.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {31.2}Choosing the Dominant Term}{81}{subsection.31.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {31.3}Common Families of Complexity}{81}{subsection.31.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {31.3.1}Constant Time - $O(1)$}{81}{subsubsection.31.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {31.3.2}Logarithmic Time - $O(\log n)$}{81}{subsubsection.31.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {31.3.3}Linear Time - $O(n)$}{82}{subsubsection.31.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {31.3.4}Quadratic Time - $O(n^2)$}{82}{subsubsection.31.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {31.3.5}Polynomial Time - $O(n^k)$}{82}{subsubsection.31.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {31.3.6}Exponential Time - $O(r^n)$}{82}{subsubsection.31.3.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Growth rates of common complexity families. Note how polynomial functions ($n$, $n^2$, $n^3$) grow much more slowly than exponential functions ($2^n$) as $n$ increases. The vertical axis uses a logarithmic scale to fit all curves; even so, exponential growth quickly dominates.}}{83}{figure.28}\protected@file@percent }
\newlabel{fig:complexity-families}{{28}{83}{Growth rates of common complexity families. Note how polynomial functions ($n$, $n^2$, $n^3$) grow much more slowly than exponential functions ($2^n$) as $n$ increases. The vertical axis uses a logarithmic scale to fit all curves; even so, exponential growth quickly dominates}{figure.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {32}Stable Matching Problem}{83}{section.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {32.1}Problem Setup}{83}{subsection.32.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {32.2}Worked Example}{84}{subsection.32.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Student preference lists: each row shows one student's ranking of hospitals from most preferred (left) to least preferred (right).}}{84}{figure.29}\protected@file@percent }
\newlabel{fig:student-preferences}{{29}{84}{Student preference lists: each row shows one student's ranking of hospitals from most preferred (left) to least preferred (right)}{figure.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Hospital preference lists: each row shows one hospital's ranking of students from most preferred (left) to least preferred (right).}}{84}{figure.30}\protected@file@percent }
\newlabel{fig:hospital-preferences}{{30}{84}{Hospital preference lists: each row shows one hospital's ranking of students from most preferred (left) to least preferred (right)}{figure.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces An \textbf  {unstable} matching: Xi'an would prefer Barry over Charlotte, and Barry would prefer Xi'an over York. Both would benefit from switching-this is an unstable pair.}}{84}{figure.31}\protected@file@percent }
\newlabel{fig:unstable-example}{{31}{84}{An \textbf {unstable} matching: Xi'an would prefer Barry over Charlotte, and Barry would prefer Xi'an over York. Both would benefit from switching-this is an unstable pair}{figure.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces A \textbf  {stable} matching: no student-hospital pair would mutually prefer to switch. For any potential pair, at least one party prefers their current assignment.}}{85}{figure.32}\protected@file@percent }
\newlabel{fig:stable-example}{{32}{85}{A \textbf {stable} matching: no student-hospital pair would mutually prefer to switch. For any potential pair, at least one party prefers their current assignment}{figure.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {33}Stable Roommate Problem: A Cautionary Example}{85}{section.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {33.1}Problem Setup}{85}{subsection.33.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces The stable roommate problem with 4 people: no stable pairing exists. For example, if we pair (A--B) and (C--D), then B and C form an unstable pair. Every possible pairing has at least one unstable pair-see slides for all permutations.}}{85}{figure.33}\protected@file@percent }
\newlabel{fig:roommate-problem}{{33}{85}{The stable roommate problem with 4 people: no stable pairing exists. For example, if we pair (A--B) and (C--D), then B and C form an unstable pair. Every possible pairing has at least one unstable pair-see slides for all permutations}{figure.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {34}Gale-Shapley Algorithm}{85}{section.34}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Gale-Shapley Algorithm}}{86}{algocf.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {34.1}Correctness Analysis}{86}{subsection.34.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {34.1.1}Does the Algorithm Halt?}{86}{subsubsection.34.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {34.1.2}Does Everyone Get Matched?}{87}{subsubsection.34.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {34.1.3}Are All Matches Stable?}{87}{subsubsection.34.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {34.2}Efficient Implementation}{88}{subsection.34.2}\protected@file@percent }
\newlabel{sec:efficient-implementation}{{34.2}{88}{Efficient Implementation}{subsection.34.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {34.2.1}The Challenge: Comparing Preferences}{88}{subsubsection.34.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {34.2.2}The Solution: Inverse Preference Tables}{88}{subsubsection.34.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Inverse preference table: for each hospital, we store each student's rank directly. Now comparing preferences is $O(1)$: just compare $\text  {inverse}[S]$ vs $\text  {inverse}[S']$.}}{88}{figure.34}\protected@file@percent }
\newlabel{fig:inverse-preference}{{34}{88}{Inverse preference table: for each hospital, we store each student's rank directly. Now comparing preferences is $O(1)$: just compare $\text {inverse}[S]$ vs $\text {inverse}[S']$}{figure.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {34.2.3}Complete Data Structure Design}{89}{subsubsection.34.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Complete efficient implementation: combining queues for managing unmatched students with arrays for preference lookups. The key line ``if $H$ prefers $S$ to current student $S'$'' becomes a single $O(1)$ array comparison using the inverse preference table.}}{89}{figure.35}\protected@file@percent }
\newlabel{fig:complete-implementation}{{35}{89}{Complete efficient implementation: combining queues for managing unmatched students with arrays for preference lookups. The key line ``if $H$ prefers $S$ to current student $S'$'' becomes a single $O(1)$ array comparison using the inverse preference table}{figure.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {34.3}Final Analysis}{90}{subsection.34.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {35}Properties of Big O}{91}{section.35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {36}Algorithm Analysis Guidelines}{92}{section.36}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {37}Case Study 1: Fibonacci Sequence}{92}{section.37}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces The Fibonacci sequence: each term is the sum of the two preceding terms. This simple recurrence relation appears throughout mathematics, computer science, and nature.}}{92}{figure.36}\protected@file@percent }
\newlabel{fig:fibonacci-sequence}{{36}{92}{The Fibonacci sequence: each term is the sum of the two preceding terms. This simple recurrence relation appears throughout mathematics, computer science, and nature}{figure.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {37.1}Naive Approach: Direct Recursion}{93}{subsection.37.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Naive Recursive Fibonacci}}{93}{algocf.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {37.1.1}Analysing Recursion via Recursion Trees}{93}{subsubsection.37.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Recursion tree for computing $F(n)$. Each node represents a function call; children represent the recursive calls made. The tree grows exponentially wide, and the same values (e.g., $F(2)$, $F(3)$) are computed many times at different positions.}}{93}{figure.37}\protected@file@percent }
\newlabel{fig:recursion-tree}{{37}{93}{Recursion tree for computing $F(n)$. Each node represents a function call; children represent the recursive calls made. The tree grows exponentially wide, and the same values (e.g., $F(2)$, $F(3)$) are computed many times at different positions}{figure.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {37.2}Improvement 1: Memoisation (Caching Values)}{94}{subsection.37.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Memoisation visualised: instead of recomputing values, we store them in an array. Each Fibonacci number is computed exactly once and retrieved from the cache thereafter. The tree effectively collapses to a linear chain.}}{94}{figure.38}\protected@file@percent }
\newlabel{fig:fib-memoisation}{{38}{94}{Memoisation visualised: instead of recomputing values, we store them in an array. Each Fibonacci number is computed exactly once and retrieved from the cache thereafter. The tree effectively collapses to a linear chain}{figure.38}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Iterative Fibonacci with Memoisation}}{95}{algocf.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {37.3}Improvement 2: Constant Space}{96}{subsection.37.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Space-Optimised Iterative Fibonacci}}{96}{algocf.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {38}Case Study 2: Binary Search}{97}{section.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {38.1}Naive Approach: Linear Search}{97}{subsection.38.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces A sorted array. Linear search would check each element from left to right until finding the target-potentially examining all $n$ elements.}}{97}{figure.39}\protected@file@percent }
\newlabel{fig:sorted-array}{{39}{97}{A sorted array. Linear search would check each element from left to right until finding the target-potentially examining all $n$ elements}{figure.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {38.2}Binary Search: Divide and Conquer}{97}{subsection.38.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Binary search setup: identify the lowest index (\texttt  {lo}), highest index (\texttt  {hi}), and middle index (\texttt  {mid}). We compare the target against the element at \texttt  {mid}.}}{97}{figure.40}\protected@file@percent }
\newlabel{fig:binary-search-setup}{{40}{97}{Binary search setup: identify the lowest index (\texttt {lo}), highest index (\texttt {hi}), and middle index (\texttt {mid}). We compare the target against the element at \texttt {mid}}{figure.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces First comparison: target 33 is greater than middle element 27, so we eliminate the left half and search only the right portion.}}{98}{figure.41}\protected@file@percent }
\newlabel{fig:bin-search-step1}{{41}{98}{First comparison: target 33 is greater than middle element 27, so we eliminate the left half and search only the right portion}{figure.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces Second comparison: update \texttt  {lo} to just past the old middle. The search space is now half its original size.}}{98}{figure.42}\protected@file@percent }
\newlabel{fig:bin-search-step2}{{42}{98}{Second comparison: update \texttt {lo} to just past the old middle. The search space is now half its original size}{figure.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces Find the new middle of the reduced search space. Each iteration halves the remaining elements.}}{98}{figure.43}\protected@file@percent }
\newlabel{fig:bin-search-step3}{{43}{98}{Find the new middle of the reduced search space. Each iteration halves the remaining elements}{figure.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces Third comparison: target 33 is less than middle element 43, so we eliminate the right half and search only the left portion.}}{98}{figure.44}\protected@file@percent }
\newlabel{fig:bin-search-step4}{{44}{98}{Third comparison: target 33 is less than middle element 43, so we eliminate the right half and search only the left portion}{figure.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {45}{\ignorespaces Final state: all three pointers (\texttt  {lo}, \texttt  {mid}, \texttt  {hi}) converge on the same element. If this element equals the target, we've found it; otherwise, the target is not in the array.}}{98}{figure.45}\protected@file@percent }
\newlabel{fig:bin-search-final}{{45}{98}{Final state: all three pointers (\texttt {lo}, \texttt {mid}, \texttt {hi}) converge on the same element. If this element equals the target, we've found it; otherwise, the target is not in the array}{figure.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {38.2.1}Algorithm}{99}{subsubsection.38.2.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Binary Search Algorithm}}{99}{algocf.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {38.2.2}Correctness}{99}{subsubsection.38.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {38.2.3}Efficiency}{100}{subsubsection.38.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {38.2.4}Applications of Binary Search}{100}{subsubsection.38.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {39}Case Study 3: Sorting}{101}{section.39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces The sorting problem: given an unordered array (top), produce a sorted array (bottom) with elements arranged from least to greatest.}}{101}{figure.46}\protected@file@percent }
\newlabel{fig:sorting-problem}{{46}{101}{The sorting problem: given an unordered array (top), produce a sorted array (bottom) with elements arranged from least to greatest}{figure.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {39.1}Naive Approach: BogoSort}{101}{subsection.39.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {47}{\ignorespaces BogoSort: randomly shuffle the array and check if it's sorted. This is extremely unlikely to produce the correct order quickly.}}{101}{figure.47}\protected@file@percent }
\newlabel{fig:bogosort}{{47}{101}{BogoSort: randomly shuffle the array and check if it's sorted. This is extremely unlikely to produce the correct order quickly}{figure.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {39.2}MergeSort: Divide and Conquer}{102}{subsection.39.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {48}{\ignorespaces Merging two sorted lists: maintain pointers to the current element of each list. At each step, compare the two current elements and append the smaller to the output. The key insight is that we only ever compare ``current'' elements-we never need to look back.}}{103}{figure.48}\protected@file@percent }
\newlabel{fig:merge-operation}{{48}{103}{Merging two sorted lists: maintain pointers to the current element of each list. At each step, compare the two current elements and append the smaller to the output. The key insight is that we only ever compare ``current'' elements-we never need to look back}{figure.48}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {39.2.1}MergeSort Algorithm}{103}{subsubsection.39.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {49}{\ignorespaces First pair: compare M and E. Since E $<$ M alphabetically, swap to get (E, M).}}{104}{figure.49}\protected@file@percent }
\newlabel{fig:merge-step1}{{49}{104}{First pair: compare M and E. Since E $<$ M alphabetically, swap to get (E, M)}{figure.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {50}{\ignorespaces Result of first pair comparison: E and M are now in sorted order.}}{104}{figure.50}\protected@file@percent }
\newlabel{fig:merge-step2}{{50}{104}{Result of first pair comparison: E and M are now in sorted order}{figure.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {51}{\ignorespaces Second pair: compare R and G. Since G $<$ R, swap to get (G, R).}}{104}{figure.51}\protected@file@percent }
\newlabel{fig:merge-step3}{{51}{104}{Second pair: compare R and G. Since G $<$ R, swap to get (G, R)}{figure.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {52}{\ignorespaces Result: both pairs in the first half are now sorted.}}{104}{figure.52}\protected@file@percent }
\newlabel{fig:merge-step4}{{52}{104}{Result: both pairs in the first half are now sorted}{figure.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {53}{\ignorespaces Merge (E, M) and (G, R) into (E, G, M, R). We only compare current elements of each list-this is why merging is efficient.}}{104}{figure.53}\protected@file@percent }
\newlabel{fig:merge-step5}{{53}{104}{Merge (E, M) and (G, R) into (E, G, M, R). We only compare current elements of each list-this is why merging is efficient}{figure.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {54}{\ignorespaces Sort the next pair: compare S and E to get (E, S).}}{104}{figure.54}\protected@file@percent }
\newlabel{fig:merge-step6}{{54}{104}{Sort the next pair: compare S and E to get (E, S)}{figure.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {55}{\ignorespaces Sort another pair: compare O and R to get (O, R).}}{104}{figure.55}\protected@file@percent }
\newlabel{fig:merge-step7}{{55}{104}{Sort another pair: compare O and R to get (O, R)}{figure.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {56}{\ignorespaces Merge (E, S) and (O, R) into (E, O, R, S).}}{104}{figure.56}\protected@file@percent }
\newlabel{fig:merge-step8}{{56}{104}{Merge (E, S) and (O, R) into (E, O, R, S)}{figure.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {57}{\ignorespaces Merge (E, G, M, R) and (E, O, R, S) into (E, E, G, M, O, R, R, S)-the first half is now fully sorted.}}{104}{figure.57}\protected@file@percent }
\newlabel{fig:merge-step9}{{57}{104}{Merge (E, G, M, R) and (E, O, R, S) into (E, E, G, M, O, R, R, S)-the first half is now fully sorted}{figure.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {58}{\ignorespaces Begin sorting the second half of the original array.}}{104}{figure.58}\protected@file@percent }
\newlabel{fig:merge-step10}{{58}{104}{Begin sorting the second half of the original array}{figure.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {59}{\ignorespaces Sort pairs in the second half.}}{105}{figure.59}\protected@file@percent }
\newlabel{fig:merge-step11}{{59}{105}{Sort pairs in the second half}{figure.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {60}{\ignorespaces Merge sorted pairs into quadruples.}}{105}{figure.60}\protected@file@percent }
\newlabel{fig:merge-step12}{{60}{105}{Merge sorted pairs into quadruples}{figure.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {61}{\ignorespaces Continue merging in the second half.}}{105}{figure.61}\protected@file@percent }
\newlabel{fig:merge-step13}{{61}{105}{Continue merging in the second half}{figure.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {62}{\ignorespaces Merge quadruples in the second half.}}{105}{figure.62}\protected@file@percent }
\newlabel{fig:merge-step14}{{62}{105}{Merge quadruples in the second half}{figure.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {63}{\ignorespaces Both halves are now sorted. One final merge remains.}}{105}{figure.63}\protected@file@percent }
\newlabel{fig:merge-step15}{{63}{105}{Both halves are now sorted. One final merge remains}{figure.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {64}{\ignorespaces Performing the final merge operation.}}{105}{figure.64}\protected@file@percent }
\newlabel{fig:merge-step16}{{64}{105}{Performing the final merge operation}{figure.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {65}{\ignorespaces Final result: the entire array is now sorted.}}{105}{figure.65}\protected@file@percent }
\newlabel{fig:merge-step17}{{65}{105}{Final result: the entire array is now sorted}{figure.65}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {39.2.2}Correctness}{105}{subsubsection.39.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {39.2.3}Efficiency}{106}{subsubsection.39.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {40}Case Study 4: Sieve of Eratosthenes}{106}{section.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {40.1}Naive Approach: Trial Division}{106}{subsection.40.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {40.2}Sieve of Eratosthenes}{107}{subsection.40.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {66}{\ignorespaces Prime numbers plotted in polar coordinates with radius $= p$ and angle $= p$ radians. The emerging spiral patterns reveal deep structure in the distribution of primes. The Sieve of Eratosthenes exploits a different structural property: multiples of primes.}}{107}{figure.66}\protected@file@percent }
\newlabel{fig:prime-structure}{{66}{107}{Prime numbers plotted in polar coordinates with radius $= p$ and angle $= p$ radians. The emerging spiral patterns reveal deep structure in the distribution of primes. The Sieve of Eratosthenes exploits a different structural property: multiples of primes}{figure.66}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {40.2.1}Algorithm}{107}{subsubsection.40.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {67}{\ignorespaces The Sieve of Eratosthenes in action (see \texttt  {https://upload.wikimedia.org/wikipedia/commons/9/94/Animation\_Sieve\_of\_Eratosth.gif} for an animated version). Starting from 2, we mark all its multiples. Then find 3 (unmarked), mark its multiples. Then 5, 7, etc. Numbers remaining unmarked are prime.}}{108}{figure.67}\protected@file@percent }
\newlabel{fig:sieve-animation}{{67}{108}{The Sieve of Eratosthenes in action (see \texttt {https://upload.wikimedia.org/wikipedia/commons/9/94/Animation\_Sieve\_of\_Eratosth.gif} for an animated version). Starting from 2, we mark all its multiples. Then find 3 (unmarked), mark its multiples. Then 5, 7, etc. Numbers remaining unmarked are prime}{figure.67}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {40.2.2}Correctness}{108}{subsubsection.40.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {40.2.3}Efficiency}{109}{subsubsection.40.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {41}Summary: The Art of Algorithm Design}{109}{section.41}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {42}Local vs Global Minima}{111}{section.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {42.1}Local Minimum}{111}{subsection.42.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {42.2}Global Minimum}{111}{subsection.42.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {68}{\ignorespaces Local versus global minima: a function may have multiple local minima, but only one global minimum (the lowest point overall).}}{112}{figure.68}\protected@file@percent }
\newlabel{fig:local-vs-global}{{68}{112}{Local versus global minima: a function may have multiple local minima, but only one global minimum (the lowest point overall)}{figure.68}{}}
\@writefile{toc}{\contentsline {section}{\numberline {43}Characterising a Local Minimum}{112}{section.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {43.1}One-Dimensional Case}{112}{subsection.43.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {43.2}High-Dimensional Case}{113}{subsection.43.2}\protected@file@percent }
\newlabel{sec:high-d-minima}{{43.2}{113}{High-Dimensional Case}{subsection.43.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {69}{\ignorespaces A saddle point is not a local minimum: the Hessian is indefinite. While the function curves upward in the $x$-direction, it curves downward in the $y$-direction; moving along $y$ would reduce the loss.}}{114}{figure.69}\protected@file@percent }
\newlabel{fig:saddle-point-1}{{69}{114}{A saddle point is not a local minimum: the Hessian is indefinite. While the function curves upward in the $x$-direction, it curves downward in the $y$-direction; moving along $y$ would reduce the loss}{figure.69}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {43.3}Quadratic Forms and Definiteness}{114}{subsection.43.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {70}{\ignorespaces Another view of a saddle point: $H$ is indefinite. While the surface curves upward in one direction, any move along the perpendicular direction would further reduce the loss function.}}{115}{figure.70}\protected@file@percent }
\newlabel{fig:saddle-point-2}{{70}{115}{Another view of a saddle point: $H$ is indefinite. While the surface curves upward in one direction, any move along the perpendicular direction would further reduce the loss function}{figure.70}{}}
\@writefile{toc}{\contentsline {section}{\numberline {44}Taylor Expansion and the Logic of Local Minima}{116}{section.44}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {44.1}Taylor Expansion of a Function}{116}{subsection.44.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {44.2}At a Critical Point}{116}{subsection.44.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {44.3}Condition for a Minimum}{117}{subsection.44.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {44.4}Positive Definiteness as Characterisation}{117}{subsection.44.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {45}Convexity}{117}{section.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {45.1}Formal Definition}{117}{subsection.45.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {71}{\ignorespaces Left: convex function (line segment lies above curve). Right: non-convex function (the function crosses the line segment).}}{118}{figure.71}\protected@file@percent }
\newlabel{fig:convexity}{{71}{118}{Left: convex function (line segment lies above curve). Right: non-convex function (the function crosses the line segment)}{figure.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {45.2}Why Convexity Matters}{118}{subsection.45.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {45.3}Local vs Global Minima: The Role of PSD Hessians}{118}{subsection.45.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {45.4}Finding Global Minima of Non-Convex Functions}{119}{subsection.45.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {45.5}OLS as a Concrete Example}{119}{subsection.45.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {45.5.1}OLS Loss Function}{119}{subsubsection.45.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {45.5.2}Gradient of OLS Loss}{119}{subsubsection.45.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {45.5.3}Hessian of OLS Loss}{120}{subsubsection.45.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {45.5.4}Convexity of OLS}{120}{subsubsection.45.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {46}First-Order Methods}{121}{section.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {46.1}General Procedure}{121}{subsection.46.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {46.2}Descent Direction}{121}{subsection.46.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.2.1}Deriving the Descent Direction via Taylor Expansion}{121}{subsubsection.46.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.2.2}Optimal Direction}{122}{subsubsection.46.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.2.3}Gradient Descent Update Rule}{122}{subsubsection.46.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {46.3}Step Size / Learning Rate}{122}{subsection.46.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.3.1}Example 1a: $f(x) = x^2$}{122}{subsubsection.46.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {72}{\ignorespaces Gradient descent on $f(x)=x^2$, iteration 1: starting at $x=4$ with $\eta =0.2$. The algorithm computes the gradient and takes a step towards the minimum.}}{123}{figure.72}\protected@file@percent }
\newlabel{fig:gd-iter1}{{72}{123}{Gradient descent on $f(x)=x^2$, iteration 1: starting at $x=4$ with $\eta =0.2$. The algorithm computes the gradient and takes a step towards the minimum}{figure.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {73}{\ignorespaces Iteration 4: continued progress towards the minimum at $x=0$.}}{123}{figure.73}\protected@file@percent }
\newlabel{fig:gd-iter4}{{73}{123}{Iteration 4: continued progress towards the minimum at $x=0$}{figure.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {74}{\ignorespaces Iteration 10: the algorithm has nearly converged to the minimum. The learning rate is small enough to avoid overshooting.}}{123}{figure.74}\protected@file@percent }
\newlabel{fig:gd-iter10}{{74}{123}{Iteration 10: the algorithm has nearly converged to the minimum. The learning rate is small enough to avoid overshooting}{figure.74}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.3.2}Example 1b: $f(x) = x^2$ with Large Learning Rate}{123}{subsubsection.46.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {75}{\ignorespaces Large learning rate ($\eta =1.0$), iteration 0: starting position.}}{124}{figure.75}\protected@file@percent }
\newlabel{fig:gd-large-iter0}{{75}{124}{Large learning rate ($\eta =1.0$), iteration 0: starting position}{figure.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {76}{\ignorespaces Iteration 1: the algorithm overshoots to the opposite side of the minimum.}}{124}{figure.76}\protected@file@percent }
\newlabel{fig:gd-large-iter1}{{76}{124}{Iteration 1: the algorithm overshoots to the opposite side of the minimum}{figure.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {77}{\ignorespaces Iteration 2: oscillating back. With $\eta =1.0$, the algorithm will never converge - it ping-pongs between $x=4$ and $x=-4$ indefinitely.}}{124}{figure.77}\protected@file@percent }
\newlabel{fig:gd-large-iter2}{{77}{124}{Iteration 2: oscillating back. With $\eta =1.0$, the algorithm will never converge - it ping-pongs between $x=4$ and $x=-4$ indefinitely}{figure.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {78}{\ignorespaces Learning curves showing loss vs iteration. We want \textbf  {smooth descent}. With a high learning rate (orange), the loss plateaus as the algorithm oscillates without making progress.}}{125}{figure.78}\protected@file@percent }
\newlabel{fig:learning-curve}{{78}{125}{Learning curves showing loss vs iteration. We want \textbf {smooth descent}. With a high learning rate (orange), the loss plateaus as the algorithm oscillates without making progress}{figure.78}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.3.3}Example 2a: Non-Convex Cubic}{125}{subsubsection.46.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {79}{\ignorespaces Cubic function, early iterations: starting at $x=2$.}}{125}{figure.79}\protected@file@percent }
\newlabel{fig:cubic-1}{{79}{125}{Cubic function, early iterations: starting at $x=2$}{figure.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {80}{\ignorespaces The algorithm descends into the local valley.}}{126}{figure.80}\protected@file@percent }
\newlabel{fig:cubic-2}{{80}{126}{The algorithm descends into the local valley}{figure.80}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {81}{\ignorespaces Approaching the local minimum.}}{126}{figure.81}\protected@file@percent }
\newlabel{fig:cubic-3}{{81}{126}{Approaching the local minimum}{figure.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {82}{\ignorespaces Converged to local minimum. The algorithm does \textbf  {not} find the global minimum (at $-\infty $) because the learning rate is too small to escape the local valley.}}{126}{figure.82}\protected@file@percent }
\newlabel{fig:cubic-4}{{82}{126}{Converged to local minimum. The algorithm does \textbf {not} find the global minimum (at $-\infty $) because the learning rate is too small to escape the local valley}{figure.82}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.3.4}Example 2b: Same Cubic, Different Starting Point}{126}{subsubsection.46.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {83}{\ignorespaces Starting at $x=-4$: on the left side of the local minimum.}}{127}{figure.83}\protected@file@percent }
\newlabel{fig:cubic-2b-1}{{83}{127}{Starting at $x=-4$: on the left side of the local minimum}{figure.83}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {84}{\ignorespaces The gradient points left (towards $-\infty $), so the algorithm moves that direction.}}{127}{figure.84}\protected@file@percent }
\newlabel{fig:cubic-2b-2}{{84}{127}{The gradient points left (towards $-\infty $), so the algorithm moves that direction}{figure.84}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {85}{\ignorespaces The algorithm continues towards $-\infty $, finding the global minimum. With the same $\eta $, the starting point determined which minimum was found.}}{127}{figure.85}\protected@file@percent }
\newlabel{fig:cubic-2b-3}{{85}{127}{The algorithm continues towards $-\infty $, finding the global minimum. With the same $\eta $, the starting point determined which minimum was found}{figure.85}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {46.4}Choosing a Constant Learning Rate}{128}{subsection.46.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {86}{\ignorespaces Smooth vs non-smooth functions: smoother functions allow larger step sizes without overshooting. The Lipschitz constant $L$ quantifies this smoothness.}}{128}{figure.86}\protected@file@percent }
\newlabel{fig:smoothness}{{86}{128}{Smooth vs non-smooth functions: smoother functions allow larger step sizes without overshooting. The Lipschitz constant $L$ quantifies this smoothness}{figure.86}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {46.5}Choosing a Variable Learning Rate}{128}{subsection.46.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.5.1}Exact Line Search (Steepest Descent)}{128}{subsubsection.46.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.5.2}Inexact Line Search}{129}{subsubsection.46.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.5.3}Convergence Rates and Condition Number}{130}{subsubsection.46.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {87}{\ignorespaces High condition number ($\kappa = 1000$): the bowl is elongated, causing inefficient zigzagging paths. Low condition number ($\kappa \approx 1$): the bowl is circular, allowing direct paths to the minimum.}}{130}{figure.87}\protected@file@percent }
\newlabel{fig:condition-number}{{87}{130}{High condition number ($\kappa = 1000$): the bowl is elongated, causing inefficient zigzagging paths. Low condition number ($\kappa \approx 1$): the bowl is circular, allowing direct paths to the minimum}{figure.87}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {46.6}Momentum}{130}{subsection.46.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.6.1}Momentum Update Rules}{131}{subsubsection.46.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {46.6.2}Understanding Momentum Accumulation}{131}{subsubsection.46.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {88}{\ignorespaces Momentum ``smooths out'' the optimisation path by averaging over past gradients, reducing oscillations and accelerating convergence in consistent directions.}}{131}{figure.88}\protected@file@percent }
\newlabel{fig:momentum}{{88}{131}{Momentum ``smooths out'' the optimisation path by averaging over past gradients, reducing oscillations and accelerating convergence in consistent directions}{figure.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {46.7}Nesterov Accelerated Momentum (NAG)}{131}{subsection.46.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {46.8}Summary: First-Order Methods}{132}{subsection.46.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {47}Second-Order Methods}{132}{section.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {47.1}Newton's Method}{132}{subsection.47.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {47.1.1}Algorithm}{133}{subsubsection.47.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {89}{\ignorespaces Newton's method iteration 1: the quadratic approximation (dashed) guides the step towards the minimum more directly than gradient descent.}}{133}{figure.89}\protected@file@percent }
\newlabel{fig:newton1}{{89}{133}{Newton's method iteration 1: the quadratic approximation (dashed) guides the step towards the minimum more directly than gradient descent}{figure.89}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {90}{\ignorespaces Newton's method iteration 2: for quadratic functions, Newton's method finds the exact minimum in one step.}}{133}{figure.90}\protected@file@percent }
\newlabel{fig:newton2}{{90}{133}{Newton's method iteration 2: for quadratic functions, Newton's method finds the exact minimum in one step}{figure.90}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {47.1.2}Derivation via Taylor Series}{134}{subsubsection.47.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {47.1.3}Newton's Method Applied to OLS}{134}{subsubsection.47.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {47.1.4}Difficulties with Newton's Method}{135}{subsubsection.47.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {47.2}BFGS (A Quasi-Newton Method)}{135}{subsection.47.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {47.2.1}Algorithm}{135}{subsubsection.47.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {47.2.2}Technical Conditions}{135}{subsubsection.47.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {47.2.3}Efficiency Improvements}{136}{subsubsection.47.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {47.3}Summary: Second-Order Methods}{136}{subsection.47.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {48}Stochastic Gradient Descent (SGD)}{137}{section.48}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {48.1}The Structure in Our Loss Functions}{137}{subsection.48.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {48.2}Relationship to Population Risk}{137}{subsection.48.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {48.3}SGD Algorithm}{138}{subsection.48.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {48.4}Practical Considerations}{138}{subsection.48.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {48.5}SGD Applied to OLS}{139}{subsection.48.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {91}{\ignorespaces LMS/SGD path: unlike gradient descent, individual steps may not always decrease the loss, but on average the algorithm progresses towards the minimum.}}{139}{figure.91}\protected@file@percent }
\newlabel{fig:lms}{{91}{139}{LMS/SGD path: unlike gradient descent, individual steps may not always decrease the loss, but on average the algorithm progresses towards the minimum}{figure.91}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {48.6}Choosing a Learning Rate for SGD}{139}{subsection.48.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {92}{\ignorespaces Learning rate effects: too large causes oscillation/divergence; too small causes slow convergence or getting stuck.}}{139}{figure.92}\protected@file@percent }
\newlabel{fig:lr-sgd}{{92}{139}{Learning rate effects: too large causes oscillation/divergence; too small causes slow convergence or getting stuck}{figure.92}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {48.6.1}Learning Rate Schedules}{140}{subsubsection.48.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {93}{\ignorespaces Learning rate schedules: various strategies for decreasing the learning rate over training iterations.}}{140}{figure.93}\protected@file@percent }
\newlabel{fig:lr-schedules}{{93}{140}{Learning rate schedules: various strategies for decreasing the learning rate over training iterations}{figure.93}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {48.6.2}Robbins--Monro Conditions}{140}{subsubsection.48.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {48.7}Advanced SGD Techniques}{141}{subsection.48.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {49}Summary}{141}{section.49}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {50}Divide and Conquer}{143}{section.50}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {51}Overview}{143}{section.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {51.1}Tools for Analysing Divide and Conquer Algorithms}{143}{subsection.51.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {51.2}Recurrence Relations}{144}{subsection.51.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {52}MergeSort}{144}{section.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {52.1}Motivation: Why Do We Sort?}{144}{subsection.52.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {52.2}The MergeSort Process}{145}{subsection.52.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {94}{\ignorespaces MergeSort recursively divides the array until reaching single elements, then merges sorted subarrays back together.}}{145}{figure.94}\protected@file@percent }
\newlabel{fig:mergesort-overview}{{94}{145}{MergeSort recursively divides the array until reaching single elements, then merges sorted subarrays back together}{figure.94}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {95}{\ignorespaces Merging two sorted halves: compare leading elements, take the smaller, and advance that pointer. The merge produces a sorted combined array.}}{146}{figure.95}\protected@file@percent }
\newlabel{fig:mergesort-merge}{{95}{146}{Merging two sorted halves: compare leading elements, take the smaller, and advance that pointer. The merge produces a sorted combined array}{figure.95}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {52.3}Implementation}{147}{subsection.52.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {96}{\ignorespaces MergeSort pseudocode showing the recursive structure: divide at the midpoint, recursively sort each half, then merge.}}{147}{figure.96}\protected@file@percent }
\newlabel{fig:mergesort-impl}{{96}{147}{MergeSort pseudocode showing the recursive structure: divide at the midpoint, recursively sort each half, then merge}{figure.96}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {52.4}Time Complexity Analysis}{148}{subsection.52.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {52.4.1}Recursion Tree Intuition}{148}{subsubsection.52.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {97}{\ignorespaces MergeSort recursion tree. Each level performs $O(n)$ total work (across all nodes at that level). With $\log n$ levels, the total work is $O(n \log n)$.}}{148}{figure.97}\protected@file@percent }
\newlabel{fig:mergesort-tree}{{97}{148}{MergeSort recursion tree. Each level performs $O(n)$ total work (across all nodes at that level). With $\log n$ levels, the total work is $O(n \log n)$}{figure.97}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {52.4.2}Master Theorem Application}{149}{subsubsection.52.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {52.4.3}Formal Inductive Proof}{149}{subsubsection.52.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {53}The Sorting Lower Bound}{149}{section.53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {53.1}The Decision Tree Model}{150}{subsection.53.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {98}{\ignorespaces Decision tree for sorting three elements $\{a, b, c\}$. Each internal node represents a comparison; each leaf represents a permutation (sorted order). The tree must have at least $n! = 6$ leaves for $n = 3$ elements.}}{151}{figure.98}\protected@file@percent }
\newlabel{fig:decision-tree}{{98}{151}{Decision tree for sorting three elements $\{a, b, c\}$. Each internal node represents a comparison; each leaf represents a permutation (sorted order). The tree must have at least $n! = 6$ leaves for $n = 3$ elements}{figure.98}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {53.2}The Lower Bound Proof}{152}{subsection.53.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {99}{\ignorespaces The relationship between tree height, number of leaves, and the sorting lower bound. A binary tree of height $h$ has at most $2^h$ leaves, so $2^h \geq n!$ implies $h \geq \log _2(n!) = \Omega (n \log n)$.}}{152}{figure.99}\protected@file@percent }
\newlabel{fig:sorting-lower-bound}{{99}{152}{The relationship between tree height, number of leaves, and the sorting lower bound. A binary tree of height $h$ has at most $2^h$ leaves, so $2^h \geq n!$ implies $h \geq \log _2(n!) = \Omega (n \log n)$}{figure.99}{}}
\@writefile{toc}{\contentsline {section}{\numberline {54}Counting Inversions}{153}{section.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {54.1}Motivation and Applications}{153}{subsection.54.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {54.2}Naive Approach}{153}{subsection.54.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {54.3}Divide and Conquer Approach}{153}{subsection.54.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {100}{\ignorespaces Inversions fall into three categories: those within the left half, those within the right half, and cross-inversions between halves. Divide and conquer handles each category.}}{154}{figure.100}\protected@file@percent }
\newlabel{fig:inversion-types}{{100}{154}{Inversions fall into three categories: those within the left half, those within the right half, and cross-inversions between halves. Divide and conquer handles each category}{figure.100}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {54.4}Sort-and-Count Algorithm}{154}{subsection.54.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {101}{\ignorespaces With sorted halves, counting cross-inversions reduces to: when taking an element from the right half during merge, count how many elements remain in the left half (all form inversions with it).}}{154}{figure.101}\protected@file@percent }
\newlabel{fig:sort-count-idea}{{101}{154}{With sorted halves, counting cross-inversions reduces to: when taking an element from the right half during merge, count how many elements remain in the left half (all form inversions with it)}{figure.101}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {102}{\ignorespaces Merge-and-count in action: when $b_j < a_i$, all remaining elements in $A$ (from position $i$ onwards) form inversions with $b_j$. Here, when taking 11 from $B$, elements $\{a_i, 18\}$ remaining in $A$ each form an inversion.}}{155}{figure.102}\protected@file@percent }
\newlabel{fig:merge-count}{{102}{155}{Merge-and-count in action: when $b_j < a_i$, all remaining elements in $A$ (from position $i$ onwards) form inversions with $b_j$. Here, when taking 11 from $B$, elements $\{a_i, 18\}$ remaining in $A$ each form an inversion}{figure.102}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {54.5}Algorithm and Analysis}{155}{subsection.54.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {103}{\ignorespaces Sort-and-Count pseudocode. The algorithm returns both the inversion count and the sorted list, enabling the divide and conquer structure.}}{155}{figure.103}\protected@file@percent }
\newlabel{fig:sort-count-algo}{{103}{155}{Sort-and-Count pseudocode. The algorithm returns both the inversion count and the sorted list, enabling the divide and conquer structure}{figure.103}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {54.5.1}Worked Example}{156}{subsubsection.54.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {55}Selection: Finding the $k$th Smallest Element}{156}{section.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {55.1}Applications}{157}{subsection.55.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {55.2}Approaches and Their Complexities}{157}{subsection.55.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {104}{\ignorespaces A binary heap structure. In a max-heap, each parent is larger than its children. Heaps enable efficient extraction of extremal elements.}}{158}{figure.104}\protected@file@percent }
\newlabel{fig:binary-heap}{{104}{158}{A binary heap structure. In a max-heap, each parent is larger than its children. Heaps enable efficient extraction of extremal elements}{figure.104}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {55.3}3-Way Partitioning}{158}{subsection.55.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {105}{\ignorespaces 3-way partitioning: elements are rearranged so all elements less than pivot $p$ are on the left, elements equal to $p$ are in the middle, and elements greater than $p$ are on the right.}}{159}{figure.105}\protected@file@percent }
\newlabel{fig:3way-partition}{{105}{159}{3-way partitioning: elements are rearranged so all elements less than pivot $p$ are on the left, elements equal to $p$ are in the middle, and elements greater than $p$ are on the right}{figure.105}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {55.3.1}The Partitioning Algorithm}{159}{subsubsection.55.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {55.4}QuickSelect Algorithm}{159}{subsection.55.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {106}{\ignorespaces QuickSelect partitions around a random pivot, then recurses only into the partition containing position $k$. Unlike QuickSort, only one recursive call is made.}}{160}{figure.106}\protected@file@percent }
\newlabel{fig:quickselect}{{106}{160}{QuickSelect partitions around a random pivot, then recurses only into the partition containing position $k$. Unlike QuickSort, only one recursive call is made}{figure.106}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {55.5}Analysis of Randomised QuickSelect}{160}{subsection.55.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {55.5.1}Intuition: The Stick-Breaking Analogy}{160}{subsubsection.55.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {55.5.2}Recurrence and Solution}{160}{subsubsection.55.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {56}Closest Pair of Points}{161}{section.56}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {107}{\ignorespaces The closest pair problem in 2D: among all $\binom  {n}{2}$ pairs of points, find the pair with minimum Euclidean distance.}}{162}{figure.107}\protected@file@percent }
\newlabel{fig:closest-pair-problem}{{107}{162}{The closest pair problem in 2D: among all $\binom {n}{2}$ pairs of points, find the pair with minimum Euclidean distance}{figure.107}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {56.1}Brute Force Approach}{162}{subsection.56.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {56.2}Divide and Conquer Solution}{162}{subsection.56.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {108}{\ignorespaces Divide and conquer for closest pair: split points by a vertical line $L$, recursively find closest pairs in each half (distances 12 and 21), then check for closer pairs straddling the line (distance 8).}}{163}{figure.108}\protected@file@percent }
\newlabel{fig:closest-pair-dc}{{108}{163}{Divide and conquer for closest pair: split points by a vertical line $L$, recursively find closest pairs in each half (distances 12 and 21), then check for closer pairs straddling the line (distance 8)}{figure.108}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {56.3}Why Only 7 Neighbours?}{164}{subsection.56.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {109}{\ignorespaces In the combine step, we only consider points within distance $\delta $ of the dividing line. After sorting these by $y$-coordinate, each point needs comparison with at most 7 subsequent points.}}{164}{figure.109}\protected@file@percent }
\newlabel{fig:strip-check}{{109}{164}{In the combine step, we only consider points within distance $\delta $ of the dividing line. After sorting these by $y$-coordinate, each point needs comparison with at most 7 subsequent points}{figure.109}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {56.4}Complete Algorithm}{164}{subsection.56.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {110}{\ignorespaces Complete closest pair algorithm with complexity annotations. The sorting within the strip can be done in $O(n)$ per level by maintaining a separate list sorted by $y$-coordinate.}}{164}{figure.110}\protected@file@percent }
\newlabel{fig:closest-pair-algo}{{110}{164}{Complete closest pair algorithm with complexity annotations. The sorting within the strip can be done in $O(n)$ per level by maintaining a separate list sorted by $y$-coordinate}{figure.110}{}}
\@writefile{toc}{\contentsline {section}{\numberline {57}Computational Geometry Applications}{165}{section.57}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Complexity improvements from divide and conquer in computational geometry. The dramatic improvement for Delaunay/Voronoi from $O(n^4)$ to $O(n \log n)$ is particularly striking.}}{165}{table.1}\protected@file@percent }
\newlabel{table:geom-complexity}{{1}{165}{Complexity improvements from divide and conquer in computational geometry. The dramatic improvement for Delaunay/Voronoi from $O(n^4)$ to $O(n \log n)$ is particularly striking}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {111}{\ignorespaces Convex hull: the smallest convex polygon containing all points. Can be computed in $O(n \log n)$ using divide and conquer (e.g., the ``gift wrapping'' or Graham scan algorithms).}}{166}{figure.111}\protected@file@percent }
\newlabel{fig:convex-hull}{{111}{166}{Convex hull: the smallest convex polygon containing all points. Can be computed in $O(n \log n)$ using divide and conquer (e.g., the ``gift wrapping'' or Graham scan algorithms)}{figure.111}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {112}{\ignorespaces Farthest pair problem (computing the diameter): find the two points with maximum distance. The farthest pair always lies on the convex hull, enabling an $O(n \log n)$ algorithm.}}{166}{figure.112}\protected@file@percent }
\newlabel{fig:farthest-pair}{{112}{166}{Farthest pair problem (computing the diameter): find the two points with maximum distance. The farthest pair always lies on the convex hull, enabling an $O(n \log n)$ algorithm}{figure.112}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {113}{\ignorespaces Delaunay triangulation: connects points such that no point lies inside the circumcircle of any triangle. Used in mesh generation, terrain modelling, and as a dual to Voronoi diagrams.}}{166}{figure.113}\protected@file@percent }
\newlabel{fig:delaunay}{{113}{166}{Delaunay triangulation: connects points such that no point lies inside the circumcircle of any triangle. Used in mesh generation, terrain modelling, and as a dual to Voronoi diagrams}{figure.113}{}}
\@writefile{toc}{\contentsline {section}{\numberline {58}The Master Theorem: General Theory}{166}{section.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {58.1}The General Recurrence}{167}{subsection.58.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {114}{\ignorespaces Recursion tree for the general divide and conquer recurrence. The total work is $T(n) = n^c \DOTSB \sum@ \slimits@ _{i=0}^{\log _b n} r^i$ where $r = a/b^c$.}}{167}{figure.114}\protected@file@percent }
\newlabel{fig:master-tree}{{114}{167}{Recursion tree for the general divide and conquer recurrence. The total work is $T(n) = n^c \sum _{i=0}^{\log _b n} r^i$ where $r = a/b^c$}{figure.114}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {58.2}The Critical Ratio}{168}{subsection.58.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {58.3}The Three Cases}{168}{subsection.58.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {58.3.1}Case 1: Root Domination ($r < 1$)}{168}{subsubsection.58.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {58.3.2}Case 2: Equal Distribution ($r = 1$)}{169}{subsubsection.58.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {58.3.3}Case 3: Leaf Domination ($r > 1$)}{169}{subsubsection.58.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {59}Parallelisation of Divide and Conquer}{169}{section.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {59.1}Embarrassingly Parallel Problems}{170}{subsection.59.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {59.1.1}Example: Training a Random Forest}{170}{subsubsection.59.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {60}Matrix Multiplication}{170}{section.60}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {60.1}Standard Matrix Multiplication}{170}{subsection.60.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {115}{\ignorespaces Computing one entry of the product matrix: $C_{12}$ is the dot product of row 1 of $A$ with column 2 of $B$, requiring $n$ multiplications and $n-1$ additions.}}{171}{figure.115}\protected@file@percent }
\newlabel{fig:matrix-entry}{{115}{171}{Computing one entry of the product matrix: $C_{12}$ is the dot product of row 1 of $A$ with column 2 of $B$, requiring $n$ multiplications and $n-1$ additions}{figure.115}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {60.2}Block Matrix Multiplication}{171}{subsection.60.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {116}{\ignorespaces Block matrix multiplication: partition each matrix into four $(n/2) \times (n/2)$ blocks. The product can be computed using 8 recursive multiplications of half-sized matrices plus additions.}}{171}{figure.116}\protected@file@percent }
\newlabel{fig:block-mult}{{116}{171}{Block matrix multiplication: partition each matrix into four $(n/2) \times (n/2)$ blocks. The product can be computed using 8 recursive multiplications of half-sized matrices plus additions}{figure.116}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {60.3}Strassen's Algorithm}{172}{subsection.60.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {60.4}Significance of Matrix Multiplication Complexity}{173}{subsection.60.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {61}Summary: The Divide and Conquer Paradigm}{174}{section.61}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary of divide and conquer algorithms covered, their recurrences, complexities, and improvements over naive approaches.}}{174}{table.2}\protected@file@percent }
\newlabel{table:dc-summary}{{2}{174}{Summary of divide and conquer algorithms covered, their recurrences, complexities, and improvements over naive approaches}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {62}Graphs}{175}{section.62}\protected@file@percent }
\newlabel{ch:graphs}{{62}{175}{Graphs}{section.62}{}}
\@writefile{toc}{\contentsline {section}{\numberline {63}What is a Graph?}{175}{section.63}\protected@file@percent }
\newlabel{sec:graph-definition}{{63}{175}{What is a Graph?}{section.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {117}{\ignorespaces An undirected graph with 8 vertices and 11 edges.}}{175}{figure.117}\protected@file@percent }
\newlabel{fig:basic-graph}{{117}{175}{An undirected graph with 8 vertices and 11 edges}{figure.117}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {118}{\ignorespaces A more complex graph illustrating various connection patterns.}}{176}{figure.118}\protected@file@percent }
\newlabel{fig:complex-graph}{{118}{176}{A more complex graph illustrating various connection patterns}{figure.118}{}}
\@writefile{toc}{\contentsline {section}{\numberline {64}Representation of a Graph}{176}{section.64}\protected@file@percent }
\newlabel{sec:graph-representation}{{64}{176}{Representation of a Graph}{section.64}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Graph Types with Nodes and Edges - Real-World Examples}}{177}{table.3}\protected@file@percent }
\newlabel{tab:graph-examples}{{3}{177}{Graph Types with Nodes and Edges - Real-World Examples}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {64.1}Adjacency Matrix}{177}{subsection.64.1}\protected@file@percent }
\newlabel{subsec:adjacency-matrix}{{64.1}{177}{Adjacency Matrix}{subsection.64.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {64.2}Adjacency List}{178}{subsection.64.2}\protected@file@percent }
\newlabel{subsec:adjacency-list}{{64.2}{178}{Adjacency List}{subsection.64.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {119}{\ignorespaces Adjacency list representation of the graph from Figure~\ref {fig:basic-graph}. Each vertex maps to a list of its neighbours.}}{178}{figure.119}\protected@file@percent }
\newlabel{fig:adjacency-list}{{119}{178}{Adjacency list representation of the graph from Figure~\ref {fig:basic-graph}. Each vertex maps to a list of its neighbours}{figure.119}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {64.3}Choosing Between the Two}{179}{subsection.64.3}\protected@file@percent }
\newlabel{subsec:representation-choice}{{64.3}{179}{Choosing Between the Two}{subsection.64.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {65}Paths and Connectivity}{180}{section.65}\protected@file@percent }
\newlabel{sec:paths-connectivity}{{65}{180}{Paths and Connectivity}{section.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {65.1}Paths}{180}{subsection.65.1}\protected@file@percent }
\newlabel{subsec:paths}{{65.1}{180}{Paths}{subsection.65.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {120}{\ignorespaces A simple path through a graph - each vertex is visited at most once.}}{180}{figure.120}\protected@file@percent }
\newlabel{fig:simple-path}{{120}{180}{A simple path through a graph - each vertex is visited at most once}{figure.120}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {65.2}Connectivity}{180}{subsection.65.2}\protected@file@percent }
\newlabel{subsec:connectivity}{{65.2}{180}{Connectivity}{subsection.65.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {65.3}Cycles}{181}{subsection.65.3}\protected@file@percent }
\newlabel{subsec:cycles}{{65.3}{181}{Cycles}{subsection.65.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {121}{\ignorespaces A simple cycle - the path returns to its starting vertex without repeating any intermediate vertex.}}{181}{figure.121}\protected@file@percent }
\newlabel{fig:simple-cycle}{{121}{181}{A simple cycle - the path returns to its starting vertex without repeating any intermediate vertex}{figure.121}{}}
\@writefile{toc}{\contentsline {section}{\numberline {66}Trees}{181}{section.66}\protected@file@percent }
\newlabel{sec:graph-trees}{{66}{181}{Trees}{section.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {66.1}Properties of Trees}{182}{subsection.66.1}\protected@file@percent }
\newlabel{subsec:tree-properties}{{66.1}{182}{Properties of Trees}{subsection.66.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {122}{\ignorespaces A tree - an undirected, connected, acyclic graph.}}{182}{figure.122}\protected@file@percent }
\newlabel{fig:tree}{{122}{182}{A tree - an undirected, connected, acyclic graph}{figure.122}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {66.2}Types of Trees}{182}{subsection.66.2}\protected@file@percent }
\newlabel{subsec:tree-types}{{66.2}{182}{Types of Trees}{subsection.66.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {123}{\ignorespaces Rooted trees - the same tree structure with different vertices designated as the root.}}{183}{figure.123}\protected@file@percent }
\newlabel{fig:rooted-trees}{{123}{183}{Rooted trees - the same tree structure with different vertices designated as the root}{figure.123}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {124}{\ignorespaces GUI hierarchy as a rooted tree; this is how HTML hierarchy works - each element is a node with parent-child relationships.}}{183}{figure.124}\protected@file@percent }
\newlabel{fig:gui-tree}{{124}{183}{GUI hierarchy as a rooted tree; this is how HTML hierarchy works - each element is a node with parent-child relationships}{figure.124}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {125}{\ignorespaces Dewey Decimal system as a rooted tree - hierarchical classification of knowledge.}}{184}{figure.125}\protected@file@percent }
\newlabel{fig:dewey-tree}{{125}{184}{Dewey Decimal system as a rooted tree - hierarchical classification of knowledge}{figure.125}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {66.3}Applications of Trees}{184}{subsection.66.3}\protected@file@percent }
\newlabel{subsec:tree-applications}{{66.3}{184}{Applications of Trees}{subsection.66.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {67}$s$-$t$ Connectivity Problems}{184}{section.67}\protected@file@percent }
\newlabel{sec:st-connectivity}{{67}{184}{$s$-$t$ Connectivity Problems}{section.67}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {67.1}$s$-$t$ Connectivity Problem}{184}{subsection.67.1}\protected@file@percent }
\newlabel{subsec:st-connectivity-problem}{{67.1}{184}{$s$-$t$ Connectivity Problem}{subsection.67.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {67.2}$s$-$t$ Shortest Path Problem}{185}{subsection.67.2}\protected@file@percent }
\newlabel{subsec:st-shortest-path}{{67.2}{185}{$s$-$t$ Shortest Path Problem}{subsection.67.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {67.3}Applications of $s$-$t$ Connectivity Problems}{185}{subsection.67.3}\protected@file@percent }
\newlabel{subsec:st-applications}{{67.3}{185}{Applications of $s$-$t$ Connectivity Problems}{subsection.67.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {68}Breadth-First Search (BFS)}{186}{section.68}\protected@file@percent }
\newlabel{sec:bfs}{{68}{186}{Breadth-First Search (BFS)}{section.68}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {68.1}BFS Algorithm Overview}{186}{subsection.68.1}\protected@file@percent }
\newlabel{subsec:bfs-overview}{{68.1}{186}{BFS Algorithm Overview}{subsection.68.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {68.1.1}BFS Algorithm Steps}{186}{subsubsection.68.1.1}\protected@file@percent }
\newlabel{subsubsec:bfs-steps}{{68.1.1}{186}{BFS Algorithm Steps}{subsubsection.68.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {68.1.2}Level Sets in BFS}{186}{subsubsection.68.1.2}\protected@file@percent }
\newlabel{subsubsec:bfs-levels}{{68.1.2}{186}{Level Sets in BFS}{subsubsection.68.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {68.2}Properties and Benefits of BFS}{187}{subsection.68.2}\protected@file@percent }
\newlabel{subsec:bfs-properties}{{68.2}{187}{Properties and Benefits of BFS}{subsection.68.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {126}{\ignorespaces BFS exploration pattern - nodes are visited in order of their distance from the source.}}{187}{figure.126}\protected@file@percent }
\newlabel{fig:bfs-pattern}{{126}{187}{BFS exploration pattern - nodes are visited in order of their distance from the source}{figure.126}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {68.3}Analysis of BFS}{187}{subsection.68.3}\protected@file@percent }
\newlabel{subsec:bfs-analysis}{{68.3}{187}{Analysis of BFS}{subsection.68.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {127}{\ignorespaces BFS tree structure: the solid black lines show the BFS-discovered shortest paths between $s$ and other vertices; the dotted lines show other edges in the graph that connect vertices within or between adjacent levels.}}{188}{figure.127}\protected@file@percent }
\newlabel{fig:bfs-tree}{{127}{188}{BFS tree structure: the solid black lines show the BFS-discovered shortest paths between $s$ and other vertices; the dotted lines show other edges in the graph that connect vertices within or between adjacent levels}{figure.127}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {68.3.1}Time Complexity of BFS}{189}{subsubsection.68.3.1}\protected@file@percent }
\newlabel{subsubsec:bfs-complexity}{{68.3.1}{189}{Time Complexity of BFS}{subsubsection.68.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {68.4}Example Applications of BFS}{190}{subsection.68.4}\protected@file@percent }
\newlabel{subsec:bfs-applications}{{68.4}{190}{Example Applications of BFS}{subsection.68.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {68.4.1}Flood Fill}{190}{subsubsection.68.4.1}\protected@file@percent }
\newlabel{subsubsec:flood-fill}{{68.4.1}{190}{Flood Fill}{subsubsection.68.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {128}{\ignorespaces Flood fill algorithm - colouring a contiguous region. Each pixel is a node; edges connect adjacent pixels of the same colour.}}{190}{figure.128}\protected@file@percent }
\newlabel{fig:flood-fill}{{128}{190}{Flood fill algorithm - colouring a contiguous region. Each pixel is a node; edges connect adjacent pixels of the same colour}{figure.128}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {68.4.2}K-Nearest Neighbours Classification}{191}{subsubsection.68.4.2}\protected@file@percent }
\newlabel{subsubsec:knn-bfs}{{68.4.2}{191}{K-Nearest Neighbours Classification}{subsubsection.68.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {68.4.3}Application in Experimental Design}{191}{subsubsection.68.4.3}\protected@file@percent }
\newlabel{subsubsec:experimental-design}{{68.4.3}{191}{Application in Experimental Design}{subsubsection.68.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {129}{\ignorespaces BFS for experimental design: start at randomly allocated $s$; then each level set is coloured in reference to that point - this gives good coverage of treatment across the network structure.}}{192}{figure.129}\protected@file@percent }
\newlabel{fig:bfs-experimental}{{129}{192}{BFS for experimental design: start at randomly allocated $s$; then each level set is coloured in reference to that point - this gives good coverage of treatment across the network structure}{figure.129}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {68.4.4}Explore All Connected Components}{192}{subsubsection.68.4.4}\protected@file@percent }
\newlabel{subsubsec:connected-components}{{68.4.4}{192}{Explore All Connected Components}{subsubsection.68.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {69}Depth-First Search (DFS)}{193}{section.69}\protected@file@percent }
\newlabel{sec:dfs}{{69}{193}{Depth-First Search (DFS)}{section.69}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {69.1}DFS Outline}{193}{subsection.69.1}\protected@file@percent }
\newlabel{subsec:dfs-outline}{{69.1}{193}{DFS Outline}{subsection.69.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {130}{\ignorespaces DFS exploration pattern - the algorithm goes as deep as possible before backtracking.}}{193}{figure.130}\protected@file@percent }
\newlabel{fig:dfs-pattern}{{130}{193}{DFS exploration pattern - the algorithm goes as deep as possible before backtracking}{figure.130}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {69.1.1}Recursive Definition}{194}{subsubsection.69.1.1}\protected@file@percent }
\newlabel{subsubsec:dfs-recursive}{{69.1.1}{194}{Recursive Definition}{subsubsection.69.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {131}{\ignorespaces Recursive DFS - the call stack implicitly manages the traversal order.}}{194}{figure.131}\protected@file@percent }
\newlabel{fig:dfs-recursive}{{131}{194}{Recursive DFS - the call stack implicitly manages the traversal order}{figure.131}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {69.1.2}Non-Recursive Definition}{195}{subsubsection.69.1.2}\protected@file@percent }
\newlabel{subsubsec:dfs-iterative}{{69.1.2}{195}{Non-Recursive Definition}{subsubsection.69.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {132}{\ignorespaces Non-recursive (iterative) DFS using an explicit stack.}}{195}{figure.132}\protected@file@percent }
\newlabel{fig:dfs-iterative}{{132}{195}{Non-recursive (iterative) DFS using an explicit stack}{figure.132}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {69.2}Properties of DFS}{196}{subsection.69.2}\protected@file@percent }
\newlabel{subsec:dfs-properties}{{69.2}{196}{Properties of DFS}{subsection.69.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {69.3}Applications of DFS}{196}{subsection.69.3}\protected@file@percent }
\newlabel{subsec:dfs-applications}{{69.3}{196}{Applications of DFS}{subsection.69.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {70}BFS vs DFS}{196}{section.70}\protected@file@percent }
\newlabel{sec:bfs-vs-dfs}{{70}{196}{BFS vs DFS}{section.70}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {70.1}Intuitive Differences}{197}{subsection.70.1}\protected@file@percent }
\newlabel{subsec:bfs-dfs-intuition}{{70.1}{197}{Intuitive Differences}{subsection.70.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {70.2}Key Differences in Operation}{197}{subsection.70.2}\protected@file@percent }
\newlabel{subsec:bfs-dfs-operation}{{70.2}{197}{Key Differences in Operation}{subsection.70.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {70.3}Practical Implications}{198}{subsection.70.3}\protected@file@percent }
\newlabel{subsec:bfs-dfs-practical}{{70.3}{198}{Practical Implications}{subsection.70.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {71}Bipartite Graphs}{198}{section.71}\protected@file@percent }
\newlabel{sec:bipartite}{{71}{198}{Bipartite Graphs}{section.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {133}{\ignorespaces An uncoloured bipartite graph - can you see how to partition the vertices into two groups?}}{199}{figure.133}\protected@file@percent }
\newlabel{fig:bipartite-uncoloured}{{133}{199}{An uncoloured bipartite graph - can you see how to partition the vertices into two groups?}{figure.133}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {134}{\ignorespaces The same graph with a valid 2-colouring - vertices are partitioned into red and blue sets with no edges within either set.}}{199}{figure.134}\protected@file@percent }
\newlabel{fig:bipartite-coloured}{{134}{199}{The same graph with a valid 2-colouring - vertices are partitioned into red and blue sets with no edges within either set}{figure.134}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {71.1}Properties of Bipartite Graphs}{200}{subsection.71.1}\protected@file@percent }
\newlabel{subsec:bipartite-properties}{{71.1}{200}{Properties of Bipartite Graphs}{subsection.71.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {135}{\ignorespaces A bipartite graph - successfully 2-coloured.}}{200}{figure.135}\protected@file@percent }
\newlabel{fig:bipartite-2col}{{135}{200}{A bipartite graph - successfully 2-coloured}{figure.135}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {136}{\ignorespaces A non-bipartite graph - contains an odd-length cycle (triangle), making 2-colouring impossible.}}{201}{figure.136}\protected@file@percent }
\newlabel{fig:non-bipartite}{{136}{201}{A non-bipartite graph - contains an odd-length cycle (triangle), making 2-colouring impossible}{figure.136}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {71.2}Checking if a Graph is Bipartite}{201}{subsection.71.2}\protected@file@percent }
\newlabel{subsec:bipartite-check}{{71.2}{201}{Checking if a Graph is Bipartite}{subsection.71.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {71.3}Applications of Bipartite Graphs}{201}{subsection.71.3}\protected@file@percent }
\newlabel{subsec:bipartite-applications}{{71.3}{201}{Applications of Bipartite Graphs}{subsection.71.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {71.4}BFS and Bipartiteness}{202}{subsection.71.4}\protected@file@percent }
\newlabel{subsec:bfs-bipartite}{{71.4}{202}{BFS and Bipartiteness}{subsection.71.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {71.5}Analysing the Results}{202}{subsection.71.5}\protected@file@percent }
\newlabel{subsec:bipartite-analysis}{{71.5}{202}{Analysing the Results}{subsection.71.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {137}{\ignorespaces Left: Bipartite graph with no intra-layer edges. Right: Non-bipartite graph with an intra-layer edge (indicating an odd cycle).}}{203}{figure.137}\protected@file@percent }
\newlabel{fig:bipartite-layers}{{137}{203}{Left: Bipartite graph with no intra-layer edges. Right: Non-bipartite graph with an intra-layer edge (indicating an odd cycle)}{figure.137}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {138}{\ignorespaces BFS layers of a non-bipartite graph: the red edge connects two vertices within the same layer $L_3$, revealing an odd-length cycle.}}{204}{figure.138}\protected@file@percent }
\newlabel{fig:non-bipartite-layers}{{138}{204}{BFS layers of a non-bipartite graph: the red edge connects two vertices within the same layer $L_3$, revealing an odd-length cycle}{figure.138}{}}
\@writefile{toc}{\contentsline {section}{\numberline {72}Directed Graphs}{204}{section.72}\protected@file@percent }
\newlabel{sec:directed-graphs}{{72}{204}{Directed Graphs}{section.72}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {72.1}Directed Acyclic Graphs (DAGs)}{204}{subsection.72.1}\protected@file@percent }
\newlabel{subsec:dags}{{72.1}{204}{Directed Acyclic Graphs (DAGs)}{subsection.72.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {72.2}Topological Ordering}{205}{subsection.72.2}\protected@file@percent }
\newlabel{subsec:topological-ordering}{{72.2}{205}{Topological Ordering}{subsection.72.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {139}{\ignorespaces Left: A standard DAG representation. Right: The same DAG with vertices arranged in topological order - all edges point from left to right.}}{205}{figure.139}\protected@file@percent }
\newlabel{fig:topological-ordering}{{139}{205}{Left: A standard DAG representation. Right: The same DAG with vertices arranged in topological order - all edges point from left to right}{figure.139}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {72.2.1}Properties of Topological Ordering}{205}{subsubsection.72.2.1}\protected@file@percent }
\newlabel{subsubsec:topological-properties}{{72.2.1}{205}{Properties of Topological Ordering}{subsubsection.72.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {72.2.2}Kahn's Algorithm for Topological Sorting}{206}{subsubsection.72.2.2}\protected@file@percent }
\newlabel{subsubsec:kahns-algorithm}{{72.2.2}{206}{Kahn's Algorithm for Topological Sorting}{subsubsection.72.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {72.2.3}DFS-Based Algorithm for Topological Sorting}{206}{subsubsection.72.2.3}\protected@file@percent }
\newlabel{subsubsec:dfs-topological}{{72.2.3}{206}{DFS-Based Algorithm for Topological Sorting}{subsubsection.72.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {72.2.4}Applications of Topological Ordering}{206}{subsubsection.72.2.4}\protected@file@percent }
\newlabel{subsubsec:topological-applications}{{72.2.4}{206}{Applications of Topological Ordering}{subsubsection.72.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {72.2.5}Topological Ordering Implies DAG}{207}{subsubsection.72.2.5}\protected@file@percent }
\newlabel{subsubsec:topological-implies-dag}{{72.2.5}{207}{Topological Ordering Implies DAG}{subsubsection.72.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {140}{\ignorespaces The backward edge (bottom) cannot exist in a valid topological ordering - it would create a contradiction.}}{208}{figure.140}\protected@file@percent }
\newlabel{fig:topological-contradiction}{{140}{208}{The backward edge (bottom) cannot exist in a valid topological ordering - it would create a contradiction}{figure.140}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {72.2.6}Kahn's Algorithm (Detailed)}{208}{subsubsection.72.2.6}\protected@file@percent }
\newlabel{subsubsec:kahns-detailed}{{72.2.6}{208}{Kahn's Algorithm (Detailed)}{subsubsection.72.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {72.2.7}Causal Inference Application}{209}{subsubsection.72.2.7}\protected@file@percent }
\newlabel{subsubsec:causal-application}{{72.2.7}{209}{Causal Inference Application}{subsubsection.72.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {141}{\ignorespaces Causal DAG with topological ordering - when measuring the effect of $v_i$ on $v_j$, be careful about which variables to control for.}}{210}{figure.141}\protected@file@percent }
\newlabel{fig:causal-topological}{{141}{210}{Causal DAG with topological ordering - when measuring the effect of $v_i$ on $v_j$, be careful about which variables to control for}{figure.141}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {72.3}Finding d-Separation}{210}{subsection.72.3}\protected@file@percent }
\newlabel{subsec:d-separation}{{72.3}{210}{Finding d-Separation}{subsection.72.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {73}Min-Cut and Max-Flow}{211}{section.73}\protected@file@percent }
\newlabel{sec:mincut-maxflow}{{73}{211}{Min-Cut and Max-Flow}{section.73}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {73.1}Flow Networks}{211}{subsection.73.1}\protected@file@percent }
\newlabel{subsec:flow-networks}{{73.1}{211}{Flow Networks}{subsection.73.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {142}{\ignorespaces A flow network with source $s$, sink $t$, and edge capacities shown as weights.}}{212}{figure.142}\protected@file@percent }
\newlabel{fig:flow-network}{{142}{212}{A flow network with source $s$, sink $t$, and edge capacities shown as weights}{figure.142}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {143}{\ignorespaces A larger flow network example: each edge label indicates capacity. The goal is to find the maximum flow from $s$ to $t$.}}{212}{figure.143}\protected@file@percent }
\newlabel{fig:flow-network-large}{{143}{212}{A larger flow network example: each edge label indicates capacity. The goal is to find the maximum flow from $s$ to $t$}{figure.143}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {73.2}Graph Cuts}{213}{subsection.73.2}\protected@file@percent }
\newlabel{subsec:graph-cuts}{{73.2}{213}{Graph Cuts}{subsection.73.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {144}{\ignorespaces An $s$-$t$ cut: the cut capacity includes only edges crossing from $A$ to $B$, not edges from $B$ to $A$.}}{213}{figure.144}\protected@file@percent }
\newlabel{fig:graph-cut}{{144}{213}{An $s$-$t$ cut: the cut capacity includes only edges crossing from $A$ to $B$, not edges from $B$ to $A$}{figure.144}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {145}{\ignorespaces Another view of graph cuts - the dashed line shows the partition between sets $A$ and $B$.}}{214}{figure.145}\protected@file@percent }
\newlabel{fig:graph-cut-partition}{{145}{214}{Another view of graph cuts - the dashed line shows the partition between sets $A$ and $B$}{figure.145}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {73.3}Min-Cut Problem}{214}{subsection.73.3}\protected@file@percent }
\newlabel{subsec:min-cut}{{73.3}{214}{Min-Cut Problem}{subsection.73.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {146}{\ignorespaces The minimum cut represents the bottleneck capacity of the network - the maximum flow that can pass from $s$ to $t$.}}{215}{figure.146}\protected@file@percent }
\newlabel{fig:min-cut}{{146}{215}{The minimum cut represents the bottleneck capacity of the network - the maximum flow that can pass from $s$ to $t$}{figure.146}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {147}{\ignorespaces Graph cuts in image segmentation - pixels are nodes, and edges connect adjacent pixels with weights based on similarity.}}{216}{figure.147}\protected@file@percent }
\newlabel{fig:graph-cuts-image}{{147}{216}{Graph cuts in image segmentation - pixels are nodes, and edges connect adjacent pixels with weights based on similarity}{figure.147}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {73.4}Max-Flow Problem}{216}{subsection.73.4}\protected@file@percent }
\newlabel{subsec:max-flow}{{73.4}{216}{Max-Flow Problem}{subsection.73.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {73.5}Ford-Fulkerson Algorithm}{217}{subsection.73.5}\protected@file@percent }
\newlabel{subsec:ford-fulkerson}{{73.5}{217}{Ford-Fulkerson Algorithm}{subsection.73.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {73.5.1}Naive Algorithm (Fails)}{217}{subsubsection.73.5.1}\protected@file@percent }
\newlabel{subsubsec:naive-algo}{{73.5.1}{217}{Naive Algorithm (Fails)}{subsubsection.73.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {148}{\ignorespaces A flow network where the naive greedy approach can fail to find the maximum flow.}}{218}{figure.148}\protected@file@percent }
\newlabel{fig:max-flow-setup}{{148}{218}{A flow network where the naive greedy approach can fail to find the maximum flow}{figure.148}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {149}{\ignorespaces Step 1: Start with $f(e) = 0$ for each edge $e \in E$.}}{218}{figure.149}\protected@file@percent }
\newlabel{fig:max-flow-step1}{{149}{218}{Step 1: Start with $f(e) = 0$ for each edge $e \in E$}{figure.149}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {150}{\ignorespaces Step 2: Find an $s \rightarrow t$ path where each edge has $f(e) < c(e)$, then augment flow along that path.}}{219}{figure.150}\protected@file@percent }
\newlabel{fig:max-flow-step2}{{150}{219}{Step 2: Find an $s \rightarrow t$ path where each edge has $f(e) < c(e)$, then augment flow along that path}{figure.150}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {151}{\ignorespaces Step 3: Augment flow along the chosen path.}}{219}{figure.151}\protected@file@percent }
\newlabel{fig:max-flow-step3}{{151}{219}{Step 3: Augment flow along the chosen path}{figure.151}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {152}{\ignorespaces Step 4: Repeat until you get stuck (no more augmenting paths with available capacity).}}{220}{figure.152}\protected@file@percent }
\newlabel{fig:max-flow-step4}{{152}{220}{Step 4: Repeat until you get stuck (no more augmenting paths with available capacity)}{figure.152}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {153}{\ignorespaces Naive result: ending flow = 16.}}{220}{figure.153}\protected@file@percent }
\newlabel{fig:max-flow-naive-result}{{153}{220}{Naive result: ending flow = 16}{figure.153}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {154}{\ignorespaces Optimal result: maximum flow = 19. The naive approach missed 3 units of flow!}}{221}{figure.154}\protected@file@percent }
\newlabel{fig:max-flow-optimal}{{154}{221}{Optimal result: maximum flow = 19. The naive approach missed 3 units of flow!}{figure.154}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {73.5.2}Residual Flow / Residual Network}{221}{subsubsection.73.5.2}\protected@file@percent }
\newlabel{subsubsec:residual-network}{{73.5.2}{221}{Residual Flow / Residual Network}{subsubsection.73.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {155}{\ignorespaces Original edge with capacity $c(e)$ and current flow $f(e)$.}}{222}{figure.155}\protected@file@percent }
\newlabel{fig:original-flow}{{155}{222}{Original edge with capacity $c(e)$ and current flow $f(e)$}{figure.155}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {156}{\ignorespaces Residual network showing both forward capacity ($c(e) - f(e)$) and backward capacity ($f(e)$).}}{222}{figure.156}\protected@file@percent }
\newlabel{fig:residual-flow}{{156}{222}{Residual network showing both forward capacity ($c(e) - f(e)$) and backward capacity ($f(e)$)}{figure.156}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {73.5.3}Augmenting Path}{223}{subsubsection.73.5.3}\protected@file@percent }
\newlabel{subsubsec:augmenting-path}{{73.5.3}{223}{Augmenting Path}{subsubsection.73.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {157}{\ignorespaces An augmenting path in the residual network - the bottleneck determines how much flow can be pushed.}}{224}{figure.157}\protected@file@percent }
\newlabel{fig:augmenting-path}{{157}{224}{An augmenting path in the residual network - the bottleneck determines how much flow can be pushed}{figure.157}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {73.5.4}Ford-Fulkerson Algorithm}{224}{subsubsection.73.5.4}\protected@file@percent }
\newlabel{subsubsec:ff-algorithm}{{73.5.4}{224}{Ford-Fulkerson Algorithm}{subsubsection.73.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {158}{\ignorespaces Ford-Fulkerson Step 1: Initial flow network with all flows set to zero.}}{226}{figure.158}\protected@file@percent }
\newlabel{fig:ff-step1}{{158}{226}{Ford-Fulkerson Step 1: Initial flow network with all flows set to zero}{figure.158}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {159}{\ignorespaces Ford-Fulkerson Step 2: Find an augmenting path and compute its bottleneck.}}{227}{figure.159}\protected@file@percent }
\newlabel{fig:ff-step2}{{159}{227}{Ford-Fulkerson Step 2: Find an augmenting path and compute its bottleneck}{figure.159}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {160}{\ignorespaces Ford-Fulkerson Step 3: Augment flow along the path and update residual capacities.}}{228}{figure.160}\protected@file@percent }
\newlabel{fig:ff-step3}{{160}{228}{Ford-Fulkerson Step 3: Augment flow along the path and update residual capacities}{figure.160}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {161}{\ignorespaces Ford-Fulkerson Step 4: Continue finding augmenting paths in the updated residual network.}}{229}{figure.161}\protected@file@percent }
\newlabel{fig:ff-step4}{{161}{229}{Ford-Fulkerson Step 4: Continue finding augmenting paths in the updated residual network}{figure.161}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {162}{\ignorespaces Ford-Fulkerson Step 5: Another augmentation using a path that includes a backward edge.}}{230}{figure.162}\protected@file@percent }
\newlabel{fig:ff-step5}{{162}{230}{Ford-Fulkerson Step 5: Another augmentation using a path that includes a backward edge}{figure.162}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {163}{\ignorespaces Ford-Fulkerson Final: No more augmenting paths exist - maximum flow achieved.}}{231}{figure.163}\protected@file@percent }
\newlabel{fig:ff-final}{{163}{231}{Ford-Fulkerson Final: No more augmenting paths exist - maximum flow achieved}{figure.163}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {73.5.5}Edmonds-Karp Algorithm - Using BFS to Choose Paths}{231}{subsubsection.73.5.5}\protected@file@percent }
\newlabel{subsubsec:edmonds-karp}{{73.5.5}{231}{Edmonds-Karp Algorithm - Using BFS to Choose Paths}{subsubsection.73.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {164}{\ignorespaces Bad path selection in Ford-Fulkerson - alternating between two paths can lead to exponentially many iterations.}}{232}{figure.164}\protected@file@percent }
\newlabel{fig:ff-bad-paths}{{164}{232}{Bad path selection in Ford-Fulkerson - alternating between two paths can lead to exponentially many iterations}{figure.164}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {165}{\ignorespaces Edmonds-Karp uses BFS to find the shortest augmenting path (fewest edges).}}{233}{figure.165}\protected@file@percent }
\newlabel{fig:shortest-augmenting}{{165}{233}{Edmonds-Karp uses BFS to find the shortest augmenting path (fewest edges)}{figure.165}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {73.6}Max-Flow Min-Cut Theorem}{234}{subsection.73.6}\protected@file@percent }
\newlabel{subsec:max-flow-min-cut}{{73.6}{234}{Max-Flow Min-Cut Theorem}{subsection.73.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {74}Greedy Algorithm Outline}{236}{section.74}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {75}Cashier's Algorithm}{237}{section.75}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {166}{\ignorespaces Cashier's algorithm pseudocode: greedily select the largest denomination that fits}}{238}{figure.166}\protected@file@percent }
\newlabel{fig:cashiers-algo}{{166}{238}{Cashier's algorithm pseudocode: greedily select the largest denomination that fits}{figure.166}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Optimal solutions for coin change with US denominations must satisfy these bounds}}{239}{table.4}\protected@file@percent }
\newlabel{tab:coin_change}{{4}{239}{Optimal solutions for coin change with US denominations must satisfy these bounds}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {76}Activity Selection (Earliest-Finish-Time-First Algorithm)}{239}{section.76}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {76.1}Problem Setup: Interval Scheduling / Activity Selection}{239}{subsection.76.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {167}{\ignorespaces Interval scheduling problem: select the maximum number of non-overlapping intervals}}{240}{figure.167}\protected@file@percent }
\newlabel{fig:interval-scheduling}{{167}{240}{Interval scheduling problem: select the maximum number of non-overlapping intervals}{figure.167}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {76.2}Solution: Earliest-Finish-Time-First Algorithm}{240}{subsection.76.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {168}{\ignorespaces Earliest-finish-time-first algorithm pseudocode}}{240}{figure.168}\protected@file@percent }
\newlabel{fig:eftf-algo}{{168}{240}{Earliest-finish-time-first algorithm pseudocode}{figure.168}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {76.2.1}Process}{240}{subsubsection.76.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {76.3}Complexity Analysis}{241}{subsection.76.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {76.4}Optimality}{241}{subsection.76.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {77}Interval Partitioning Problem (Resource Allocation)}{242}{section.77}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {77.1}Problem Setup}{242}{subsection.77.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {77.2}Algorithm}{243}{subsection.77.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {169}{\ignorespaces Earliest-start-time-first algorithm for interval partitioning}}{243}{figure.169}\protected@file@percent }
\newlabel{fig:estf-algo}{{169}{243}{Earliest-start-time-first algorithm for interval partitioning}{figure.169}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {170}{\ignorespaces Incorrect approach: not ordered by start time leads to suboptimal classroom usage}}{243}{figure.170}\protected@file@percent }
\newlabel{fig:interval-partition-incorrect}{{170}{243}{Incorrect approach: not ordered by start time leads to suboptimal classroom usage}{figure.170}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {171}{\ignorespaces Correct approach: ordering by start time yields minimum classrooms}}{243}{figure.171}\protected@file@percent }
\newlabel{fig:interval-partition-correct}{{171}{243}{Correct approach: ordering by start time yields minimum classrooms}{figure.171}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {172}{\ignorespaces Binary heap structure: parent nodes satisfy heap property with children}}{245}{figure.172}\protected@file@percent }
\newlabel{fig:heap}{{172}{245}{Binary heap structure: parent nodes satisfy heap property with children}{figure.172}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {173}{\ignorespaces Binary heap stored as an array: children of node $i$ are at positions $2i+1$ and $2i+2$}}{245}{figure.173}\protected@file@percent }
\newlabel{fig:heap-array}{{173}{245}{Binary heap stored as an array: children of node $i$ are at positions $2i+1$ and $2i+2$}{figure.173}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {77.3}Complexity Analysis}{246}{subsection.77.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {77.4}Optimality}{247}{subsection.77.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {174}{\ignorespaces The depth (maximum overlap) provides a lower bound on classrooms needed; the greedy algorithm achieves this bound}}{247}{figure.174}\protected@file@percent }
\newlabel{fig:optimality}{{174}{247}{The depth (maximum overlap) provides a lower bound on classrooms needed; the greedy algorithm achieves this bound}{figure.174}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {77.5}Summary}{248}{subsection.77.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {78}Greedy Algorithms on Graphs}{248}{section.78}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {79}Dijkstra's Algorithm}{248}{section.79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {79.1}Problem Setup}{248}{subsection.79.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {175}{\ignorespaces Single-pair shortest path problem: find the minimum-weight path from $s$ to $t$}}{249}{figure.175}\protected@file@percent }
\newlabel{fig:shortest-path}{{175}{249}{Single-pair shortest path problem: find the minimum-weight path from $s$ to $t$}{figure.175}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {176}{\ignorespaces Single-source shortest paths: Dijkstra's algorithm finds shortest paths from $s$ to all other nodes}}{249}{figure.176}\protected@file@percent }
\newlabel{fig:single-source}{{176}{249}{Single-source shortest paths: Dijkstra's algorithm finds shortest paths from $s$ to all other nodes}{figure.176}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {79.2}Algorithm}{249}{subsection.79.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {177}{\ignorespaces Considering adding node $v$ to the explored set $S$: we examine all edges crossing the frontier}}{250}{figure.177}\protected@file@percent }
\newlabel{fig:dijkstra-1}{{177}{250}{Considering adding node $v$ to the explored set $S$: we examine all edges crossing the frontier}{figure.177}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {178}{\ignorespaces The greedy choice: select the node $v$ that minimises $d[u] + l_{(u,v)}$ over all frontier edges}}{250}{figure.178}\protected@file@percent }
\newlabel{fig:dijkstra-2}{{178}{250}{The greedy choice: select the node $v$ that minimises $d[u] + l_{(u,v)}$ over all frontier edges}{figure.178}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {79.3}Correctness}{252}{subsection.79.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {179}{\ignorespaces Path validation: if a shorter path $P$ existed, its first edge $(x,y)$ leaving $S$ would have been chosen instead of $(u,v)$}}{252}{figure.179}\protected@file@percent }
\newlabel{fig:path-validation}{{179}{252}{Path validation: if a shorter path $P$ existed, its first edge $(x,y)$ leaving $S$ would have been chosen instead of $(u,v)$}{figure.179}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {79.4}Efficiency and Implementation}{252}{subsection.79.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {180}{\ignorespaces Dijkstra's algorithm pseudocode with priority queue operations}}{253}{figure.180}\protected@file@percent }
\newlabel{fig:dijkstra-algo}{{180}{253}{Dijkstra's algorithm pseudocode with priority queue operations}{figure.180}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {79.5}Summary}{255}{subsection.79.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {80}Cycle-Cut Intersection}{255}{section.80}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {181}{\ignorespaces A cut partitions the vertex set into two non-empty subsets}}{255}{figure.181}\protected@file@percent }
\newlabel{fig:cut-set}{{181}{255}{A cut partitions the vertex set into two non-empty subsets}{figure.181}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {182}{\ignorespaces Cutset example: for cut $S = \{1, 3, 6, 7\}$, the cutset contains all edges crossing between $S$ and $V \setminus S$}}{255}{figure.182}\protected@file@percent }
\newlabel{fig:cutset-example}{{182}{255}{Cutset example: for cut $S = \{1, 3, 6, 7\}$, the cutset contains all edges crossing between $S$ and $V \setminus S$}{figure.182}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {183}{\ignorespaces Cycle-cut intersection: every cycle crosses a cutset an even number of times (entering and leaving)}}{256}{figure.183}\protected@file@percent }
\newlabel{fig:cycle-cut-1}{{183}{256}{Cycle-cut intersection: every cycle crosses a cutset an even number of times (entering and leaving)}{figure.183}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {184}{\ignorespaces Another example of cycle-cut intersection: the cycle enters and exits the cut region equally often}}{256}{figure.184}\protected@file@percent }
\newlabel{fig:cycle-cut-2}{{184}{256}{Another example of cycle-cut intersection: the cycle enters and exits the cut region equally often}{figure.184}{}}
\@writefile{toc}{\contentsline {section}{\numberline {81}Spanning Trees and Minimum Spanning Trees}{256}{section.81}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {81.1}Spanning Tree: General Characteristics}{256}{subsection.81.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {185}{\ignorespaces A spanning tree connects all vertices using a subset of edges, forming no cycles}}{257}{figure.185}\protected@file@percent }
\newlabel{fig:spanning-tree}{{185}{257}{A spanning tree connects all vertices using a subset of edges, forming no cycles}{figure.185}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {186}{\ignorespaces Different spanning trees of the same graph: the same vertices connected by different edge subsets}}{257}{figure.186}\protected@file@percent }
\newlabel{fig:spanning-tree-2}{{186}{257}{Different spanning trees of the same graph: the same vertices connected by different edge subsets}{figure.186}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {81.2}Minimum Spanning Tree (MST)}{258}{subsection.81.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {187}{\ignorespaces A minimum spanning tree: the spanning tree with the smallest total edge weight}}{259}{figure.187}\protected@file@percent }
\newlabel{fig:mst}{{187}{259}{A minimum spanning tree: the spanning tree with the smallest total edge weight}{figure.187}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {81.3}Fundamental Cycle}{260}{subsection.81.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {188}{\ignorespaces Fundamental cycle: adding edge $e$ (not in tree $T$) creates exactly one cycle}}{260}{figure.188}\protected@file@percent }
\newlabel{fig:fundamental-cycle}{{188}{260}{Fundamental cycle: adding edge $e$ (not in tree $T$) creates exactly one cycle}{figure.188}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {81.4}Fundamental Cutset}{261}{subsection.81.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {189}{\ignorespaces Fundamental cutset: removing edge $f$ splits the tree into two components; the cutset contains all edges crossing between them. If $c_e < c_f$, then $(V,T)$ was not an MST}}{261}{figure.189}\protected@file@percent }
\newlabel{fig:fundamental-cutset}{{189}{261}{Fundamental cutset: removing edge $f$ splits the tree into two components; the cutset contains all edges crossing between them. If $c_e < c_f$, then $(V,T)$ was not an MST}{figure.189}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {81.5}Greedy Algorithm to Find MST: Red-Blue Rules}{262}{subsection.81.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {81.6}Prim's Algorithm}{264}{subsection.81.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {190}{\ignorespaces Prim's algorithm visualisation: the tree (blue) grows by adding the minimum-weight edge to an unexplored vertex}}{264}{figure.190}\protected@file@percent }
\newlabel{fig:prims-visual}{{190}{264}{Prim's algorithm visualisation: the tree (blue) grows by adding the minimum-weight edge to an unexplored vertex}{figure.190}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {191}{\ignorespaces Prim's algorithm pseudocode: similar structure to Dijkstra but tracking edge costs rather than path distances}}{265}{figure.191}\protected@file@percent }
\newlabel{fig:prims-algo}{{191}{265}{Prim's algorithm pseudocode: similar structure to Dijkstra but tracking edge costs rather than path distances}{figure.191}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {81.7}Kruskal's Algorithm}{266}{subsection.81.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {81.8}MSTs in Experimental Design}{266}{subsection.81.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {192}{\ignorespaces Using MST for experimental design: similar units (connected in the MST) are assigned to different treatment groups to ensure balance}}{267}{figure.192}\protected@file@percent }
\newlabel{fig:mst-experimental}{{192}{267}{Using MST for experimental design: similar units (connected in the MST) are assigned to different treatment groups to ensure balance}{figure.192}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {81.9}Summary: Minimum Spanning Trees}{268}{subsection.81.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {82}Intractability}{269}{section.82}\protected@file@percent }
\newlabel{ch:intractability}{{82}{269}{Intractability}{section.82}{}}
\@writefile{toc}{\contentsline {section}{\numberline {83}Introduction and Motivation}{269}{section.83}\protected@file@percent }
\newlabel{sec:intractability-intro}{{83}{269}{Introduction and Motivation}{section.83}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {83.1}Classifying Computational Problems}{269}{subsection.83.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {84}Complexity Class P: Polynomial-Time Algorithms}{270}{section.84}\protected@file@percent }
\newlabel{sec:class-p}{{84}{270}{Complexity Class P: Polynomial-Time Algorithms}{section.84}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {84.1}What Makes Polynomial Time ``Efficient''?}{270}{subsection.84.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {84.2}Easy vs Hard: A Surprising Dichotomy}{270}{subsection.84.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {85}Polynomial-Time Reductions}{271}{section.85}\protected@file@percent }
\newlabel{sec:reductions}{{85}{271}{Polynomial-Time Reductions}{section.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {193}{\ignorespaces Polynomial-time reduction from problem $X$ to problem $Y$. An instance of $X$ is transformed (in polynomial time) into an instance of $Y$. The oracle solves $Y$, and the solution is transformed back to answer $X$.}}{272}{figure.193}\protected@file@percent }
\newlabel{fig:poly-reduction}{{193}{272}{Polynomial-time reduction from problem $X$ to problem $Y$. An instance of $X$ is transformed (in polynomial time) into an instance of $Y$. The oracle solves $Y$, and the solution is transformed back to answer $X$}{figure.193}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {85.1}The Reduction Process}{272}{subsection.85.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {85.2}Understanding the Direction of Reductions}{272}{subsection.85.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {85.3}Polynomial Equivalence}{273}{subsection.85.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {86}Classic Reductions: Vertex Cover and Independent Set}{273}{section.86}\protected@file@percent }
\newlabel{sec:vc-is}{{86}{273}{Classic Reductions: Vertex Cover and Independent Set}{section.86}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {86.1}Independent Set}{273}{subsection.86.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {194}{\ignorespaces An independent set in a graph. The highlighted vertices form an independent set of size 6-no two highlighted vertices share an edge. This graph has an independent set of size $\geq 6$ but not of size $\geq 7$.}}{273}{figure.194}\protected@file@percent }
\newlabel{fig:independent-set}{{194}{273}{An independent set in a graph. The highlighted vertices form an independent set of size 6-no two highlighted vertices share an edge. This graph has an independent set of size $\geq 6$ but not of size $\geq 7$}{figure.194}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {86.2}Vertex Cover}{274}{subsection.86.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {195}{\ignorespaces A vertex cover in a graph. The highlighted vertices form a vertex cover of size 4-every edge has at least one endpoint among the highlighted vertices. This graph has a vertex cover of size $\leq 4$ but not of size $\leq 3$.}}{274}{figure.195}\protected@file@percent }
\newlabel{fig:vertex-cover}{{195}{274}{A vertex cover in a graph. The highlighted vertices form a vertex cover of size 4-every edge has at least one endpoint among the highlighted vertices. This graph has a vertex cover of size $\leq 4$ but not of size $\leq 3$}{figure.195}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {86.3}Intuitive Comparison}{274}{subsection.86.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {86.4}The Complement Relationship}{274}{subsection.86.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {196}{\ignorespaces The complement relationship between vertex cover and independent set. In this graph, the white vertices form a vertex cover (every edge has at least one white endpoint), while the black vertices form an independent set (no two black vertices are adjacent). Together, they partition the vertex set.}}{275}{figure.196}\protected@file@percent }
\newlabel{fig:vc-is-complement}{{196}{275}{The complement relationship between vertex cover and independent set. In this graph, the white vertices form a vertex cover (every edge has at least one white endpoint), while the black vertices form an independent set (no two black vertices are adjacent). Together, they partition the vertex set}{figure.196}{}}
\@writefile{toc}{\contentsline {section}{\numberline {87}Set Cover and Its Relationship to Vertex Cover}{276}{section.87}\protected@file@percent }
\newlabel{sec:set-cover}{{87}{276}{Set Cover and Its Relationship to Vertex Cover}{section.87}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {87.1}The Set Cover Problem}{276}{subsection.87.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {197}{\ignorespaces A Set Cover instance with $U = \{1, 2, 3, 4, 5, 6, 7\}$ and six subsets. The question asks whether we can cover all elements using $k = 2$ or fewer subsets. Here, $S_c \cup S_f = \{3, 4, 5, 6\} \cup \{1, 2, 6, 7\} = U$, so yes.}}{276}{figure.197}\protected@file@percent }
\newlabel{fig:set-cover}{{197}{276}{A Set Cover instance with $U = \{1, 2, 3, 4, 5, 6, 7\}$ and six subsets. The question asks whether we can cover all elements using $k = 2$ or fewer subsets. Here, $S_c \cup S_f = \{3, 4, 5, 6\} \cup \{1, 2, 6, 7\} = U$, so yes}{figure.197}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {87.2}Reduction: Vertex Cover $\leq _p$ Set Cover}{276}{subsection.87.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {198}{\ignorespaces Reduction from Vertex Cover to Set Cover. Each vertex becomes a subset containing its incident edges. The graph on the left has vertices $\{a, b, c, d, e, f\}$ and edges $\{e_1, \ldots  , e_7\}$. Selecting vertices $\{a, c\}$ as a vertex cover corresponds to selecting subsets $\{S_a, S_c\}$ in the Set Cover instance, which together cover all edges.}}{277}{figure.198}\protected@file@percent }
\newlabel{fig:vc-to-sc}{{198}{277}{Reduction from Vertex Cover to Set Cover. Each vertex becomes a subset containing its incident edges. The graph on the left has vertices $\{a, b, c, d, e, f\}$ and edges $\{e_1, \ldots , e_7\}$. Selecting vertices $\{a, c\}$ as a vertex cover corresponds to selecting subsets $\{S_a, S_c\}$ in the Set Cover instance, which together cover all edges}{figure.198}{}}
\@writefile{toc}{\contentsline {section}{\numberline {88}Satisfiability and the 3-SAT Problem}{277}{section.88}\protected@file@percent }
\newlabel{sec:sat}{{88}{277}{Satisfiability and the 3-SAT Problem}{section.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {88.1}Preliminaries: Boolean Logic}{278}{subsection.88.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {88.2}The SAT and 3-SAT Problems}{278}{subsection.88.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {88.3}Reduction: 3-SAT $\leq _p$ Independent Set}{278}{subsection.88.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {199}{\ignorespaces Reduction from 3-SAT to Independent Set. Each clause becomes a triangle of three vertices. Edges within triangles ensure we pick at most one literal per clause. Edges between contradictory literals (e.g., $x_1$ and $\neg x_1$) ensure consistency: if we include $x_1$ in our independent set, we cannot include $\neg x_1$. An independent set of size $k$ (one vertex per clause) corresponds to a satisfying assignment.}}{279}{figure.199}\protected@file@percent }
\newlabel{fig:3sat-to-is}{{199}{279}{Reduction from 3-SAT to Independent Set. Each clause becomes a triangle of three vertices. Edges within triangles ensure we pick at most one literal per clause. Edges between contradictory literals (e.g., $x_1$ and $\neg x_1$) ensure consistency: if we include $x_1$ in our independent set, we cannot include $\neg x_1$. An independent set of size $k$ (one vertex per clause) corresponds to a satisfying assignment}{figure.199}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {88.4}Transitivity of Reductions}{279}{subsection.88.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {89}Reduction Strategies}{280}{section.89}\protected@file@percent }
\newlabel{sec:reduction-strategies}{{89}{280}{Reduction Strategies}{section.89}{}}
\@writefile{toc}{\contentsline {section}{\numberline {90}Decision, Search, and Optimisation Problems}{280}{section.90}\protected@file@percent }
\newlabel{sec:decision-search-opt}{{90}{280}{Decision, Search, and Optimisation Problems}{section.90}{}}
\@writefile{toc}{\contentsline {section}{\numberline {91}Complexity Class NP: Efficient Verification}{281}{section.91}\protected@file@percent }
\newlabel{sec:class-np}{{91}{281}{Complexity Class NP: Efficient Verification}{section.91}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {91.1}Certificates and Verification}{281}{subsection.91.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {91.2}The Class NP}{282}{subsection.91.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {91.3}Examples of Problems in NP}{282}{subsection.91.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {91.4}What is NOT in NP?}{283}{subsection.91.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {92}P, NP, and EXP: The Complexity Landscape}{283}{section.92}\protected@file@percent }
\newlabel{sec:complexity-classes}{{92}{283}{P, NP, and EXP: The Complexity Landscape}{section.92}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {92.1}The Three Main Classes}{283}{subsection.92.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {92.2}Known Relationships}{283}{subsection.92.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {92.3}The P vs NP Question}{284}{subsection.92.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {200}{\ignorespaces Two possible worlds. \textbf  {Left:} If P = NP, the classes collapse-every problem with efficiently verifiable solutions also has an efficient algorithm. \textbf  {Right:} If P $\neq  $ NP (the prevailing belief), P is strictly contained in NP, and there exist problems that can be verified but not solved efficiently.}}{284}{figure.200}\protected@file@percent }
\newlabel{fig:p-vs-np}{{200}{284}{Two possible worlds. \textbf {Left:} If P = NP, the classes collapse-every problem with efficiently verifiable solutions also has an efficient algorithm. \textbf {Right:} If P $\neq $ NP (the prevailing belief), P is strictly contained in NP, and there exist problems that can be verified but not solved efficiently}{figure.200}{}}
\@writefile{toc}{\contentsline {section}{\numberline {93}NP-Completeness: The Hardest Problems in NP}{285}{section.93}\protected@file@percent }
\newlabel{sec:np-complete}{{93}{285}{NP-Completeness: The Hardest Problems in NP}{section.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {93.1}Definition}{285}{subsection.93.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {201}{\ignorespaces Relationship between complexity classes (assuming P $\neq  $ NP). NP-complete problems sit at the boundary of NP-the hardest problems within NP. NP-hard problems are at least as hard as NP-complete but may lie outside NP entirely.}}{285}{figure.201}\protected@file@percent }
\newlabel{fig:complexity-venn}{{201}{285}{Relationship between complexity classes (assuming P $\neq $ NP). NP-complete problems sit at the boundary of NP-the hardest problems within NP. NP-hard problems are at least as hard as NP-complete but may lie outside NP entirely}{figure.201}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {93.2}The Cook-Levin Theorem}{285}{subsection.93.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {93.3}Examples of NP-Complete Problems}{286}{subsection.93.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {94}NP-Hard: Beyond NP}{286}{section.94}\protected@file@percent }
\newlabel{sec:np-hard}{{94}{286}{NP-Hard: Beyond NP}{section.94}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {94.1}The Landscape of Hard Problems}{287}{subsection.94.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {95}Coping with NP-Complete Problems}{287}{section.95}\protected@file@percent }
\newlabel{sec:coping}{{95}{287}{Coping with NP-Complete Problems}{section.95}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {95.1}Approximation Algorithms}{288}{subsection.95.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {95.2}Heuristics and Metaheuristics}{288}{subsection.95.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {95.3}Special Cases and Problem Structure}{288}{subsection.95.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {95.4}Exponential Algorithms Done Well}{289}{subsection.95.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {95.5}Connection to Machine Learning}{289}{subsection.95.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {96}Summary}{289}{section.96}\protected@file@percent }
\newlabel{sec:intractability-summary}{{96}{289}{Summary}{section.96}{}}
\gdef \@abspage@last{294}
