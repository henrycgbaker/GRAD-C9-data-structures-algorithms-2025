% Week 2: Introduction to Computing
% Covers: bits, logic gates, hexadecimal, memory, text/integer representation,
% binary arithmetic, floating point, and CPU architecture

\chapter{Introduction to Computing}
\label{ch:intro-computing}

Understanding how computers represent and manipulate data at the lowest level is fundamental to data structures and algorithms. The efficiency of algorithms often depends critically on how data is stored and accessed in memory, how arithmetic operations are performed at the hardware level, and what the inherent limitations of numerical representation are. This week, we build the foundation upon which all subsequent algorithmic analysis rests.

\section{What is a Computer?}
\label{sec:what-is-computer}

At its core, a computer is a machine that manipulates symbols according to a set of rules. These symbols are represented as \emph{bits}-the fundamental unit of information in computing.

\subsection{Bits: The Fundamental Unit}
\label{subsec:bits}

\begin{rigour}[Definition: Bit]
A \textbf{bit} (binary digit) is the smallest unit of data in computing, representing exactly one of two possible states. We denote these states as:
\begin{itemize}
    \item Logical: \texttt{TRUE} / \texttt{FALSE}
    \item Numerical: \texttt{1} / \texttt{0}
    \item Physical: \texttt{HIGH VOLTAGE} / \texttt{LOW VOLTAGE}, \texttt{ON} / \texttt{OFF}
\end{itemize}
All of these representations are isomorphic-they encode exactly the same information.
\end{rigour}

The power of bits lies in their simplicity and reliability. A continuous signal is inherently susceptible to noise and degradation, but a discrete binary signal only needs to distinguish between two states. This makes digital systems remarkably robust: small variations in voltage do not change the interpretation of a bit.

\begin{keybox}[Why Bits Matter for Algorithms]
Every data structure you will encounter-arrays, linked lists, trees, hash tables-ultimately reduces to patterns of bits in memory. Understanding bit-level representation helps you:
\begin{itemize}
    \item Analyse the true memory footprint of data structures
    \item Understand overflow and precision errors in numerical algorithms
    \item Design efficient bit manipulation algorithms (e.g., bit vectors, bloom filters)
    \item Reason about cache efficiency and memory access patterns
\end{itemize}
\end{keybox}

\subsection{Logic Gates}
\label{subsec:logic-gates}

Logic gates are the physical building blocks that implement Boolean operations on bits. They are constructed from transistors and form the foundation of all digital circuits.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{truth tables.png}
    \caption{Truth tables for fundamental logic gates. A useful mnemonic: read ``A is 1, \textit{[gate name]} B is 1'' to determine when the output is 1. For AND: ``A is 1 \textit{and} B is 1''; for OR: ``A is 1 \textit{or} B is 1''.}
    \label{fig:truth-tables}
\end{figure}

The fundamental gates are:

\begin{itemize}
    \item \textbf{AND gate}: Output is 1 only when \emph{both} inputs are 1. Denoted $A \land B$ or $A \cdot B$.
    \item \textbf{OR gate}: Output is 1 when \emph{at least one} input is 1. Denoted $A \lor B$ or $A + B$.
    \item \textbf{NOT gate} (inverter): Output is the opposite of the input. Denoted $\lnot A$ or $\overline{A}$.
    \item \textbf{XOR gate} (exclusive or): Output is 1 when inputs are \emph{different}. Denoted $A \oplus B$.
\end{itemize}

\begin{rigour}[Boolean Algebra Identities]
These identities are useful for simplifying logical expressions and understanding circuit design:
\begin{align*}
    A \land 0 &= 0 & A \lor 0 &= A & \text{(Identity)} \\
    A \land 1 &= A & A \lor 1 &= 1 & \text{(Identity)} \\
    A \land A &= A & A \lor A &= A & \text{(Idempotence)} \\
    A \land \lnot A &= 0 & A \lor \lnot A &= 1 & \text{(Complement)} \\
    \lnot(\lnot A) &= A & & & \text{(Double negation)} \\
    \lnot(A \land B) &= \lnot A \lor \lnot B & \lnot(A \lor B) &= \lnot A \land \lnot B & \text{(De Morgan's laws)}
\end{align*}
\end{rigour}

\subsubsection{The NAND Gate: A Universal Building Block}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{NAND gate.png}
    \caption{The NAND gate symbol and truth table. NAND outputs 0 only when both inputs are 1-it is the negation of AND.}
    \label{fig:nand-gate}
\end{figure}

The NAND gate (``NOT AND'') is particularly important because it is \emph{functionally complete}: any Boolean function can be implemented using only NAND gates. This is why NAND gates are the most commonly used gates in integrated circuit design.

\begin{rigour}[Functional Completeness of NAND]
A set of logic gates is \textbf{functionally complete} if any Boolean function can be expressed using only gates from that set. NAND is functionally complete because:
\begin{itemize}
    \item NOT: $\lnot A = A \text{ NAND } A$
    \item AND: $A \land B = (A \text{ NAND } B) \text{ NAND } (A \text{ NAND } B)$
    \item OR: $A \lor B = (A \text{ NAND } A) \text{ NAND } (B \text{ NAND } B)$
\end{itemize}
Since NOT, AND, and OR can express any Boolean function, so can NAND alone.
\end{rigour}

\section{Hexadecimal Notation}
\label{sec:hexadecimal}

While binary is the native language of computers, it is cumbersome for humans. Hexadecimal (base-16) provides a compact representation that maps cleanly to binary.

\begin{rigour}[Definition: Hexadecimal]
The \textbf{hexadecimal} (hex) number system is a positional numeral system with base 16. The digit set is:
\[
\mathcal{H} = \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F\}
\]
where $A = 10$, $B = 11$, $C = 12$, $D = 13$, $E = 14$, $F = 15$ in decimal.

Hexadecimal numbers are typically prefixed with \texttt{0x} (e.g., \texttt{0x3A}) to distinguish them from decimal.
\end{rigour}

\begin{keybox}[Why Hexadecimal?]
Since $16 = 2^4$, each hexadecimal digit corresponds to exactly 4 binary digits (bits). This makes conversion between hex and binary trivial, while being four times more compact than binary representation.
\end{keybox}

\subsection{Conversion Reference}

\begin{center}
    \begin{tabular}{|c|c|c||c|c|c|}
    \hline
    Hex & Decimal & Binary & Hex & Decimal & Binary \\
    \hline
    0 & 0 & 0000 & 8 & 8 & 1000 \\
    1 & 1 & 0001 & 9 & 9 & 1001 \\
    2 & 2 & 0010 & A & 10 & 1010 \\
    3 & 3 & 0011 & B & 11 & 1011 \\
    4 & 4 & 0100 & C & 12 & 1100 \\
    5 & 5 & 0101 & D & 13 & 1101 \\
    6 & 6 & 0110 & E & 14 & 1110 \\
    7 & 7 & 0111 & F & 15 & 1111 \\
    \hline
    \end{tabular}
\end{center}

\subsection{Conversion Procedures}

\subsubsection{Hexadecimal to Binary}
Replace each hex digit with its 4-bit binary equivalent:

\textbf{Example:} Convert $\texttt{0x3A}$ to binary.
\begin{itemize}
    \item $\texttt{3} \to 0011_2$
    \item $\texttt{A} \to 1010_2$
    \item Combined: $\texttt{0x3A} = 0011\,1010_2$
\end{itemize}

\subsubsection{Binary to Hexadecimal}
Group binary digits into sets of four from the right, padding with leading zeros if necessary:

\textbf{Example:} Convert $10111011_2$ to hexadecimal.
\begin{itemize}
    \item Group: $1011\,1011$
    \item $1011_2 = \texttt{B}$
    \item $1011_2 = \texttt{B}$
    \item Combined: $10111011_2 = \texttt{0xBB}$
\end{itemize}

\subsubsection{Hexadecimal to Decimal}
Expand using positional notation:

\textbf{Example:} Convert $\texttt{0x3A}$ to decimal.
\[
\texttt{0x3A} = 3 \times 16^1 + 10 \times 16^0 = 48 + 10 = 58_{10}
\]

\section{Memory}
\label{sec:memory}

Memory is where computers store information. Understanding memory organisation is crucial for analysing the space complexity of algorithms and understanding how data structures are laid out.

\subsection{Flip-Flops: The Basis of Memory}

\begin{rigour}[Definition: Flip-Flop]
A \textbf{flip-flop} is a bistable electronic circuit that can store one bit of information. It maintains its state (0 or 1) until explicitly changed by an input signal. Flip-flops are constructed from logic gates arranged with feedback loops.
\end{rigour}

Memory can be conceptualised as an organised collection of flip-flops:
\begin{itemize}
    \item A \textbf{row of flip-flops} forms a sequence of bits: $[\text{bit}_0, \text{bit}_1, \text{bit}_2, \ldots, \text{bit}_{n-1}]$
    \item Each bit or group of bits has a unique \textbf{address}-a numerical identifier used to locate it
    \item Memory is organised hierarchically: bits $\to$ bytes $\to$ words $\to$ pages
\end{itemize}

\subsection{Random Access Memory (RAM)}

\begin{keybox}[Random Access]
\textbf{Random Access Memory (RAM)} allows any memory location to be accessed directly in constant time $O(1)$, regardless of its address. This is in contrast to \textbf{sequential access} memory (like magnetic tape), where accessing an element requires traversing all preceding elements.

The ``random'' in RAM refers to the ability to access any location equally quickly-not to randomness in the statistical sense.
\end{keybox}

This constant-time access property is fundamental to the performance of array-based data structures and is assumed in most algorithm analysis.

\subsection{Memory Units and Addressing}

\begin{rigour}[Definition: Byte]
A \textbf{byte} is a unit of digital information consisting of 8 bits:
\[
1 \text{ byte} = 8 \text{ bits}
\]
The byte is the standard addressable unit in most modern computer architectures.
\end{rigour}

\begin{redbox}[KB vs kB vs Kb: A Common Source of Confusion]
Memory units have two competing conventions:
\begin{itemize}
    \item \textbf{Binary prefixes} (IEC standard): $1 \text{ KiB (kibibyte)} = 2^{10} = 1024$ bytes
    \item \textbf{SI prefixes}: $1 \text{ kB (kilobyte)} = 10^3 = 1000$ bytes
\end{itemize}

Additionally, note the case distinction:
\begin{itemize}
    \item Uppercase B = bytes (e.g., KB, MB, GB)
    \item Lowercase b = bits (e.g., kb, Mb, Gb)
\end{itemize}

Network speeds are typically measured in bits per second (e.g., 100 Mbps), while storage is measured in bytes (e.g., 500 GB). A ``100 Mbps'' connection transfers at most $100/8 = 12.5$ MB per second.
\end{redbox}

\begin{center}
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Unit} & \textbf{Binary (Traditional)} & \textbf{SI (Decimal)} \\
    \hline
    Kilobyte & $2^{10} = 1024$ bytes & $10^3 = 1000$ bytes \\
    Megabyte & $2^{20} = 1{,}048{,}576$ bytes & $10^6 = 1{,}000{,}000$ bytes \\
    Gigabyte & $2^{30} \approx 1.07 \times 10^9$ bytes & $10^9$ bytes \\
    \hline
    \end{tabular}
\end{center}

\subsection{Memory Addressing}

In a byte-addressable machine, each byte has a unique address. For a memory with $2^n$ addressable locations, we need $n$ bits for addresses.

\begin{verbatim}
Address:    0x00  0x01  0x02  0x03  0x04  0x05  0x06  0x07
          +---+---+---+---+---+---+---+---+
Data:     |byte0|byte1|byte2|byte3|byte4|byte5|byte6|byte7|
          +---+---+---+---+---+---+---+---+
\end{verbatim}

\textbf{Example:} In a 1 KB memory:
\begin{itemize}
    \item Total bytes: $1024$
    \item Address range: $0$ to $1023$ (or \texttt{0x000} to \texttt{0x3FF})
    \item Bits needed for addressing: $\lceil \log_2 1024 \rceil = 10$ bits
\end{itemize}

To access the 10th byte (address 10), we retrieve bits 80--87 (since byte 10 contains bits $10 \times 8 = 80$ through $10 \times 8 + 7 = 87$).

\section{Data Representation}
\label{sec:representation}

All data in a computer-text, numbers, images, programs-must ultimately be represented as sequences of bits. This section examines how different types of data are encoded.

\subsection{Text Representation}
\label{subsec:text-representation}

\subsubsection{ASCII (American Standard Code for Information Interchange)}

\begin{rigour}[Definition: ASCII]
\textbf{ASCII} is a character encoding standard that assigns numerical codes to 128 characters:
\begin{itemize}
    \item \textbf{Bit length}: 7 bits per character (often stored in 8 bits with a leading zero)
    \item \textbf{Range}: 0--127
    \item \textbf{Coverage}: English letters (A--Z, a--z), digits (0--9), punctuation, and control characters
\end{itemize}
\end{rigour}

\textbf{Examples:}
\begin{itemize}
    \item \texttt{'A'} = 65 = $01000001_2$ = \texttt{0x41}
    \item \texttt{'a'} = 97 = $01100001_2$ = \texttt{0x61}
    \item \texttt{'0'} = 48 = $00110000_2$ = \texttt{0x30}
\end{itemize}

Notice that uppercase and lowercase letters differ by exactly 32 ($2^5$), which means converting case requires only flipping bit 5.

\subsubsection{Unicode}

\begin{rigour}[Definition: Unicode]
\textbf{Unicode} is a universal character encoding standard that aims to represent every character from every writing system. Key points:
\begin{itemize}
    \item \textbf{Code points}: Over 140,000 characters across 150+ scripts
    \item \textbf{Notation}: Code points written as U+XXXX (e.g., U+0041 for `A')
    \item \textbf{Encodings}:
    \begin{itemize}
        \item \textbf{UTF-8}: Variable length (1--4 bytes), backwards compatible with ASCII
        \item \textbf{UTF-16}: Variable length (2 or 4 bytes)
        \item \textbf{UTF-32}: Fixed length (4 bytes per character)
    \end{itemize}
\end{itemize}
\end{rigour}

\begin{keybox}[UTF-8: The Web's Encoding]
UTF-8 is the dominant encoding on the web because:
\begin{itemize}
    \item ASCII text is valid UTF-8 (backwards compatibility)
    \item Common characters use fewer bytes (efficient for English)
    \item Self-synchronising: you can find character boundaries by examining any byte
\end{itemize}
\end{keybox}

\subsection{Integer Representation}
\label{subsec:integer-representation}

\subsubsection{Unsigned Binary Integers}

The simplest integer representation interprets a sequence of bits as a base-2 number.

\begin{rigour}[Definition: Unsigned Integer]
An \textbf{unsigned $n$-bit integer} interprets a bit sequence $b_{n-1}b_{n-2}\ldots b_1 b_0$ as:
\[
\text{value} = \sum_{i=0}^{n-1} b_i \cdot 2^i
\]
\textbf{Range}: $0$ to $2^n - 1$
\end{rigour}

\begin{keybox}[Unsigned Integer Range]
For $n$ bits, an unsigned integer can represent values from $0$ to $2^n - 1$:
\begin{itemize}
    \item 8 bits: $0$ to $255$
    \item 16 bits: $0$ to $65{,}535$
    \item 32 bits: $0$ to $4{,}294{,}967{,}295$ ($\approx 4.3 \times 10^9$)
    \item 64 bits: $0$ to $18{,}446{,}744{,}073{,}709{,}551{,}615$ ($\approx 1.8 \times 10^{19}$)
\end{itemize}
\end{keybox}

\textbf{Example:} The 8-bit sequence $1011_2$ represents:
\[
1011_2 = 1 \times 2^3 + 0 \times 2^2 + 1 \times 2^1 + 1 \times 2^0 = 8 + 0 + 2 + 1 = 11_{10}
\]

\subsubsection{Two's Complement for Signed Integers}

To represent negative numbers, we need a scheme that:
\begin{enumerate}
    \item Uses a fixed number of bits
    \item Makes arithmetic operations simple (ideally using the same circuitry as unsigned)
    \item Has a unique representation for zero
\end{enumerate}

Two's complement achieves all three goals and is the standard for signed integer representation.

\begin{rigour}[Definition: Two's Complement]
In \textbf{two's complement} representation for $n$ bits:
\begin{itemize}
    \item The most significant bit (MSB) $b_{n-1}$ is the \textbf{sign bit}: 0 for non-negative, 1 for negative
    \item The value is computed as:
    \[
    \text{value} = -b_{n-1} \cdot 2^{n-1} + \sum_{i=0}^{n-2} b_i \cdot 2^i
    \]
    \item \textbf{Range}: $-2^{n-1}$ to $2^{n-1} - 1$
\end{itemize}
\end{rigour}

\begin{keybox}[Two's Complement Range]
For $n$ bits in two's complement:
\begin{itemize}
    \item 8 bits: $-128$ to $127$
    \item 16 bits: $-32{,}768$ to $32{,}767$
    \item 32 bits: $-2{,}147{,}483{,}648$ to $2{,}147{,}483{,}647$ ($\approx \pm 2.1 \times 10^9$)
\end{itemize}
Note the asymmetry: there is one more negative value than positive values.
\end{keybox}

\textbf{Converting a positive number to its negative (and vice versa):}
\begin{enumerate}
    \item \textbf{Invert all bits} (flip 0s to 1s and 1s to 0s)
    \item \textbf{Add 1} to the result
\end{enumerate}

\textbf{Example:} Represent $-6$ in 4-bit two's complement.
\begin{enumerate}
    \item Start with $+6 = 0110_2$
    \item Invert all bits: $1001_2$
    \item Add 1: $1001_2 + 1 = 1010_2$
    \item Result: $-6 = 1010_2$
\end{enumerate}

\textbf{Verification using the formula:}
\[
1010_2 = -1 \times 2^3 + 0 \times 2^2 + 1 \times 2^1 + 0 \times 2^0 = -8 + 2 = -6 \checkmark
\]

\begin{keybox}[Two's Complement Heuristics]
Useful shortcuts for working with two's complement:
\begin{enumerate}
    \item If the sign bit is 0, the remaining bits give the magnitude directly
    \item If the sign bit is 1, find the magnitude by inverting and adding 1 (same procedure as negation)
    \item A number and its negation share the same bits from the rightmost 1 onwards; bits to the left of this are complements:
    \begin{itemize}
        \item $+6 = 0\mathbf{110}$
        \item $-6 = 1\mathbf{010}$
    \end{itemize}
    (The rightmost `10' is the same; the bits to the left are complementary)
\end{enumerate}
\end{keybox}

\subsubsection{Excess-$K$ (Biased) Notation}

Excess notation is another way to represent signed integers, primarily used for exponents in floating-point numbers.

\begin{rigour}[Definition: Excess-$K$ Notation]
In \textbf{excess-$K$} (or biased) notation, a value $v$ is stored as the unsigned binary representation of $v + K$, where $K$ is a fixed bias.

To decode: $\text{value} = \text{stored unsigned value} - K$

For $n$ bits, typically $K = 2^{n-1}$ or $K = 2^{n-1} - 1$.
\end{rigour}

\textbf{Example: Excess-8 notation with 4 bits}

The bias is $K = 8$, so zero is stored as $8 = 1000_2$.

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Value} & \textbf{Stored (unsigned)} & \textbf{Binary} \\ \hline
$+7$ & $7 + 8 = 15$ & $1111$ \\ \hline
$+6$ & $6 + 8 = 14$ & $1110$ \\ \hline
$+1$ & $1 + 8 = 9$ & $1001$ \\ \hline
$0$ & $0 + 8 = 8$ & $\mathbf{1000}$ \\ \hline
$-1$ & $-1 + 8 = 7$ & $0111$ \\ \hline
$-7$ & $-7 + 8 = 1$ & $0001$ \\ \hline
$-8$ & $-8 + 8 = 0$ & $\mathbf{0000}$ \\ \hline
\end{tabular}
\end{center}

\begin{keybox}[Why Use Excess Notation?]
The key advantage of excess notation is that the \emph{ordering} of stored values matches the ordering of actual values when treated as unsigned integers. This makes comparison operations simpler in hardware-important for comparing floating-point exponents quickly.
\end{keybox}

\section{Binary Arithmetic}
\label{sec:binary-arithmetic}

\subsection{Binary Addition}

Binary addition follows the same principles as decimal addition, but with only two digits:
\begin{align*}
    0 + 0 &= 0 \\
    0 + 1 &= 1 \\
    1 + 0 &= 1 \\
    1 + 1 &= 10_2 \quad \text{(0 with carry 1)}
\end{align*}

\textbf{Example:} Add $00111001_2 + 01101101_2$ (57 + 109 in decimal)
\[
\begin{array}{r}
    \phantom{+}00111001 \\
    +01101101 \\
    \hline
    10100110
\end{array}
\]
Result: $10100110_2 = 166_{10}$ \checkmark

\subsection{Binary Subtraction via Two's Complement}

One of the elegant properties of two's complement is that subtraction can be performed using addition:
\[
A - B = A + (-B) = A + (\text{two's complement of } B)
\]

This means the same hardware circuit can perform both addition and subtraction.

\subsection{Overflow}
\label{subsec:overflow}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{overflow.png}
    \caption{Overflow occurs when the result of an arithmetic operation exceeds the representable range. When adding two 8-bit numbers that produce a 9-bit result, the leftmost bit is truncated, leading to an incorrect answer within the 8-bit representation.}
    \label{fig:overflow}
\end{figure}

\begin{rigour}[Definition: Overflow]
\textbf{Overflow} occurs when an arithmetic operation produces a result outside the representable range for the given number of bits. The result ``wraps around'' due to the fixed bit width.
\end{rigour}

\begin{redbox}[Detecting Overflow]
Overflow detection differs for signed and unsigned arithmetic:

\textbf{Unsigned overflow}: Occurs when there is a carry out of the most significant bit.

\textbf{Signed (two's complement) overflow}: Occurs when:
\begin{itemize}
    \item Adding two positive numbers yields a negative result, OR
    \item Adding two negative numbers yields a positive result
\end{itemize}
Equivalently: overflow occurs when the carry \emph{into} the sign bit differs from the carry \emph{out of} the sign bit.
\end{redbox}

\textbf{Example: Signed overflow with 4-bit two's complement}

Compute $7 + 3$:
\begin{itemize}
    \item $7 = 0111_2$
    \item $3 = 0011_2$
\end{itemize}
\[
\begin{array}{r}
    \phantom{+}0111 \\
    +0011 \\
    \hline
    1010
\end{array}
\]

The result $1010_2$ is interpreted in two's complement as:
\[
1010_2 = -1 \times 2^3 + 0 \times 2^2 + 1 \times 2^1 + 0 \times 2^0 = -8 + 2 = -6
\]

We added two positive numbers (7 and 3) and got a negative result ($-6$). This is \textbf{signed overflow}-the true answer (10) exceeds the maximum representable value (7) for signed 4-bit integers.

\begin{keybox}[Overflow in Programming]
Many programming languages do not automatically check for integer overflow:
\begin{itemize}
    \item In C/C++, signed overflow is \emph{undefined behaviour}; unsigned overflow wraps around
    \item Python's arbitrary-precision integers cannot overflow (but are slower)
    \item Java's integers overflow silently with wrap-around
\end{itemize}
Overflow bugs have caused serious software failures, including the Ariane 5 rocket explosion (1996).
\end{keybox}

\section{Floating-Point Representation}
\label{sec:floating-point}

Integers can only represent whole numbers. For fractional values and very large or very small numbers, we use floating-point representation.

\subsection{The Concept of Floating Point}

\begin{rigour}[Definition: Floating-Point Representation]
A \textbf{floating-point number} represents a real number in the form:
\[
(-1)^s \times m \times 2^e
\]
where:
\begin{itemize}
    \item $s$ is the \textbf{sign bit} (0 = positive, 1 = negative)
    \item $m$ is the \textbf{mantissa} (or significand), representing the significant digits
    \item $e$ is the \textbf{exponent}, determining the magnitude
\end{itemize}
\end{rigour}

This is analogous to scientific notation in decimal (e.g., $6.022 \times 10^{23}$), but in base 2.

\subsection{8-Bit Floating Point (Pedagogical Example)}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{binary fractions.png}
    \caption{Structure of an 8-bit floating-point format: 1 sign bit, 3 exponent bits (using excess-4 notation), and 4 mantissa bits. Real-world formats use more bits but follow the same principles.}
    \label{fig:float-structure}
\end{figure}

Consider a simplified 8-bit floating-point format:
\begin{itemize}
    \item \textbf{1 sign bit}: Position 7 (leftmost)
    \item \textbf{3 exponent bits}: Positions 4--6, using excess-4 notation
    \item \textbf{4 mantissa bits}: Positions 0--3
\end{itemize}

\subsubsection{Normalised Form and the Hidden Bit}

In \textbf{normalised} floating-point numbers, the mantissa is adjusted so that the leading digit (before the radix point) is always 1. Since this leading 1 is always present, it can be implicit (not stored), giving us an extra bit of precision ``for free.''

Thus, a stored mantissa of $1011$ actually represents $1.1011_2$.

\subsubsection{Conversion: Binary Floating Point to Decimal}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fraction examples.png}
    \caption{Examples of converting 8-bit floating-point representations to decimal values. The process involves extracting the sign, decoding the exponent (subtracting the bias), and reconstructing the mantissa with the implicit leading 1.}
    \label{fig:float-examples}
\end{figure}

\textbf{Example 1:} Convert $01101011_2$ to decimal.
\begin{enumerate}
    \item \textbf{Sign bit}: $0$ (positive)
    \item \textbf{Exponent bits}: $110_2 = 6_{10}$; subtract bias: $6 - 4 = 2$
    \item \textbf{Mantissa bits}: $1011$; with implicit leading 1: $1.1011_2$
    \item \textbf{Value}: $1.1011_2 \times 2^2 = 110.11_2$
\end{enumerate}

Converting $110.11_2$ to decimal:
\[
110.11_2 = 1 \times 2^2 + 1 \times 2^1 + 0 \times 2^0 + 1 \times 2^{-1} + 1 \times 2^{-2} = 4 + 2 + 0 + 0.5 + 0.25 = 6.75
\]

\textbf{Example 2:} Convert $10001000_2$ to decimal.
\begin{enumerate}
    \item \textbf{Sign bit}: $1$ (negative)
    \item \textbf{Exponent bits}: $000_2 = 0_{10}$; subtract bias: $0 - 4 = -4$
    \item \textbf{Mantissa bits}: $1000$; with implicit leading 1: $1.1000_2$
    \item \textbf{Value}: $-(1.1000_2 \times 2^{-4}) = -0.00011_2$
\end{enumerate}

Converting $-0.00011_2$ to decimal:
\[
-0.00011_2 = -(1 \times 2^{-4} + 1 \times 2^{-5}) = -(0.0625 + 0.03125) = -0.09375
\]

\subsubsection{Conversion: Decimal to Binary Floating Point}

\textbf{Example:} Convert $1\frac{1}{3}$ to our 8-bit format.

\textbf{Step 1: Convert to binary.}

$1\frac{1}{3} = 1.333\ldots$

For the fractional part, multiply by 2 repeatedly:
\begin{align*}
    0.333\ldots \times 2 &= 0.666\ldots \quad \to \quad 0 \\
    0.666\ldots \times 2 &= 1.333\ldots \quad \to \quad 1 \\
    0.333\ldots \times 2 &= 0.666\ldots \quad \to \quad 0 \\
    &\vdots \text{ (repeating)}
\end{align*}

So $1\frac{1}{3} = 1.\overline{01}_2$ (repeating pattern).

\textbf{Step 2: Normalise.}

Already normalised as $1.010101\ldots_2 \times 2^0$

\textbf{Step 3: Extract components.}
\begin{itemize}
    \item Sign: $0$ (positive)
    \item Exponent: $0$; stored as $0 + 4 = 4 = 100_2$
    \item Mantissa: First 4 bits after the decimal: $0101$ (truncated from $010101\ldots$)
\end{itemize}

\textbf{Result:} $0\,100\,0101_2 = 01000101_2$

\textbf{Verification:} Converting back: $1.0101_2 \times 2^0 = 1.3125_{10}$

Note the error: $1.333\ldots - 1.3125 = 0.020\overline{8}$. This demonstrates the inherent precision limitation of floating point.

\subsection{IEEE 754 Standard}

Real-world computing uses the IEEE 754 standard for floating-point arithmetic.

\begin{keybox}[IEEE 754 Formats]
\textbf{Single Precision (32-bit):}
\begin{itemize}
    \item 1 sign bit
    \item 8 exponent bits (excess-127)
    \item 23 mantissa bits (+1 implicit)
    \item Precision: 6--9 significant decimal digits
    \item Range: $\approx \pm 3.4 \times 10^{38}$
\end{itemize}

\textbf{Double Precision (64-bit):}
\begin{itemize}
    \item 1 sign bit
    \item 11 exponent bits (excess-1023)
    \item 52 mantissa bits (+1 implicit)
    \item Precision: 15--17 significant decimal digits
    \item Range: $\approx \pm 1.8 \times 10^{308}$
\end{itemize}
\end{keybox}

\subsection{Floating-Point Precision Issues}

\begin{redbox}[Floating-Point Gotchas]
Floating-point arithmetic has several counter-intuitive behaviours that can cause bugs:

\textbf{1. Representation error:} Many decimal fractions cannot be represented exactly in binary.
\begin{lstlisting}[language=Python]
>>> 0.1 + 0.2
0.30000000000000004
>>> 0.1 + 0.2 == 0.3
False
\end{lstlisting}

\textbf{2. Comparison failures:}
\begin{lstlisting}[language=Python]
>>> (3 - 2.9) <= 0.1
False  # 3 - 2.9 is slightly greater than 0.1!
\end{lstlisting}

\textbf{3. Absorption:} Adding a very small number to a very large number may have no effect.
\begin{lstlisting}[language=Python]
>>> 1e16 + 1 == 1e16
True  # The 1 is too small to affect the large number
\end{lstlisting}

\textbf{4. Non-associativity:} $(a + b) + c \neq a + (b + c)$ in floating point.
\end{redbox}

\begin{keybox}[Best Practices for Floating-Point Comparison]
Never compare floating-point numbers for exact equality. Instead:
\begin{lstlisting}[language=Python]
import math

# Use a tolerance (epsilon)
def approx_equal(a, b, rel_tol=1e-9, abs_tol=0.0):
    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)

# Or use math.isclose (Python 3.5+)
math.isclose(0.1 + 0.2, 0.3)  # True
\end{lstlisting}
\end{keybox}

\section{CPU Architecture}
\label{sec:cpu}

Understanding CPU architecture provides context for how algorithms are actually executed and why certain operations are faster than others.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{cpu.png}
    \caption{Simplified CPU architecture showing the main components: the Arithmetic Logic Unit (ALU), Control Unit, registers, and their connection to main memory via the system bus.}
    \label{fig:cpu}
\end{figure}

\subsection{CPU Components}

\subsubsection{Arithmetic Logic Unit (ALU)}

The ALU is the computational core of the CPU, performing:
\begin{itemize}
    \item \textbf{Arithmetic operations}: Addition, subtraction, multiplication, division
    \item \textbf{Logical operations}: AND, OR, NOT, XOR, and other bitwise operations
    \item \textbf{Comparison operations}: Testing equality, less than, greater than
\end{itemize}

\subsubsection{Control Unit}

The Control Unit orchestrates the operation of the CPU:
\begin{itemize}
    \item \textbf{Instruction Fetch}: Retrieves the next instruction from memory
    \item \textbf{Instruction Decode}: Interprets what operation the instruction requires
    \item \textbf{Execution Control}: Directs the ALU, manages data flow, and coordinates components
\end{itemize}

\subsubsection{Registers}

\begin{keybox}[Registers: The Fastest Storage]
\textbf{Registers} are small, extremely fast storage locations within the CPU itself. They hold:
\begin{itemize}
    \item Data currently being processed
    \item Memory addresses being accessed
    \item The program counter (address of the next instruction)
    \item Status flags (overflow, zero result, carry, etc.)
\end{itemize}

Register access is essentially instantaneous compared to memory access (typically 1 cycle vs.\ 100+ cycles for main memory).
\end{keybox}

\subsubsection{Memory Hierarchy}

Data moves between different levels of storage, each with different speed/capacity trade-offs:

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Level} & \textbf{Typical Size} & \textbf{Access Time} \\
\hline
Registers & 64--256 bytes & $<1$ ns \\
L1 Cache & 32--64 KB & 1--4 ns \\
L2 Cache & 256 KB -- 1 MB & 4--10 ns \\
L3 Cache & 4--50 MB & 10--40 ns \\
Main Memory (RAM) & 8--64 GB & 50--100 ns \\
SSD Storage & 256 GB -- 4 TB & 10--100 $\mu$s \\
HDD Storage & 1--10 TB & 1--10 ms \\
\hline
\end{tabular}
\end{center}

This hierarchy is crucial for algorithm performance: algorithms that access memory in predictable patterns (``cache-friendly'') can be orders of magnitude faster than those with scattered access patterns.

\subsection{The Stored-Program Concept}

\begin{rigour}[The von Neumann Architecture]
The \textbf{stored-program concept} (von Neumann architecture) is the fundamental principle that both \emph{data} and \emph{programs} are stored in the same memory as sequences of bits. This means:
\begin{itemize}
    \item Programs can be loaded and modified like any other data
    \item Programs can generate and execute other programs
    \item The same memory and processing hardware handles both code and data
\end{itemize}
\end{rigour}

An instruction in memory consists of:
\begin{itemize}
    \item \textbf{Opcode}: Specifies the operation (e.g., ADD, LOAD, STORE, JUMP)
    \item \textbf{Operands}: Specify the data or addresses to operate on
\end{itemize}

\subsection{The Machine Cycle}

The CPU operates in a continuous cycle:

\begin{enumerate}
    \item \textbf{Fetch}: Retrieve the next instruction from memory (address in program counter)
    \item \textbf{Decode}: Interpret the instruction to determine the operation and operands
    \item \textbf{Execute}: Perform the operation using the ALU or other components
    \item \textbf{Repeat}: Increment the program counter and return to step 1
\end{enumerate}

Modern CPUs use \textbf{pipelining} to overlap these stages for different instructions, and \textbf{multiple cores} to execute instructions in parallel, achieving billions of operations per second.

\section{Relevance to Data Structures and Algorithms}
\label{sec:relevance}

\begin{keybox}[Why This Matters]
Understanding low-level computing concepts is essential for DSA because:

\textbf{Space complexity analysis:}
\begin{itemize}
    \item An \texttt{int} typically uses 4 bytes; a \texttt{double} uses 8 bytes
    \item An array of $n$ integers uses $4n$ bytes plus overhead
    \item Pointer/reference sizes depend on architecture (4 or 8 bytes)
\end{itemize}

\textbf{Time complexity in practice:}
\begin{itemize}
    \item Array access is $O(1)$ because of random-access memory
    \item Cache-efficient algorithms can be 10--100x faster than cache-unfriendly ones with the same big-O complexity
    \item Integer arithmetic is faster than floating-point; bitwise operations are fastest
\end{itemize}

\textbf{Correctness:}
\begin{itemize}
    \item Integer overflow can cause incorrect results or security vulnerabilities
    \item Floating-point comparison requires tolerance-based approaches
    \item Character encoding matters for string algorithms
\end{itemize}
\end{keybox}

\section{Summary}

This chapter established the foundational concepts of how computers represent and process information:

\begin{itemize}
    \item \textbf{Bits and Logic Gates}: All computation reduces to Boolean operations on bits
    \item \textbf{Hexadecimal}: A compact notation for binary data
    \item \textbf{Memory}: Organised collections of bits with addresses; RAM provides $O(1)$ access
    \item \textbf{Text Representation}: ASCII for basic characters; Unicode for universal coverage
    \item \textbf{Integer Representation}: Unsigned for non-negative; two's complement for signed; excess notation for biased values
    \item \textbf{Binary Arithmetic}: Addition/subtraction with overflow considerations
    \item \textbf{Floating Point}: Approximate representation of real numbers with inherent precision limitations
    \item \textbf{CPU Architecture}: The fetch-decode-execute cycle operating on data in registers and memory
\end{itemize}

These concepts form the substrate upon which all data structures are built and all algorithms execute. A deep understanding of this level enables you to make informed decisions about data representation, predict performance characteristics, and avoid subtle bugs in numerical computation.
