% Week 9: Graphs
% Topics: Graph representation, BFS, DFS, Bipartite graphs, DAGs, Topological sorting, Flow networks, Ford-Fulkerson

\chapter{Graphs}
\label{ch:graphs}

\section{What is a Graph?}
\label{sec:graph-definition}

A graph \( G \) is a mathematical structure used to model pairwise relations between objects. It consists of two components:

\begin{enumerate}
    \item \textcolor{blue}{\textbf{Vertices (or nodes), \( V \)}}: These represent the objects in the graph.
    \item \textcolor{red}{\textbf{Edges, \( E \)}}: These are the connections or relationships between pairs of objects (nodes) in the graph.
\end{enumerate}

\( G = (\textcolor{blue}{V}, \textcolor{red}{E}) \) (a tuple object, with 2 lists)

\textbf{Size}:
\textcolor{blue}{ \( n = |V| \)}, \textcolor{red}{\( m = |E| \)}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/graph.png}
    \caption{An undirected graph with 8 vertices and 11 edges.}
    \label{fig:basic-graph}
\end{figure}

For the graph in Figure~\ref{fig:basic-graph}:
\begin{align*}
V &= \{1,2,3,4,5,6,7,8\} \\
E &= \{1 - 2, 1 - 3, 2 - 3, 2 - 4, 2 - 5, 3 - 5, 3 - 7, 3 - 8, 4 - 5, 5 - 6, 7 - 8\}\\
m &= 11\\
n &= 8
\end{align*}

\begin{rigour}[Graph - Formal Definition]
A \textbf{graph} is an ordered pair $G = (V, E)$ where:
\begin{itemize}
    \item $V$ is a set of \textbf{vertices} (or nodes)
    \item $E$ is a set of \textbf{edges}, which are 2-element subsets of $V$ (for undirected graphs) or ordered pairs from $V \times V$ (for directed graphs)
\end{itemize}

The \textbf{order} of a graph is $|V| = n$ (number of vertices).

The \textbf{size} of a graph is $|E| = m$ (number of edges).
\end{rigour}

Graphs can be used to model a wide variety of systems in many fields, including computer networks, social networks, biological networks, and transportation networks, among others.

Edges can be \textbf{directed} or \textbf{undirected}, representing the nature of the relationship (unidirectional or bidirectional), and can also have \textbf{weights} assigned to them, which can represent the strength or capacity of the connection.

\textit{Note: you can also use graphs to express measures of similarity as a network - e.g.\ kernels; strength of ties between units, which can then be analysed as a network.}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/graph2.png}
    \caption{A more complex graph illustrating various connection patterns.}
    \label{fig:complex-graph}
\end{figure}

\begin{table}[htbp]
    \centering
    \caption{Graph Types with Nodes and Edges - Real-World Examples}
    \label{tab:graph-examples}
    \begin{tabular}{p{3.5cm}p{4.5cm}p{6cm}}
        \toprule
        \textbf{Graph Type} & \textbf{Nodes} & \textbf{Edges} \\
        \midrule
        Communication & Telephone, Computer & Fibre Optic Cable \\
        Circuit & Gate, Register, Processor & Wire \\
        Mechanical & Joint, Rod, Beam, Spring & Physical connections \\
        Financial & Stock, Currency & Transactions \\
        Transportation & Street Intersection, Airport & Highway, Airway Route \\
        Internet & Class C Network & Connection \\
        Game & Board Position & Legal Move \\
        Social & Person, Actor & Friendship, Movie Cast \\
        Neural Network & Neuron & Synapse \\
        Protein Network & Protein & Protein-Protein Interaction \\
        Molecule & Atom & Bond \\
        \bottomrule
    \end{tabular}
\end{table}

%------------------------
\section{Representation of a Graph}
\label{sec:graph-representation}

There are two primary ways to represent a graph in a computer: the \textbf{adjacency matrix} and the \textbf{adjacency list}. The choice between them depends on the graph's characteristics and the operations you need to perform.

\subsection{Adjacency Matrix}
\label{subsec:adjacency-matrix}

An \(n \times n\) matrix used to represent a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph.

\begin{itemize}
    \item \textbf{Representation}: Each row and column represent a vertex. If there is an edge between vertex \(i\) and vertex \(j\), then the matrix entry \(A[i][j] = 1\); otherwise, \(A[i][j] = 0\).

    \[
    \begin{matrix}
     & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
    1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
    2 & 1 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
    3 & 1 & 1 & 0 & 0 & 1 & 0 & 1 & 1 \\
    4 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
    5 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 \\
    6 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    7 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 \\
    8 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\
    \end{matrix}
    \]

    This is the adjacency matrix for the graph in Figure~\ref{fig:basic-graph}.

    \item \textbf{Space Complexity}: The space complexity is \(O(n^2)\) regardless of the number of edges, making it \textbf{less space-efficient for sparse graphs} (graphs with few edges relative to the number of vertices).
    \item \textbf{Time Complexity}:
    \begin{itemize}
        \item Checking if an edge exists between two vertices is \(O(1)\), as you only need to look at the corresponding matrix element.
        \item Identifying all edges takes \(O(n^2)\) because you must inspect each entry in the matrix.
    \end{itemize}
\end{itemize}

\begin{keybox}[Adjacency Matrix Complexity]
    \textbf{Space:} $O(n^2)$

    \textbf{Check if edge exists:} $O(1)$

    \textbf{Enumerate all edges:} $O(n^2)$

    Best for: \textbf{dense graphs}, frequent edge queries
\end{keybox}

\subsection{Adjacency List}
\label{subsec:adjacency-list}

Represents a graph as a node-indexed array of lists. The index of the array represents a vertex and each element in its linked list represents the other vertices that form an edge with the vertex.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/representation list.png}
    \caption{Adjacency list representation of the graph from Figure~\ref{fig:basic-graph}. Each vertex maps to a list of its neighbours.}
    \label{fig:adjacency-list}
\end{figure}

\textit{Note: the order of the nodes within the linked lists is not important.}

\begin{itemize}
    \item \textbf{Representation}: The graph is represented as an array of lists (or linked lists). Each list at array index \(i\) contains a list of nodes that are adjacent to vertex \(i\).
    \item \textbf{Space Complexity}: The space complexity is \(O(m + n)\), where \(m\) is the number of edges and \(n\) is the number of vertices. This makes it \textbf{more space-efficient} than an adjacency matrix, \textbf{especially for sparse graphs}.
    \item \textbf{Time Complexity}:
    \begin{itemize}
        \item Checking if an edge exists between vertex \(i\) and \(j\) can take \(O(\text{degree}(i))\), as it requires a search through the list at index \(i\).
        \item Identifying all edges takes \(O(n + m)\) because each vertex and each edge is explored once.
    \end{itemize}
\end{itemize}

\textit{Additionally, as a linked list, it allows for \textbf{easy addition of new nodes} by simply adding a pointer, which can be done in constant time.}

\begin{keybox}[Adjacency List Complexity]
    \textbf{Space:} $O(n + m)$

    \textbf{Check if edge exists:} $O(\text{degree}(i))$

    \textbf{Enumerate all edges:} $O(n + m)$

    Best for: \textbf{sparse graphs}, graph traversal algorithms
\end{keybox}

\subsection{Choosing Between the Two}
\label{subsec:representation-choice}

\begin{keybox}[Graph Representation Summary]
\textbf{Adjacency Matrix} - use when:
\begin{itemize}
    \item Graph is \textbf{dense} (edge-to-vertex ratio is high)
    \item \textbf{Frequent edge existence checks} are needed ($O(1)$ lookup)
    \item Algorithm needs to \textbf{frequently access edge weights}
\end{itemize}

\textbf{Adjacency List} - use when:
\begin{itemize}
    \item Graph is \textbf{sparse} (few edges relative to vertices)
    \item Need to \textbf{explore edges of specific nodes} (e.g., BFS, DFS)
    \item Memory efficiency is important
\end{itemize}
\end{keybox}

\textbf{Adjacency Matrix}:
\begin{itemize}
    \item Preferable for \textbf{dense graphs} where the edge-to-vertex ratio is high.
    \item Better when \textbf{frequent edge existence checks are needed}, as these can be done in constant time.
    \item Simpler for implementing algorithms that need to \textbf{frequently access edge weights}, like many algorithms in numerical simulations and optimisations.
\end{itemize}

\textbf{Adjacency List}:
\begin{itemize}
    \item Preferable for \textbf{sparse graphs} with few edges relative to vertices.
    \item More space-efficient when the graph is large and mostly consists of empty connections.
    \item Often \textbf{faster for algorithms that explore the edges of specific nodes}, such as most graph traversal algorithms like \textbf{DFS and BFS}, especially when the node degree is much less than the number of nodes.
\end{itemize}

%------------------------
\section{Paths and Connectivity}
\label{sec:paths-connectivity}

\subsection{Paths}
\label{subsec:paths}

\begin{rigour}[Path - Formal Definition]
A \textbf{path} in an undirected graph is a sequence of vertices $v_1, v_2, \ldots, v_k$ such that for each pair of consecutive vertices $v_i$ and $v_{i+1}$, there is an edge $\{v_i, v_{i+1}\} \in E$.

A path is \textbf{simple} if each vertex appears at most once (no repeated vertices).

The \textbf{length} of a path is the number of edges it contains, which is $k-1$ for a path with $k$ vertices.
\end{rigour}

\begin{itemize}
    \item \textbf{Path}: A sequence of nodes connected by edges.
    \item \textbf{Simple Path}: A path where \textbf{each node is distinct} - i.e., it does not contain any repeated vertices. This excludes the possibility of looping back to a previously visited vertex within the same path.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/paths.png}
    \caption{A simple path through a graph - each vertex is visited at most once.}
    \label{fig:simple-path}
\end{figure}

\subsection{Connectivity}
\label{subsec:connectivity}

\begin{itemize}
    \item \textbf{Connected Graph}: An undirected graph is connected if there is \textbf{at least one path between any two} distinct nodes in the graph. This means that you can \textbf{start at any vertex and reach any other vertex} through some sequence of edges.
    \item \textbf{Components}: If an undirected graph is not connected, it can be decomposed into \textbf{connected components}. Each component is a maximal connected subgraph, meaning that within a component, any two vertices are connected by paths, and no additional edges or vertices outside of the component can be included without losing the property of connectivity.
\end{itemize}

\textbf{Properties of Paths and Connectivity}
\begin{itemize}
    \item \textbf{Fundamental Property}: In a connected graph, there exists at least one path between any two nodes. This property is crucial for many algorithms in graph theory, such as those used for network analysis, routing, and resource distribution.
    \item \textbf{Graph Traversals}: Depth-First Search (DFS) and Breadth-First Search (BFS) are two fundamental graph traversal techniques that can be used to explore all vertices and edges of a graph to determine connectivity, find connected components, or simply to visit all nodes for various computational purposes.
\end{itemize}

\subsection{Cycles}
\label{subsec:cycles}

\begin{rigour}[Cycle - Formal Definition]
A \textbf{cycle} is a path $v_1, v_2, \ldots, v_k$ where $v_1 = v_k$ (the path starts and ends at the same vertex).

A cycle is \textbf{simple} if it does not contain any repeated vertices or edges, except that the starting and ending vertex is the same, and $k \geq 4$ (at least 3 distinct vertices).
\end{rigour}

\begin{itemize}
    \item \textbf{Cycle}: A path that \textbf{starts and ends at the same node}.
    \item \textbf{Simple Cycle}: A cycle that \textbf{does not contain any repeated} vertices or edges, except that the starting and ending vertex is the same.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/cylce.png}
    \caption{A simple cycle - the path returns to its starting vertex without repeating any intermediate vertex.}
    \label{fig:simple-cycle}
\end{figure}

%------------------------
\section{Trees}
\label{sec:graph-trees}

A tree is an \textbf{undirected} graph that is \textbf{connected} and \textbf{acyclic}. This means that there is \textbf{exactly one path between any two vertices}, and no path leads back to its starting vertex except the trivial path.

From any point, once you start moving, you are \textbf{monotonically moving away} from your starting location - you cannot return without retracing your steps.

\begin{keybox}[Tree Characterisation]
Any \textbf{two} of these properties of an undirected graph $G$ imply the third:
\begin{enumerate}
    \item \textbf{Connected}: There is a path between every pair of vertices
    \item \textbf{Acyclic}: The graph contains no cycles
    \item \textbf{$n-1$ edges}: A graph with $n$ vertices has exactly $n-1$ edges
\end{enumerate}
\end{keybox}

\subsection{Properties of Trees}
\label{subsec:tree-properties}

\begin{itemize}
    \item \textbf{Connected}: There is a path between every pair of vertices in the tree. No vertex is isolated.
    \item \textbf{Acyclic}: The graph does not contain any cycles, ensuring there is only one path between any two nodes.
    \item \textbf{Edge Count $n-1$}: A tree with \( n \) vertices always has \( n - 1 \) edges.
    \begin{itemize}
        \item This is the minimal number of edges needed to connect all vertices without creating a cycle, and it is a key property that helps in characterising trees.
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/tree.png}
    \caption{A tree - an undirected, connected, acyclic graph.}
    \label{fig:tree}
\end{figure}

\subsection{Types of Trees}
\label{subsec:tree-types}

\begin{itemize}
    \item \textbf{Rooted Trees}: Trees can also be discussed as rooted, where one vertex is designated as the root, and the directionality can be implied (though not explicitly directed) from the root outward. This hierarchy allows for representing structures like organisational charts, decision trees, and family trees.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{week09/rooted trees.png}
        \caption{Rooted trees - the same tree structure with different vertices designated as the root.}
        \label{fig:rooted-trees}
    \end{figure}

    Applications of rooted trees include:
    \begin{itemize}
        \item Decision trees
        \item Binary search trees
        \item GUI architectures
        \item Library Dewey Decimal system
        \item (many other applications)
    \end{itemize}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{week09/rooted tree gui.png}
        \caption{GUI hierarchy as a rooted tree; this is how HTML hierarchy works - each element is a node with parent-child relationships.}
        \label{fig:gui-tree}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{week09/dewy decimal.png}
        \caption{Dewey Decimal system as a rooted tree - hierarchical classification of knowledge.}
        \label{fig:dewey-tree}
    \end{figure}

    \item \textbf{Binary Trees}: A popular type of rooted tree where each node has at most two children. This is widely used in computer science, particularly for binary search trees and binary heaps.

    \item \textbf{Spanning Trees}: A spanning tree of a graph is a subgraph that includes all the vertices of the original graph, is a tree, and is connected. This concept is crucial in network design and optimisation algorithms, ensuring minimal connection without redundancy.
\end{itemize}

\subsection{Applications of Trees}
\label{subsec:tree-applications}

\begin{itemize}
    \item \textbf{Data Structures}: Trees form the basis of several fundamental data structures, including binary search trees, heaps, and B-trees, all crucial for efficient searching, sorting, and indexing operations.
    \item \textbf{Network Routing}: Tree structures are utilised in routing algorithms to ensure efficient and cycle-free paths for data communication.
    \item \textbf{Algorithm Design}: Trees are used in a variety of algorithms, particularly those that involve hierarchical structures, such as recursive algorithms and divide-and-conquer strategies.
\end{itemize}

%------------------------
\section{$s$-$t$ Connectivity Problems}
\label{sec:st-connectivity}

These problems focus on the existence and properties of paths between two specific nodes in a graph.

\subsection{$s$-$t$ Connectivity Problem}
\label{subsec:st-connectivity-problem}

\begin{itemize}
    \item \textbf{Problem Statement}: Given two nodes, \(s\) and \(t\), determine whether there is a path connecting them.
    \item \textbf{Purpose}: This problem helps in understanding whether two points in a network are accessible to each other.
    \item \textbf{Methods of Solution}:
    \begin{itemize}
        \item \textbf{Depth-First Search (DFS)} or \textbf{Breadth-First Search (BFS)} can be used in an unweighted graph to find whether a path exists between \(s\) and \(t\).
        \item These methods will traverse the graph starting from node \(s\) and attempt to reach node \(t\), confirming connectivity if \(t\) is reached.
    \end{itemize}
\end{itemize}

\subsection{$s$-$t$ Shortest Path Problem}
\label{subsec:st-shortest-path}

\begin{itemize}
    \item \textbf{Problem Statement}: Given two nodes, \(s\) and \(t\), determine the shortest path between them, typically in terms of the least number of edges (unweighted) or minimum path weight (weighted).
    \item \textbf{Purpose}: This problem is critical for routing and navigation, where the goal is to find the most efficient route between points.
    \item \textbf{Methods of Solution}:
    \begin{itemize}
        \item \textbf{BFS}: For unweighted graphs, BFS finds the shortest path in terms of number of edges.
        \item \textbf{Dijkstra's Algorithm}: Efficient for finding the shortest paths from a single source node \(s\) to all other nodes in a graph with non-negative edge weights.
        \item \textbf{Bellman-Ford Algorithm}: Handles graphs with negative weights and computes the shortest paths from a single source \(s\) to all other vertices.
        \item \textbf{Floyd-Warshall Algorithm}: Computes shortest paths between all pairs of vertices, useful for dense graphs.
        \item \textbf{A* Search Algorithm}: Utilises heuristics to dramatically speed up the search for a shortest path, useful in practical applications like GPS navigation.
    \end{itemize}
\end{itemize}

\subsection{Applications of $s$-$t$ Connectivity Problems}
\label{subsec:st-applications}

\begin{itemize}
    \item \textbf{Solving a Maze}: This can be seen as an \(s\)-\(t\) connectivity problem where \(s\) is the start of the maze and \(t\) is the exit. The solution involves finding a path through the maze from start to finish.
    \item \textbf{Finding a Route}: Similar to solving a maze but typically involves navigating a network, such as road maps for vehicle routing or links in a network topology.
    \item \textbf{Calculating Kevin Bacon Numbers}: This is an application of the \(s\)-\(t\) shortest path problem in a social network where actors are vertices and edges are films they have appeared in together. The Kevin Bacon number of an actor is the shortest path length to Kevin Bacon in this network.
\end{itemize}

%------------------------
\section{Breadth-First Search (BFS)}
\label{sec:bfs}

\subsection{BFS Algorithm Overview}
\label{subsec:bfs-overview}

The BFS algorithm starts from the source node \( s \), exploring all its immediate neighbours, then for each of those neighbours, it explores their unvisited neighbours, and so on. It proceeds level by level, hence exploring the graph in ``waves.''

\begin{keybox}[BFS Key Properties]
\begin{itemize}
    \item Explores graph \textbf{level by level} (by distance from source)
    \item Uses a \textbf{queue} (FIFO) data structure
    \item Finds \textbf{shortest paths} in unweighted graphs
    \item Time complexity: $O(n + m)$
    \item Space complexity: $O(n)$ for the visited array and queue
\end{itemize}
\end{keybox}

\subsubsection{BFS Algorithm Steps}
\label{subsubsec:bfs-steps}

\begin{enumerate}
    \item \textbf{Initialisation}: Start with a queue that contains only the source node \( s \) and mark \( s \) as visited.
    \item \textbf{Exploration}:
    \begin{itemize}
        \item Dequeue an element from the front of the queue (say \( v \)).
        \item Visit all unvisited neighbours of \( v \), mark them as visited, and enqueue them.
        \item Repeat until the queue is empty.
    \end{itemize}
\end{enumerate}

\subsubsection{Level Sets in BFS}
\label{subsubsec:bfs-levels}

\begin{itemize}
    \item \textbf{Level Set \( L_0 \)}: Contains only source node \( s \).
    \item \textbf{Level Set \( L_1 \)}: Contains all neighbours of \( L_0 \).
    \item \textbf{Level Set \( L_2 \)}: Contains all nodes that are two edges away from \( s \). These nodes are the neighbours of nodes in \( L_1 \) that have not been visited before or included in previous levels.
    \begin{itemize}
        \item Formally: all nodes that do not belong to $L_0 \cup L_1$, and have an edge to a node in $L_1$
    \end{itemize}
    \item \textbf{Level Set $L_{i+1}$}: All nodes that do not belong to $\bigcup_{j \leq i} L_j$, and have an edge to a node in $L_i$
\end{itemize}

Each subsequent level set \( L_i \) contains all nodes that are exactly \( i \) edges away from \( s \).

\begin{rigour}[BFS Correctness]
\textbf{Theorem}: There is a path from $s$ to $t$ if and only if $t$ appears in some level set $L_i$.

\textbf{Proof idea}: BFS systematically explores all reachable nodes by expanding outwards from $s$. Every node connected to $s$ will eventually be discovered. Conceptualise this as a contagion spreading through the network - every connected node will be reached.
\end{rigour}

\subsection{Properties and Benefits of BFS}
\label{subsec:bfs-properties}

\begin{itemize}
    \item \textbf{Path Existence (Correctness)}: There is a path from \( s \) to \( t \) if and only if \( t \) appears in one of the level sets during the BFS execution.
    \item \textbf{Shortest Path Guarantee}: BFS guarantees that the first time a node is visited, the path to that node from \( s \) is the shortest possible path in terms of the number of edges (this only applies to unweighted graphs).
    \item \textbf{Completeness}: BFS is complete, meaning that if there is a solution or if the node \( t \) is reachable from \( s \), BFS will definitely find it.
    \item \textbf{Time Complexity}: The time complexity of BFS is \( O(n + m) \), where \( n \) is the number of vertices and \( m \) is the number of edges in the graph. This is because each vertex and each edge will be explored in the worst-case scenario.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/BFS.png}
    \caption{BFS exploration pattern - nodes are visited in order of their distance from the source.}
    \label{fig:bfs-pattern}
\end{figure}

\subsection{Analysis of BFS}
\label{subsec:bfs-analysis}

\begin{rigour}[BFS Tree Property]
Let $T$ be a BFS tree of $G=(V,E)$, and let $x$-$y$ be an edge of $G$. Then the \textbf{levels of $x$ and $y$ differ by at most 1}.

This property ensures:
\begin{itemize}
    \item The BFS tree is correctly structured according to shortest path distances
    \item Each edge connects vertices at the same level or adjacent levels
    \item The path from $s$ to any vertex in the BFS tree is a shortest path
\end{itemize}
\end{rigour}

\textbf{Implications of the Property}
\begin{itemize}
    \item \textbf{Ensures BFS Tree Structure}: This property ensures that the BFS tree is correctly structured according to the shortest path distances from the source vertex. Each edge in the original graph connects vertices that are either at the same level or at adjacent levels.
    \item \textbf{Shortest Path}: Because each vertex is visited at its earliest possible level, the path from the source vertex to any other vertex in the BFS tree is the shortest path in terms of the number of edges.
    \item \textbf{Graph Analysis}: This characteristic of BFS is particularly useful in applications such as finding the shortest path in unweighted graphs, analysing the structure of networks, and more.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{week09/bfs.png}
    \caption{BFS tree structure: the solid black lines show the BFS-discovered shortest paths between $s$ and other vertices; the dotted lines show other edges in the graph that connect vertices within or between adjacent levels.}
    \label{fig:bfs-tree}
\end{figure}

\subsubsection{Time Complexity of BFS}
\label{subsubsec:bfs-complexity}

BFS runs in \( O(m + n) \) time, if the graph is given by its adjacency list representation, where:
\begin{itemize}
    \item \( m \) is the number of edges
    \item \( n \) is the number of vertices
\end{itemize}

This complexity is derived from the way BFS explores each vertex and edge of the graph exactly once in the case of an adjacency list representation.

\textbf{Adjacency List Representation}:
\begin{itemize}
    \item \textbf{Vertex Exploration}: Each vertex is enqueued and dequeued exactly once. The enqueuing and dequeuing operations take constant time, i.e., \( O(1) \), and since there are \( n \) vertices, this part of the algorithm takes \( O(n) \) time.
    \item \textbf{Edge Exploration}: Each edge in the graph is considered exactly once during the entire run of BFS. When a vertex is dequeued, BFS examines each adjacent vertex connected by an edge. Since each edge is examined exactly once and there are \( m \) edges, this part of the algorithm takes \( O(m) \) time.
\end{itemize}

\textbf{Detailed Breakdown}
\begin{enumerate}
    \item \textbf{Initialisation}: BFS initialises a queue and a boolean array (or similar structure) to keep track of visited vertices, which takes \( O(n) \) time.
    \item \textbf{Processing Vertices}: As BFS processes each vertex exactly once, and each vertex enqueue/dequeue operation takes \( O(1) \), the total time taken for all vertices is \( O(n) \).
    \item \textbf{Processing Edges}: Since the adjacency list of each vertex contains all its adjacent vertices (edges), and each edge in the graph is examined exactly once throughout the BFS run, the total time taken for all edges is \( O(m) \).
\end{enumerate}

\begin{tcolorbox}[title=Why $O(n+m)$?]
    \begin{enumerate}
        \item Consider node $i$: there are $\text{degree}(i)$ connected edges
        \item Total time processing edges is $\sum_{i \in V} \text{degree}(i) = 2m$
        \begin{itemize}
            \item \textbf{Each edge $i$-$j$ is counted twice}: once in $\text{degree}(i)$ and once in $\text{degree}(j)$
        \end{itemize}
        \item Even when $m=1$, we still have to initialise and check each vertex, giving $O(n)$
    \end{enumerate}
\end{tcolorbox}

\textbf{Why Each Edge is Counted Twice}

In an undirected graph represented by an adjacency list, each edge is stored twice: once in the list of each of the two vertices it connects.

For example, if there is an edge between vertices \( i \) and \( j \), this edge will appear in the adjacency list of \( i \) and in the adjacency list of \( j \). Therefore, summing degrees gives \( 2m \) because each edge contributes 2 to the total degree count.

\begin{tcolorbox}
    In \textbf{sparse graphs}, where the number of edges \( m \) is much less than the possible maximum \( \frac{n(n-1)}{2} \) for undirected graphs, the \( n \) term can dominate because you must account for potentially disconnected vertices or those with fewer connections.

    In \textbf{dense graphs}, as \( m \) approaches \( \frac{n(n-1)}{2} \), the number of edges \( m \) significantly influences the time complexity, reflecting the extensive connectivity among the vertices.
\end{tcolorbox}

\textbf{Practical Implications}
\begin{itemize}
    \item \textbf{Efficiency}: BFS is particularly efficient on \textbf{sparse graphs} where \( m \) is much less than \( n^2 \) (the number of edges in a complete graph). \textbf{In such cases, \( O(m + n) \) (from using BFS on an adjacency list) is a better bound than \( O(n^2) \), which you might get using an adjacency matrix}.
    \item \textbf{Applications}: BFS is used in scenarios requiring the exploration of all vertices reachable from a certain vertex, finding shortest paths in unweighted graphs, and in various algorithms requiring level order traversal (like in networking, pathfinding in games, and more).
\end{itemize}

\subsection{Example Applications of BFS}
\label{subsec:bfs-applications}

\subsubsection{Flood Fill}
\label{subsubsec:flood-fill}

Given an image, colour a contiguous region of one colour into another colour.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/flood fill.png}
    \caption{Flood fill algorithm - colouring a contiguous region. Each pixel is a node; edges connect adjacent pixels of the same colour.}
    \label{fig:flood-fill}
\end{figure}

\begin{itemize}
    \item \textbf{Node}: pixel
    \item \textbf{Edge}: exists if two adjacent pixels are the same colour
    \item \textbf{Blob}: connected component of pixels of the same colour
\end{itemize}

\subsubsection{K-Nearest Neighbours Classification}
\label{subsubsec:knn-bfs}

Identifying some region where all the labels are the same for any test point in that region. It would have the same classification, so you can think of this as a flood fill over the decision regions.

\subsubsection{Application in Experimental Design}
\label{subsubsec:experimental-design}

\begin{enumerate}
  \item \textbf{Systematic Sampling}: Starting from a randomly chosen unit, BFS can be used to generate a tree where each level represents a set of units that are equidistant from the starting point. This method ensures that all units within the same level are similarly distant from the starting conditions, potentially controlling for confounding variables related to proximity or similarity.

  \item \textbf{Uniform Distribution}: By treating each level set \( L_i \) with the same experimental condition (either treatment or control), researchers can ensure that the assignment is not biased by the network structure of the data. This is particularly useful in cases where the connectivity of units might influence the outcome (e.g., in social networks or spatially distributed units).

  \item \textbf{Coverage and Separation}: BFS ensures that treatment is distributed broadly across the network, covering diverse connections and reducing the risk of clustered treatment effects. This spread helps in evaluating the treatment's impact across a variety of contexts and connections between units.

  \item \textbf{Graph-Based Data Analysis}: In settings where the units are connected in a graph-like structure (e.g., social networks, geographic locations with adjacency, etc.), using BFS helps to understand how treatments might propagate through the network, which is useful for understanding diffusion of effects, peer influence, and network externalities.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{week09/bfs app.png}
    \caption{BFS for experimental design: start at randomly allocated $s$; then each level set is coloured in reference to that point - this gives good coverage of treatment across the network structure.}
    \label{fig:bfs-experimental}
\end{figure}

\subsubsection{Explore All Connected Components}
\label{subsubsec:connected-components}

To find all nodes connected to $s$:
\begin{itemize}
    \item \textbf{BFS}: explores by distance from source
    \item \textbf{DFS}: explores as far as possible along each branch before backtracking
\end{itemize}

Both methods will find all connected nodes; the difference is in the order of exploration.

%------------------------
\section{Depth-First Search (DFS)}
\label{sec:dfs}

\textit{Keep getting distracted by the first undiscovered thing you find.}

Both BFS and DFS explore the whole graph; the difference is in the order in which they do so. Neither is inherently more efficient - it depends on what you want to achieve. In general, BFS is more useful when shortest paths matter.

The key data structure for DFS is the \textbf{stack}:
\begin{itemize}
    \item As you traverse, you add other connected vertices to the stack
    \item If you reach a vertex with no unvisited adjacent vertices, you backtrack by popping the stack (or returning from a recursive call) to explore the next vertex
\end{itemize}

\textit{Whereas BFS leverages the queue.}

\subsection{DFS Outline}
\label{subsec:dfs-outline}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/DFS.png}
    \caption{DFS exploration pattern - the algorithm goes as deep as possible before backtracking.}
    \label{fig:dfs-pattern}
\end{figure}

\subsubsection{Recursive Definition}
\label{subsubsec:dfs-recursive}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/recursive dfs.png}
    \caption{Recursive DFS - the call stack implicitly manages the traversal order.}
    \label{fig:dfs-recursive}
\end{figure}

\begin{verbatim}
function DFS(vertex v)
    mark v as visited
    visit v

    for each vertex u adjacent to v
        if u is not visited
            DFS(u)
\end{verbatim}

\subsubsection{Non-Recursive Definition}
\label{subsubsec:dfs-iterative}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/non recursive dfs.png}
    \caption{Non-recursive (iterative) DFS using an explicit stack.}
    \label{fig:dfs-iterative}
\end{figure}

\begin{verbatim}
function DFS(start_vertex)
    create a stack
    push start_vertex onto the stack
    mark start_vertex as visited

    while stack is not empty
        v = pop the stack
        visit v

        for each vertex u adjacent to v
            if u is not visited
                mark u as visited
                push u onto the stack
\end{verbatim}

The algorithm starts at a selected node (the root in the case of trees, or any node in the case of graphs) and explores as far as possible along each branch before backtracking:

\begin{itemize}
  \item Visit an adjacent, unvisited vertex, mark it as visited, and push it on a stack (or move into it recursively).
  \item Continue moving to an adjacent unvisited vertex, marking new vertices and pushing them onto the stack as you go.
  \item If you reach a vertex with no unvisited adjacent vertices, you backtrack by popping the stack (or returning from a recursive call) to explore the next vertex.
\end{itemize}

\subsection{Properties of DFS}
\label{subsec:dfs-properties}

\begin{keybox}[DFS Key Properties]
\begin{itemize}
    \item Explores graph by going \textbf{as deep as possible} before backtracking
    \item Uses a \textbf{stack} (LIFO) - either explicit or via recursion
    \item Time complexity: $O(n + m)$ (same as BFS)
    \item Space complexity: $O(n)$ in the worst case (path length)
    \item More memory-efficient than BFS for sparse, deep graphs
\end{itemize}
\end{keybox}

\begin{itemize}
    \item \textbf{Memory Efficiency}: DFS uses less memory than BFS in sparse graphs because it holds a single path from the root node rather than storing all the nodes at a given depth level.
    \item \textbf{Path Finding}: DFS is often used when we need to find a path in the graph, especially if the path is likely to be deep.
    \item \textbf{Complexity (same as BFS)}: \(O(n + m)\), where \(n\) is the number of vertices and \(m\) is the number of edges in the graph. This is because each vertex and each edge is explored exactly once.
\end{itemize}

\subsection{Applications of DFS}
\label{subsec:dfs-applications}

\begin{itemize}
    \item \textbf{Topological Sorting}: DFS is useful for ordering vertices such that every directed edge from vertex \( u \) to vertex \( v \) implies that \( u \) comes before \( v \) in the ordering.
    \item \textbf{Cycle Detection}: DFS can be used to detect cycles in a graph, which is important in many applications, including detecting deadlocks in concurrent systems.
    \item \textbf{Component Finding}: DFS helps in identifying the connected components in a graph.
    \item \textbf{Solving Puzzles}: Such as mazes or logical problems where exploring to the deepest level might yield a solution.
\end{itemize}

%------------------------
\section{BFS vs DFS}
\label{sec:bfs-vs-dfs}

DFS is particularly useful in applications such as topological sorting, detecting cycles, and finding strongly connected components. It is also used to simulate scenarios where complete exploration of one path or option is needed before moving to another (e.g., solving mazes, puzzle games).

\begin{keybox}[When to Use BFS vs DFS]
\textbf{Use BFS when:}
\begin{itemize}
    \item Task requires exploring the graph \textbf{level by level}
    \item \textbf{Shortest path} or minimum moves are of interest
    \item Solution is likely \textbf{close to the starting point}
\end{itemize}

\textbf{Use DFS when:}
\begin{itemize}
    \item \textbf{Complete exploration} of a path is necessary before alternatives (backtracking)
    \item \textbf{Memory is constrained} (DFS can be more space-efficient)
    \item Graph is \textbf{deep} and solutions are potentially \textbf{far from root}
    \item Need to detect cycles or perform topological sorting
\end{itemize}
\end{keybox}

\subsection{Intuitive Differences}
\label{subsec:bfs-dfs-intuition}

\textit{Here we consider the recursive form of DFS:}
\begin{itemize}
    \item \textbf{BFS}:
    \begin{itemize}
        \item BFS explores all the nodes at the present ``depth'' (distance from the start node) before moving on to nodes at the next depth level.
        \item It expands outward from the starting point and is implemented using a \textbf{queue (First In, First Out - FIFO)}. This ensures that nodes are explored in the order they are discovered.
        \item Think of it as going in waves at each level set: filling up the queue $\rightarrow$ emptying the queue $\rightarrow$ filling up the queue $\rightarrow$ emptying the queue
    \end{itemize}

    \item \textbf{DFS}:
    \begin{itemize}
        \item In contrast, DFS dives as deep as possible into the graph before backtracking to explore other paths.
        \item It is typically implemented using a \textbf{stack (Last In, First Out - LIFO)}, or through \textbf{recursion, which inherently uses a call stack}.
        \item This means DFS will follow a path from the starting node to an end node, then backtrack and explore other paths from previously visited nodes.
        \item It adds all the adjacent nodes to a stack but doesn't explore them until it has reached the end of its current path.
    \end{itemize}
\end{itemize}

\subsection{Key Differences in Operation}
\label{subsec:bfs-dfs-operation}

\begin{enumerate}
    \item \textbf{Node Exploration Order}:
    \begin{itemize}
        \item \textbf{BFS}: Nodes are explored in layers based on their distance from the start node. It uses a queue to keep track of the next node to explore. BFS adds all adjacent nodes to the queue and explores the oldest node first, ensuring that the distance from the start node increases gradually.
        \item \textbf{DFS}: Nodes are explored as deeply as possible before backtracking. Using a stack (or recursion), DFS adds one adjacent node and moves deeper immediately, exploring as far as possible along each branch before backtracking.
    \end{itemize}

    \item \textbf{Properties and Use Cases}:
    \begin{itemize}
        \item \textbf{BFS} is particularly useful for finding the shortest path in unweighted graphs and for scenarios where you need to explore all options uniformly, like in the case of level-wise processing required in algorithms like the minimum spanning tree or for layer-by-layer computation in neural networks.
        \item \textbf{DFS} is advantageous in scenarios involving exhaustive searches, as in puzzle games (e.g., solving mazes or the Knight's tour problem) where a solution involves exploring all possible configurations. DFS is also useful for topological sorting, and in scenarios where space complexity might be a constraint since iterative DFS can be more space-efficient than BFS.
    \end{itemize}

    \item \textbf{Performance}:
    \begin{itemize}
        \item \textbf{Time complexity is the same: \(O(m + n)\)}, where \(m\) is the number of edges, and \(n\) is the number of vertices. However, the actual performance may depend on the graph's structure and the specific needs of the application, such as whether the problem requires visiting every node or just ensuring a particular condition is met (like finding a cycle).
        \item \textbf{Space complexity differs: iterative DFS can be slightly more space-efficient than BFS}, especially in sparse graphs where backtracking happens frequently and the stack depth remains relatively small compared to the number of nodes that would have to be kept in the queue for BFS.
    \end{itemize}
\end{enumerate}

\subsection{Practical Implications}
\label{subsec:bfs-dfs-practical}

The choice between BFS and DFS can significantly impact the performance and correctness of graph-based algorithms depending on the specific requirements of the application:
\begin{itemize}
    \item \textbf{BFS} might be more suitable for networking applications or GPS navigation systems where finding the shortest route is essential.
    \item \textbf{DFS} could be more appropriate for applications involving the exploration of complex branchings, such as syntax tree exploration in compilers or backtracking algorithms in constraint satisfaction problems.
\end{itemize}

%------------------------
\section{Bipartite Graphs}
\label{sec:bipartite}

A bipartite graph is a special class of graph where the set of vertices can be divided into \textbf{two disjoint subsets} such that \textbf{no two vertices within the same subset are adjacent}.

This type of graph can be coloured using just two colours:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/bipartite1.png}
    \caption{An uncoloured bipartite graph - can you see how to partition the vertices into two groups?}
    \label{fig:bipartite-uncoloured}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{week09/bipartite2.png}
    \caption{The same graph with a valid 2-colouring - vertices are partitioned into red and blue sets with no edges within either set.}
    \label{fig:bipartite-coloured}
\end{figure}

\subsection{Properties of Bipartite Graphs}
\label{subsec:bipartite-properties}

\begin{keybox}[Bipartite Graph Characterisation]
A graph $G$ is bipartite if and only if:
\begin{enumerate}
    \item It can be \textbf{2-coloured} (no adjacent vertices share a colour)
    \item It contains \textbf{no odd-length cycles}
    \item Its vertices can be partitioned into two sets $U$ and $V$ where every edge connects a vertex in $U$ to one in $V$
\end{enumerate}

\textbf{Important}: Trees are always bipartite (they have no cycles at all).
\end{keybox}

\begin{itemize}
    \item \textbf{Two-Colour Property}: A graph is bipartite if and only if it can be coloured using two colours such that no two adjacent vertices share the same colour.
    \begin{itemize}
        \item This implies that the graph does \textbf{not contain any odd-length cycles}, which would require at least three colours to colour the graph properly.
    \end{itemize}

    \item \textbf{Subsets}: If you can split the graph's vertices into two groups \(U\) and \(V\) such that \textbf{every edge connects a vertex in \(U\) to one in \(V\)} (and no edge connects vertices within the same group), the graph is bipartite.

    \item \textbf{Trees are always bipartite}: Because trees are acyclic, you can always alternate colours along any path from the root, ensuring that no two adjacent vertices share the same colour.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{week09/bipartite (2 colourable).png}
    \caption{A bipartite graph - successfully 2-coloured.}
    \label{fig:bipartite-2col}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{week09/non-bipartite (not 2 colourable).png}
    \caption{A non-bipartite graph - contains an odd-length cycle (triangle), making 2-colouring impossible.}
    \label{fig:non-bipartite}
\end{figure}

\subsection{Checking if a Graph is Bipartite}
\label{subsec:bipartite-check}

You can determine whether a graph is bipartite using graph traversal techniques like Depth-First Search (DFS) or Breadth-First Search (BFS):

\begin{enumerate}
    \item \textbf{Start from any vertex} and colour it with one colour.
    \item \textbf{Use BFS/DFS} to traverse the graph:
    \begin{itemize}
        \item When visiting a vertex, colour it with the alternate colour of its parent.
        \item \textit{If you find a vertex that has already been coloured with the same colour as the current vertex, the graph is not bipartite}.
    \end{itemize}
    \item \textbf{Continue} until all vertices are coloured or a contradiction is found.
\end{enumerate}

\begin{redbox}[Bipartite Detection]
A graph is \textbf{not bipartite} if and only if it contains an \textbf{odd-length cycle}.

During BFS/DFS traversal, if you ever find an edge connecting two vertices at the \textbf{same level} (same colour), the graph contains an odd cycle and is not bipartite.
\end{redbox}

\subsection{Applications of Bipartite Graphs}
\label{subsec:bipartite-applications}

\begin{enumerate}
    \item \textbf{Stable Matching}:
    \begin{itemize}
        \item In scenarios like the stable marriage problem or matching medical residents to hospitals, bipartite graphs are used to model preferences on both sides (e.g., applicants and hospitals), ensuring that matches are stable and no two individuals would prefer each other over their current matches.
    \end{itemize}

    \item \textbf{Scheduling}:
    \begin{itemize}
        \item Bipartite graphs are ideal for scheduling problems where you need to match two distinct groups, such as machines and jobs, ensuring that no machine is overloaded or that prerequisites are respected.
    \end{itemize}

    \item \textbf{Causal Inference}:
    \begin{itemize}
        \item In experimental design and statistics, causal inference models can use bipartite graphs to differentiate between treatment and control groups, ensuring that the influence of treatments can be distinctly measured without interference.
    \end{itemize}

    \item \textbf{Network Flows}:
    \begin{itemize}
        \item Many network flow problems, such as the maximum flow problem, can be represented as bipartite graphs, where the flow from a source to a sink through intermediary nodes (representing two distinct sets) needs to be maximised.
    \end{itemize}

    \item \textbf{Data Clustering}:
    \begin{itemize}
        \item In data science, clustering algorithms can use bipartite graphs to categorise data into two groups based on different attributes or relationships, useful in recommendation systems or for segmenting markets.
    \end{itemize}
\end{enumerate}

\subsection{BFS and Bipartiteness}
\label{subsec:bfs-bipartite}

BFS naturally layers a graph by levels of distance from a starting node - this structure can determine if a graph is bipartite by checking for the presence of odd-length cycles.

\begin{itemize}
    \item \textbf{Initialisation}:
    \begin{itemize}
        \item Start BFS from node \( i \), marking it with one colour (say, red).
        \item Initialise a queue and enqueue the starting node.
    \end{itemize}

    \item \textbf{BFS Traversal}:
    \begin{itemize}
        \item While the queue is not empty, dequeue a node \( u \).
        \item For each unvisited adjacent node \( v \) of \( u \):
        \begin{itemize}
            \item If \( v \) is not coloured, assign it the opposite colour of \( u \) and enqueue \( v \).
            \item If \( v \) is already coloured and shares the same colour as \( u \), then the graph cannot be bipartite.
        \end{itemize}
    \end{itemize}

    \item \textbf{Layer Formation}:
    \begin{itemize}
        \item As BFS proceeds, it forms layers \( L_1, L_2, \ldots, L_k \) where each layer \( L_j \) contains all nodes that are \( j \) edges away from the starting node \( i \).
        \item No two nodes within the same layer should be connected by an edge if the graph is bipartite.
    \end{itemize}
\end{itemize}

\subsection{Analysing the Results}
\label{subsec:bipartite-analysis}

Let $G$ be the graph, and let $L_0, L_1, \ldots, L_k$ be the layers produced by BFS starting at node $i$.

\begin{rigour}[Bipartiteness via BFS]
\textbf{Theorem}: A graph $G$ is bipartite if and only if there are no intra-layer edges in any BFS traversal.

\textbf{If no intra-layer edges exist} $\Rightarrow$ $G$ is bipartite:
\begin{itemize}
    \item All edges connect nodes from adjacent layers only
    \item This alignment confirms the graph is bipartite because it effectively assigns two colours such that no two adjacent nodes share the same colour
    \item No odd-length cycles can exist
\end{itemize}

\textbf{If an intra-layer edge exists} $\Rightarrow$ $G$ is not bipartite:
\begin{itemize}
    \item An edge connecting nodes within the same layer indicates an odd-length cycle
    \item To connect within the same layer, a cycle must ``dip'' down to a lower layer and return, creating an odd cycle
\end{itemize}
\end{rigour}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{week09/Bipartite.png}
    \caption{Left: Bipartite graph with no intra-layer edges. Right: Non-bipartite graph with an intra-layer edge (indicating an odd cycle).}
    \label{fig:bipartite-layers}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{week09/Non-bipartite.png}
    \caption{BFS layers of a non-bipartite graph: the red edge connects two vertices within the same layer $L_3$, revealing an odd-length cycle.}
    \label{fig:non-bipartite-layers}
\end{figure}

We are trying to colour each level a single colour; we can have connections between levels, but not within levels.

%------------------------
\section{Directed Graphs}
\label{sec:directed-graphs}

In a directed graph (or \textbf{digraph}), edges have a direction: $i \rightarrow j$ does \textbf{not} imply that $j \rightarrow i$.

\subsection{Directed Acyclic Graphs (DAGs)}
\label{subsec:dags}

A \textbf{Directed Acyclic Graph (DAG)} is a directed graph with no directed cycles. DAGs are particularly important because they can encode causal relationships and dependency structures.

\begin{keybox}[DAG Properties]
A Directed Acyclic Graph (DAG) has:
\begin{itemize}
    \item \textbf{No directed cycles}: There is no path $v_1 \rightarrow v_2 \rightarrow \cdots \rightarrow v_k \rightarrow v_1$
    \item \textbf{At least one source}: A vertex with no incoming edges
    \item \textbf{At least one sink}: A vertex with no outgoing edges
    \item \textbf{A topological ordering}: Vertices can be linearly ordered respecting edge directions
\end{itemize}
\end{keybox}

\textbf{Acyclic}: There is no directed path $i \rightarrow j \rightarrow k \rightarrow \cdots \rightarrow i$ that returns to its starting vertex.

\subsection{Topological Ordering}
\label{subsec:topological-ordering}

A \textbf{topological ordering} (or topological sort) is a concept applied to DAGs to provide a linear ordering of its vertices such that for every directed edge $v_i \rightarrow v_j$, vertex $v_i$ comes before $v_j$.

Topological order encodes \textit{causal precedence} in a causal DAG.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{week09/topological ordering.png}
    \caption{Left: A standard DAG representation. Right: The same DAG with vertices arranged in topological order - all edges point from left to right.}
    \label{fig:topological-ordering}
\end{figure}

\subsubsection{Properties of Topological Ordering}
\label{subsubsec:topological-properties}

\begin{itemize}
    \item \textbf{Directed Acyclic Graph (DAG)}: Topological ordering only applies to DAGs, as the presence of a cycle would make such an ordering impossible.
    \item \textbf{Uniqueness}: A topological order is not necessarily unique; a DAG might have several valid topological sorts.
    \item \textbf{Algorithms}: The common algorithms to find a topological order are Kahn's Algorithm and DFS-based methods.
\end{itemize}

\subsubsection{Kahn's Algorithm for Topological Sorting}
\label{subsubsec:kahns-algorithm}

Kahn's algorithm builds the topological order by repeatedly choosing vertices with no incoming edges.

\begin{enumerate}
    \item \textbf{Identify Vertices with No Incoming Edge}: These will be the starting points for the sorting.
    \item \textbf{Remove Chosen Vertex and Its Edges}: Once a vertex with no incoming edges is selected, it is removed from the graph along with all its outgoing edges.
    \item \textbf{Repeat Process}: Continue removing vertices until all have been ordered.
\end{enumerate}

\subsubsection{DFS-Based Algorithm for Topological Sorting}
\label{subsubsec:dfs-topological}

\begin{enumerate}
    \item \textbf{Perform DFS}: Run DFS from every unvisited node.
    \item \textbf{Track the Finish Times}: Record the finish times for each node.
    \item \textbf{Order by Finish Times}: Once all nodes have been visited, sort them by decreasing finish time, which gives a topological order.
\end{enumerate}

\subsubsection{Applications of Topological Ordering}
\label{subsubsec:topological-applications}

\begin{itemize}
    \item \textbf{Task Scheduling}: In project scheduling or task management where tasks have dependencies, a topological sort provides an order in which to perform the tasks without conflicts.
    \item \textbf{Causal Inference}: In DAGs that represent causal relationships, a topological sort can give the order in which events must occur or be considered.
    \item \textbf{Package Dependency Resolution}: In package managers for software development, topological sorting can be used to decide the order in which packages or libraries should be installed to satisfy dependencies.
\end{itemize}

\subsubsection{Topological Ordering Implies DAG}
\label{subsubsec:topological-implies-dag}

\begin{rigour}[Topological Ordering $\Leftrightarrow$ DAG]
\textbf{Theorem}: A directed graph $G$ has a topological ordering if and only if $G$ is a DAG.

\textbf{Proof} ($\Rightarrow$): If $G$ has a topological ordering, then $G$ is acyclic.

Suppose for contradiction that $G$ has both a topological ordering $v_1, v_2, \ldots, v_n$ and a cycle $C$.
\begin{enumerate}
    \item Let $v_{c_1}, v_{c_2}, \ldots, v_{c_k}$ be the vertices in cycle $C$, indexed according to their positions in the topological ordering.
    \item For each edge $v_{c_i} \rightarrow v_{c_{i+1}}$ in the cycle, the topological ordering requires $c_i < c_{i+1}$.
    \item But since $C$ is a cycle, there must be an edge from $v_{c_k}$ back to $v_{c_1}$, requiring $c_k < c_1$.
    \item This contradicts $c_1 < c_2 < \cdots < c_k$.
\end{enumerate}

Therefore, if $G$ has a topological ordering, it cannot have a cycle.
\end{rigour}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{week09/contradiction.png}
    \caption{The backward edge (bottom) cannot exist in a valid topological ordering - it would create a contradiction.}
    \label{fig:topological-contradiction}
\end{figure}

\textbf{DAGs have source and sink nodes}

DAGs must have at least one vertex with no entering edges (the ``source'' vertex), and at least one vertex with no outgoing edges (the ``sink'' vertex). This property is useful for setting up topological orderings (see Kahn's algorithm).
\begin{itemize}
    \item If a DAG had no source, every vertex would have an incoming edge, which would necessarily create a cycle.
\end{itemize}

\subsubsection{Kahn's Algorithm (Detailed)}
\label{subsubsec:kahns-detailed}

\textbf{Initialisation:}
Start by scanning the graph to initialise the following:
\begin{enumerate}
    \item \textbf{\texttt{count[i]}}: An array to maintain the count of incoming edges for each vertex \(i\) in the graph. This takes \(O(m)\) time, where \(m\) is the number of edges, since you have to potentially look at every edge in the graph.
    \item \textbf{\texttt{S}}: A set or list to keep track of all nodes with no incoming edges, which can be identified during the initial scan. Finding these nodes will take \(O(n)\) time, where \(n\) is the number of vertices in the graph.
\end{enumerate}
The overall time for this initialisation step is \(O(m + n)\), since you are scanning each edge once and each vertex once.

\textbf{Process of Creating Topological Order:}
\begin{itemize}
    \item \textbf{While \texttt{S} is not empty:}
    \begin{itemize}
        \item Remove a node \(i\) from \texttt{S}.
        \item Add \(i\) to the end of the topological ordering.
        \item For each node \(j\) such that there is an edge \(i \to j\):
        \begin{itemize}
            \item Decrement \texttt{count[j]} by 1 (since we are effectively removing the edge \(i \to j\) from the graph).
            \item If \texttt{count[j]} becomes zero, add \(j\) to \texttt{S} because \(j\) now has no incoming edges.
        \end{itemize}
    \end{itemize}
\end{itemize}

\textbf{Complexity Analysis:}

Each removal operation is $O(1)$.

The removal of each edge is done exactly once, and each edge can only cause one decrement operation, so the \textbf{complexity of processing all edges is \(O(m)\)}.

Since each node is inserted into \texttt{S} once and removed from \texttt{S} once, the \textbf{complexity associated with processing all nodes is \(O(n)\)}.

\begin{keybox}[Kahn's Algorithm Complexity]
    \textbf{Overall complexity}: $O(m + n)$

    This reflects the initial graph scan ($O(n + m)$) and the subsequent edge removal and node processing steps.

    The initialisation is the most expensive computational part - we must scan through the entire graph to set this up.
\end{keybox}

\subsubsection{Causal Inference Application}
\label{subsubsec:causal-application}

Suppose you want to measure the causal effect of $v_i$ on $v_j$. What nodes should you control for?

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/casual topolical .png}
    \caption{Causal DAG with topological ordering - when measuring the effect of $v_i$ on $v_j$, be careful about which variables to control for.}
    \label{fig:causal-topological}
\end{figure}

\begin{redbox}[Causal Inference Warning]
You should \textbf{not} control for $v_k$ such that $k > j$ in the topological ordering.

Controlling for a node that appears after $j$ (i.e., a descendant of $j$) can introduce \textbf{collider bias} or open a back-door path, which can distort the causal estimate.
\end{redbox}

\subsection{Finding d-Separation}
\label{subsec:d-separation}

In the context of causal inference, \textbf{d-separation} is a concept used to determine whether a set \( X \) of variables provides evidence about the independence between two other sets of variables, say \( A \) and \( Y \). If \( A \) and \( Y \) are d-separated by \( X \), then \( A \) is conditionally independent of \( Y \) given \( X \), and in a regression model where \( Y \) is regressed on \( A \) and \( X \), the coefficient of \( A \) would be expected to be zero if the only pathways between \( A \) and \( Y \) are blocked by \( X \).

\textbf{How to Find d-Separations:}

The basic idea behind algorithms for finding d-separations in a graph involves the following steps:
\begin{enumerate}
    \item \textbf{Identify All Paths Between \( A \) and \( Y \)}: In a directed graph (often a DAG in causal models), you would enumerate all paths between the nodes in sets \( A \) and \( Y \).
    \item \textbf{Analyse Conditional Independencies}: Determine whether each path is blocked or not when conditioning on set \( X \). A path is considered blocked if it contains a non-collider that is in \( X \) or if it contains a collider that is not in \( X \) and has no descendants in \( X \).
    \item \textbf{Consider All Nodes \( Y \)}: Run the analysis for each variable that could be an outcome of interest in the graph to see if it is d-separated from \( A \) by \( X \).
\end{enumerate}

\textbf{Complexity of Finding d-Separations:}

The complexity of finding d-separations is often \( O(m + n) \) where \( m \) is the number of edges and \( n \) is the number of nodes in the graph. This is because checking for d-separation between two nodes in a DAG usually involves examining paths between the nodes, and in the worst case, you might need to traverse all the nodes and edges.

\textbf{Algorithmic Approaches:}

Specific algorithms, like the one mentioned by Neapolitan (2003) and the work of Geiger, Verma, and Pearl (1989), provide structured methods for efficiently computing these d-separations. These works typically provide pseudocode or step-by-step procedures for traversing the DAG and checking for conditional independencies.

In practical terms, these methods can be applied in statistical software using algorithms that traverse causal networks to aid in the design of statistical models and in the interpretation of regression coefficients, particularly in fields such as epidemiology, economics, and social sciences where causal inference plays a critical role in understanding relationships between variables.

%------------------------
\section{Min-Cut and Max-Flow}
\label{sec:mincut-maxflow}

\subsection{Flow Networks}
\label{subsec:flow-networks}

Flow networks are a specialised type of directed graph that facilitate modelling and solving various types of network flow problems.

Think of the network as a system of pipes - we want to know how water flows through each pipe. The capacity of each pipe corresponds to the edge weight.

\begin{rigour}[Flow Network - Formal Definition]
A \textbf{flow network} is defined by a tuple $G = (V, E, s, t, c)$ where:
\begin{itemize}
    \item $V$ is the set of vertices
    \item $E$ is the set of directed edges
    \item $s \in V$ is the \textbf{source} vertex (where flow originates)
    \item $t \in V$ is the \textbf{sink} vertex (where flow terminates)
    \item $c: E \to \mathbb{R}_{\geq 0}$ is the \textbf{capacity function} assigning a non-negative capacity to each edge
\end{itemize}

For each edge $e \in E$: $c(e) \geq 0$
\end{rigour}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/flow network.png}
    \caption{A flow network with source $s$, sink $t$, and edge capacities shown as weights.}
    \label{fig:flow-network}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{week09/ut.png}
    \caption{A larger flow network example: each edge label indicates capacity. The goal is to find the maximum flow from $s$ to $t$.}
    \label{fig:flow-network-large}
\end{figure}

\textbf{Properties and Usage:}

\begin{itemize}
    \item \textbf{Edge Direction}: Edges in flow networks are directed, meaning that flow can only move in the specified direction from one vertex to another.

    \item \textbf{Conservation of Flow}: Except for the source, which ``produces'' flow, and the sink, which ``consumes'' flow, each vertex in a flow network has the property that the total flow into the vertex equals the total flow out of the vertex. This is known as the conservation of flow or Kirchhoff's first law in the context of electrical networks.
\end{itemize}

\subsection{Graph Cuts}
\label{subsec:graph-cuts}

\begin{rigour}[$s$-$t$ Cut - Formal Definition]
An \textbf{$s$-$t$ cut} in a flow network is a partition of the vertices into two disjoint subsets $A$ and $B$ such that:
\begin{itemize}
    \item $A \cup B = V$ and $A \cap B = \emptyset$
    \item $s \in A$ (source is in $A$)
    \item $t \in B$ (sink is in $B$)
\end{itemize}

The \textbf{capacity of a cut} $(A, B)$ is:
\[
\text{cap}(A, B) = \sum_{e \text{ from } A \text{ to } B} c(e)
\]
where the sum is over all edges that cross from $A$ to $B$.

\textbf{Note}: Edges from $B$ to $A$ do \textbf{not} contribute to the cut capacity.
\end{rigour}

\textit{The cut is the cumulative capacity of all the ``pipes'' at the point at which we cut.}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/cut.png}
    \caption{An $s$-$t$ cut: the cut capacity includes only edges crossing from $A$ to $B$, not edges from $B$ to $A$.}
    \label{fig:graph-cut}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/graph cut.png}
    \caption{Another view of graph cuts - the dashed line shows the partition between sets $A$ and $B$.}
    \label{fig:graph-cut-partition}
\end{figure}

\subsection{Min-Cut Problem}
\label{subsec:min-cut}

The \textbf{minimum cut} (min-cut) is the $s$-$t$ cut with the smallest capacity among all possible cuts.

\begin{keybox}[Min-Cut Problem]
\textbf{Goal}: Find the $s$-$t$ cut $(A, B)$ that minimises $\text{cap}(A, B)$.

\textbf{Key insight}: To determine the min-cut, you typically compute the max-flow first. This relationship is grounded in the \textbf{Max-Flow Min-Cut Theorem}.

After computing maximum flow, you can find the minimum cut in $O(m)$ time.
\end{keybox}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/Min-cut.png}
    \caption{The minimum cut represents the bottleneck capacity of the network - the maximum flow that can pass from $s$ to $t$.}
    \label{fig:min-cut}
\end{figure}

\textbf{Applications:}
\begin{itemize}
    \item \textbf{Network Design}: Understanding the limitations and bottleneck of a network.
    \item \textbf{Reliability Analysis}: Evaluating the robustness of a system against failures.
    \item \textbf{Image Segmentation}: In computer vision, graph cuts are used to separate objects from the background.
    \item \textbf{Project Management}: Identifying critical paths and constraints in project planning.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{week09/graph cuts 2.png}
    \caption{Graph cuts in image segmentation - pixels are nodes, and edges connect adjacent pixels with weights based on similarity.}
    \label{fig:graph-cuts-image}
\end{figure}

\subsection{Max-Flow Problem}
\label{subsec:max-flow}

The \textbf{maximum flow problem} asks for the maximum possible flow that can be sent from a source node $s$ to a sink node $t$ in a flow network, subject to capacity constraints on the edges and flow conservation at each node.

\begin{rigour}[$s$-$t$ Flow - Formal Definition]
An \textbf{$s$-$t$ flow} $f$ is a function $f: E \to \mathbb{R}_{\geq 0}$ that satisfies:

\textbf{1. Capacity Constraint}: For each edge $e \in E$:
\[
0 \leq f(e) \leq c(e)
\]

\textbf{2. Flow Conservation}: For every vertex $v \in V \setminus \{s, t\}$:
\[
\sum_{e \text{ into } v} f(e) = \sum_{e \text{ out of } v} f(e)
\]
(flow in equals flow out)

The \textbf{value} of the flow is:
\[
|f| = \sum_{e \text{ out of } s} f(e) - \sum_{e \text{ into } s} f(e)
\]
(net flow leaving the source)
\end{rigour}

\begin{keybox}[Max-Flow Intuition]
\textit{``How high can I turn up the water pressure at the source that this pipe network can support?''}

The maximum flow equals the total amount of ``stuff'' that can be pushed from $s$ to $t$ per unit time, respecting all capacity constraints.
\end{keybox}

\textbf{Note on flow value}: The flow value $|f|$ represents the net flow leaving the source (which equals the net flow entering the sink, by conservation). Typically, there is no flow entering the source, so $|f| = \sum_{e \text{ out of } s} f(e)$.

\textbf{Algorithms to Solve Max-Flow:}
\begin{itemize}
    \item \textbf{Ford-Fulkerson Method}: Uses augmenting paths and works well in practice. However, its time complexity is not polynomial in the size of the graph for irrational capacities.
    \item \textbf{Edmonds-Karp Algorithm}: An implementation of the Ford-Fulkerson method that uses BFS for finding augmenting paths, which leads to a polynomial runtime.
    \item \textbf{Dinic's Algorithm}: Relies on repeatedly constructing level graphs and finding blocking flows, with better average performance.
\end{itemize}

\textbf{Applications of Max-Flow:}
\begin{itemize}
    \item \textbf{Networking}: Finding the maximum throughput in a network - e.g., internet routing
    \item \textbf{Transportation}: Determining the most efficient way to route goods through a transportation network.
    \item \textbf{Project Management}: Identifying critical paths in project scheduling.
    \item \textbf{Matching Problems}: Solving problems in bipartite graphs for matching resources to tasks.
\end{itemize}

\subsection{Ford-Fulkerson Algorithm}
\label{subsec:ford-fulkerson}

\subsubsection{Naive Algorithm (Fails)}
\label{subsubsec:naive-algo}

\begin{redbox}[Why the Naive Approach Fails]
The naive greedy approach fails because once you choose a path and push flow through it, you can never decrease the flow through that path.

You need the ability to ``undo'' or redirect flow to fix suboptimal early choices.
\end{redbox}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/max flow.png}
    \caption{A flow network where the naive greedy approach can fail to find the maximum flow.}
    \label{fig:max-flow-setup}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/max flow 1.png}
    \caption{Step 1: Start with $f(e) = 0$ for each edge $e \in E$.}
    \label{fig:max-flow-step1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/max flow 2.png}
    \caption{Step 2: Find an $s \rightarrow t$ path where each edge has $f(e) < c(e)$, then augment flow along that path.}
    \label{fig:max-flow-step2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/max flow 3.png}
    \caption{Step 3: Augment flow along the chosen path.}
    \label{fig:max-flow-step3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/max flow 4.png}
    \caption{Step 4: Repeat until you get stuck (no more augmenting paths with available capacity).}
    \label{fig:max-flow-step4}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/max flow 5.png}
    \caption{Naive result: ending flow = 16.}
    \label{fig:max-flow-naive-result}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/max flow 6.png}
    \caption{Optimal result: maximum flow = 19. The naive approach missed 3 units of flow!}
    \label{fig:max-flow-optimal}
\end{figure}

The naive approach fails because once you choose a path, you can never decrease the flow through that path. We need the ability to go back and ``fix'' suboptimal choices.

\subsubsection{Residual Flow / Residual Network}
\label{subsubsec:residual-network}

The \textbf{residual network} shows how much more flow can be pushed through the network and the directions in which it can be pushed. This network adjusts dynamically as the flow changes.

\begin{rigour}[Residual Network - Formal Definition]
Given a flow network $G = (V, E, s, t, c)$ and a flow $f$, the \textbf{residual network} $G_f = (V, E_f)$ is defined as follows:

For each edge $e = (u \rightarrow v)$ in $G$:

\textbf{Forward edge} (remaining capacity):
\[
c_f(u \rightarrow v) = c(e) - f(e)
\]
Include this edge in $E_f$ if $c_f(u \rightarrow v) > 0$.

\textbf{Backward edge} (ability to ``undo'' flow):
\[
c_f(v \rightarrow u) = f(e)
\]
Include this edge in $E_f$ if $f(e) > 0$.

The residual capacity formula:
\[
c_f(e') = \begin{cases}
    c(e) - f(e) & \text{if } e' \text{ is a forward edge of } e \in E \\
    f(e) & \text{if } e' \text{ is a backward edge of } e \in E
\end{cases}
\]
\end{rigour}

\begin{enumerate}
    \item \textbf{Original Edge and Flow}:
    \begin{itemize}
        \item Let \( e = (i \rightarrow j) \) be a directed edge in the original flow network \( G \) with a capacity \( c(e) \) and a flow \( f(e) \).
    \end{itemize}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{week09/original flow.png}
        \caption{Original edge with capacity $c(e)$ and current flow $f(e)$.}
        \label{fig:original-flow}
    \end{figure}

    \item \textbf{Residual Capacity}:
    \begin{itemize}
        \item The \textbf{residual capacity} of an edge \( e \) is defined as \( c_f(e) = c(e) - f(e) \). This value represents the additional amount of flow that can still be pushed through edge \( e \) without exceeding its capacity.
    \end{itemize}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{week09/residual flow.png}
        \caption{Residual network showing both forward capacity ($c(e) - f(e)$) and backward capacity ($f(e)$).}
        \label{fig:residual-flow}
    \end{figure}

    \item \textbf{Reverse Edge}:
    \begin{itemize}
        \item For each edge \( e = (i \rightarrow j) \) in \( G \), the residual network includes a reverse edge \( e_{\text{rev}} = (j \rightarrow i) \). This reverse edge allows for the possibility of reducing the flow that was previously sent through \( e \), effectively ``undoing'' some of the flow if it enhances the overall solution.
        \item The capacity of the reverse edge \( e_{\text{rev}} \) is set to \( f(e) \), which is the current flow through edge \( e \).
    \end{itemize}

    \item \textbf{Key Property}:
    \begin{itemize}
        \item A flow \( f' \) in the residual network \( G_f \) can be combined with the flow \( f \) in \( G \) to form a new valid flow in \( G \). If \( f' \) represents an increase or decrease along certain paths, \( f + f' \) reflects the adjusted flow in \( G \) that accommodates these changes.
    \end{itemize}
\end{enumerate}

\subsubsection{Augmenting Path}
\label{subsubsec:augmenting-path}

An \textbf{augmenting path} is the key concept for finding maximum flow.

\begin{rigour}[Augmenting Path]
An \textbf{augmenting path} $P$ is a simple path from $s$ to $t$ in the residual network $G_f$.

The \textbf{bottleneck capacity} of $P$ is the minimum residual capacity among all edges in $P$:
\[
\text{bottleneck}(P) = \min_{e \in P} c_f(e)
\]

\textbf{Augmentation}: When an augmenting path is found, the flow can be increased by the bottleneck capacity:
\begin{itemize}
    \item For forward edges $e \in P$: $f'(e) = f(e) + \text{bottleneck}(P)$
    \item For backward edges in $P$ (corresponding to original edge $e$): $f'(e) = f(e) - \text{bottleneck}(P)$
\end{itemize}
\end{rigour}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{week09/augmenting path.png}
    \caption{An augmenting path in the residual network - the bottleneck determines how much flow can be pushed.}
    \label{fig:augmenting-path}
\end{figure}

The new flow $f'$ after applying the augmentation along path $P$ increases the total flow from $s$ to $t$ by the amount of the bottleneck capacity:
\[
|f'| = |f| + \text{bottleneck}(P)
\]

This increment continues until no more augmenting paths can be found in the residual network, at which point the flow $f$ is maximum.

\subsubsection{Ford-Fulkerson Algorithm}
\label{subsubsec:ff-algorithm}

\begin{keybox}[Ford-Fulkerson Algorithm]
\begin{enumerate}
    \item \textbf{Initialise}: Set $f(e) = 0$ for each edge $e \in E$
    \item \textbf{While} there exists an augmenting path $P$ from $s$ to $t$ in residual graph $G_f$:
    \begin{itemize}
        \item Compute bottleneck capacity of $P$
        \item Augment flow along $P$
        \item Update residual capacities
    \end{itemize}
    \item \textbf{Return} flow $f$ (this is the maximum flow)
\end{enumerate}
\end{keybox}

\begin{enumerate}
    \item \textbf{Initialisation}:
    \begin{itemize}
        \item Start with \( f(e) = 0 \) for each edge \( e \) in the set of edges \( E \), implying that initially, there is no flow through the network.
    \end{itemize}
    \item \textbf{Finding Augmenting Paths}:
    \begin{itemize}
        \item Look for a path from the source \( s \) to the sink \( t \) in the residual network such that each edge along the path has positive residual capacity.
    \end{itemize}
    \item \textbf{Augmenting Flow}:
    \begin{itemize}
        \item Once an augmenting path is found, increase the flow along the path by the bottleneck capacity (the smallest residual capacity on any edge in the path).
    \end{itemize}
    \item \textbf{Repeat the Process}:
    \begin{itemize}
        \item Continue finding augmenting paths and increasing the flow until no more augmenting paths can be found.
    \end{itemize}
    \item \textbf{Algorithm Termination}:
    \begin{itemize}
        \item When you can no longer find any augmenting paths, the flow \( f \) is a maximum flow, and the algorithm terminates.
    \end{itemize}
\end{enumerate}

\textbf{Key Points:}

\begin{itemize}
    \item \textbf{Residual Graph}: The residual graph reflects the capacity left for each edge to carry more flow. It changes as flows are augmented along paths.
    \item \textbf{Bottleneck Capacity}: For each augmenting path, the flow is increased by the bottleneck capacity, which is the maximum additional flow that can be pushed through the path without violating capacity constraints.
    \item \textbf{Edge Direction}: The algorithm considers reverse edges to allow for the possibility of ``undoing'' or reducing flow if it leads to an improved overall solution.
    \item \textbf{Complexity}: While the Ford-Fulkerson method is intuitive and often efficient in practice, its runtime depends on the magnitude of the flow. If capacities are integers, the algorithm runs in \( O(m \cdot |f_{\max}|) \), where \( |f_{\max}| \) is the value of the maximum flow.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{week09/ff algo 1.png}
    \caption{Ford-Fulkerson Step 1: Initial flow network with all flows set to zero.}
    \label{fig:ff-step1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{week09/ff algo 2.png}
    \caption{Ford-Fulkerson Step 2: Find an augmenting path and compute its bottleneck.}
    \label{fig:ff-step2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{week09/ff algo 3.png}
    \caption{Ford-Fulkerson Step 3: Augment flow along the path and update residual capacities.}
    \label{fig:ff-step3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{week09/ff algo 4.png}
    \caption{Ford-Fulkerson Step 4: Continue finding augmenting paths in the updated residual network.}
    \label{fig:ff-step4}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{week09/ff algo 5.png}
    \caption{Ford-Fulkerson Step 5: Another augmentation using a path that includes a backward edge.}
    \label{fig:ff-step5}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{week09/ff algo 6.png}
    \caption{Ford-Fulkerson Final: No more augmenting paths exist - maximum flow achieved.}
    \label{fig:ff-final}
\end{figure}

\subsubsection{Edmonds-Karp Algorithm - Using BFS to Choose Paths}
\label{subsubsec:edmonds-karp}

The \textbf{Edmonds-Karp algorithm} is an implementation of Ford-Fulkerson that uses Breadth-First Search (BFS) to choose paths, ensuring systematic selection of the \textbf{shortest} augmenting paths in terms of the number of edges.

\begin{redbox}[Ford-Fulkerson: Almost a Good Algorithm!]
Ford-Fulkerson can misbehave if paths are chosen badly - in the worst case, you could have an \textbf{exponential number of iterations}!

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{week09/FF bad paths.png}
    \caption{Bad path selection in Ford-Fulkerson - alternating between two paths can lead to exponentially many iterations.}
    \label{fig:ff-bad-paths}
\end{figure}

The solution: use BFS to always find the \textbf{shortest} augmenting path.
\end{redbox}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{week09/shortest augmenting path.png}
    \caption{Edmonds-Karp uses BFS to find the shortest augmenting path (fewest edges).}
    \label{fig:shortest-augmenting}
\end{figure}

\textbf{BFS for Augmenting Paths}

BFS is utilised to find the shortest augmenting paths because it explores all vertices at the present depth before moving on to vertices at the next depth level, ensuring the path found has the minimum number of edges:

\begin{enumerate}
    \item \textbf{Augmenting Path Lengths}: When BFS is used, the length of the shortest augmenting path, measured in edges, never decreases between iterations. This is because once all shortest paths of a given length have been augmented, any subsequent augmenting paths must necessarily be longer.

    \item \textbf{Increment in Path Lengths}: After exhausting all augmenting paths of a given length, the only remaining paths (if any) must involve more edges, hence after \( m \) augmenting paths of a given length, the shortest augmenting path must increase in length.
\end{enumerate}

\begin{keybox}[Edmonds-Karp Complexity]
\textbf{Per iteration}: $O(m)$ (BFS to find shortest augmenting path)

\textbf{Total iterations}: $O(mn)$ (path length increases, bounded by $n$, and at most $m$ augmentations per length)

\textbf{Overall complexity}: $O(m^2 n)$

This is a polynomial-time algorithm, unlike vanilla Ford-Fulkerson which can be exponential.
\end{keybox}

\textbf{Complexity Analysis}

\begin{itemize}
    \item \textbf{Iteration Complexity}: In each iteration, BFS is used to find the shortest augmenting path, which runs in \( O(m) \) time where \( m \) is the number of edges. BFS is optimal for this part of the algorithm because it efficiently handles the exploration of all vertices and edges.

    \item \textbf{Total Number of Iterations}: The total number of iterations required by the algorithm is bounded by the number of edges \( m \) times the maximum number of distinct shortest path lengths, which in the worst case is \( n - 1 \) (where \( n \) is the number of vertices). Hence, the complexity becomes \( O(m^2n) \).
\end{itemize}

\textbf{Discussion on Efficiency and Improvements}

\begin{itemize}
    \item \textbf{Efficiency}: While the \( O(m^2n) \) time complexity is a significant improvement over the unrestricted Ford-Fulkerson approach, especially for networks where the capacities are not excessively large relative to the number of edges, it can still be suboptimal for very large or dense networks.

    \item \textbf{Faster Approaches}: Algorithms like Dinic's algorithm and Push-Relabel provide better performance in many scenarios. Dinic's algorithm, for example, uses a combination of BFS for constructing a ``level graph'' and DFS for finding blocking flows, resulting in an \( O(n^2m) \) complexity, which can be more efficient especially in dense graphs.
\end{itemize}

\subsection{Max-Flow Min-Cut Theorem}
\label{subsec:max-flow-min-cut}

\begin{rigour}[Max-Flow Min-Cut Theorem]
In any flow network, the \textbf{maximum value of an $s$-$t$ flow} equals the \textbf{minimum capacity of an $s$-$t$ cut}.

\[
\max_f |f| = \min_{(A,B)} \text{cap}(A, B)
\]

\textbf{Proof sketch}:
\begin{enumerate}
    \item \textbf{Flow $\leq$ Cut}: For any flow $f$ and any cut $(A, B)$, we have $|f| \leq \text{cap}(A, B)$. This is because all flow from $s$ to $t$ must cross the cut, and cannot exceed the cut's capacity.

    \item \textbf{Equality at optimum}: When Ford-Fulkerson terminates with no augmenting path, define $A$ as the set of vertices reachable from $s$ in the residual network. Then:
    \begin{itemize}
        \item $s \in A$ and $t \notin A$ (otherwise there would be an augmenting path)
        \item Every edge from $A$ to $B = V \setminus A$ is saturated: $f(e) = c(e)$
        \item Every edge from $B$ to $A$ has zero flow: $f(e) = 0$
    \end{itemize}
    Therefore $|f| = \text{cap}(A, B)$, achieving equality.
\end{enumerate}
\end{rigour}

\begin{keybox}[Finding the Min-Cut from Max-Flow]
After computing max-flow (using Ford-Fulkerson, Edmonds-Karp, or Dinic's algorithm), you can find the min-cut in $O(m)$ time:

\textbf{Method}:
\begin{enumerate}
    \item Construct the final residual graph $G_f$
    \item Use BFS/DFS from $s$ to find all vertices reachable in $G_f$
    \item Let $A$ = set of reachable vertices, $B = V \setminus A$
    \item The edges crossing from $A$ to $B$ in the original graph form the min-cut
\end{enumerate}
\end{keybox}

\textbf{Process for Finding Min-Cut}:
\begin{enumerate}
    \item \textbf{Compute Residual Graph}: After finding the max-flow, construct the residual graph $G_f$.
    \begin{itemize}
        \item Each edge $e=(u \rightarrow v)$ has residual capacity $c_f(e) = c(e) - f(e)$
    \end{itemize}
    \item \textbf{Identify Reachable Vertices}: Run BFS/DFS from source $s$ \textit{in the residual graph $G_f$} - mark all vertices reachable from $s$.
    \item \textbf{Form the Min-Cut}:
    \begin{itemize}
        \item $A$ = set of all vertices reachable from $s$ in $G_f$
        \item $B = V \setminus A$
        \item The edges crossing from $A$ to $B$ in the original graph $G$ form the min-cut
    \end{itemize}
\end{enumerate}

The max-flow min-cut theorem is one of the fundamental results in combinatorial optimisation, connecting two seemingly different problems and providing both an algorithm (Ford-Fulkerson finds both max-flow and min-cut) and a certificate of optimality (the min-cut proves the flow is maximal).
