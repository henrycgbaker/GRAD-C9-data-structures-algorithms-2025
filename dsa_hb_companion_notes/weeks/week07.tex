% Week 7: Optimization
% This chapter provides foundational context for machine learning optimisation methods

\thispagestyle{empty}
\begin{tabular}{p{15.5cm}}
{\large \bf DS\&A Lecture Notes 2024 \\ Henry Baker} \\
\hline
\end{tabular}

\vspace*{0.3cm}
\begin{center}
	{\Large \bf DS\&A Lecture Notes: Wk 7\\ Optimisation}
	\vspace{2mm}

\end{center}
\vspace{0.4cm}

\begin{keybox}[Chapter Overview]
This chapter covers numerical optimisation methods that form the backbone of machine learning algorithms. Understanding these techniques is essential for training models effectively, as most ML problems reduce to finding parameters that minimise a loss function. The concepts here underpin everything from linear regression to deep neural networks.
\end{keybox}

\textbf{Central Learning Objective: Constrained Optimisation}
\[\theta^* = \arg\min_{\theta \in \Theta} \mathcal{L}(\theta)\]
Where $\Theta$ is the \textit{constrained} parameter space.

%=====================================================
\section{Local vs Global Minima}
%=====================================================

\subsection{Local Minimum}

A point is a local minimum if the function value is greater or equal in every \textbf{direction} within some neighbourhood.

\begin{rigour}[Local Minimum]
\[
\exists \delta > 0, \forall \theta \in \Theta \text{ s.t. } |\theta - \theta^*| < \delta, \mathcal{L}(\theta^*) \leq \mathcal{L}(\theta)
\]

Breaking down this definition:
\begin{itemize}
    \item $\exists \delta > 0$: there exists a positive number $\delta$ (the neighbourhood radius).
    \item $\forall \theta \in \Theta \text{ s.t. } |\theta - \theta^*| < \delta$: for all $\theta$ in the set $\Theta$ such that the absolute difference between $\theta$ and $\theta^*$ is less than $\delta$.
    \item $\mathcal{L}(\theta^*) \leq \mathcal{L}(\theta)$: the value of the function $\mathcal{L}$ evaluated at $\theta^*$ is less than or equal to the value of the function $\mathcal{L}$ evaluated at $\theta$.
\end{itemize}
\end{rigour}

\subsection{Global Minimum}

A point is a global minimum if the function value is larger at every other \textbf{location} in the entire domain.

\begin{rigour}[Global Minimum]
\[
\forall \theta \in \Theta, \mathcal{L}(\theta^*) \leq \mathcal{L}(\theta)
\]

\begin{itemize}
    \item For all $\theta$ in the set $\Theta$.
    \item The value of the function $\mathcal{L}$ evaluated at $\theta^*$ is less than or equal to the value of the function $\mathcal{L}$ evaluated at $\theta$.
\end{itemize}
\end{rigour}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{minima.png}
    \caption{Local versus global minima: a function may have multiple local minima, but only one global minimum (the lowest point overall).}
    \label{fig:local-vs-global}
\end{figure}

%=====================================================
\section{Characterising a Local Minimum}
%=====================================================

\subsection{One-Dimensional Case}

In one dimension, a local minimum is characterised by two conditions:

\begin{enumerate}
    \item \textbf{First derivative = 0}
        \[\frac{d}{d\theta}\mathcal{L}(\theta)\bigg|_{\theta^*} = 0\]
        Evaluate the derivative of the loss function at $\theta^*$; at a minimum, the function is neither increasing nor decreasing in that infinitesimal region.

    \item \textbf{Second derivative $\geq 0$ (positive)}
     \[\frac{d^2}{d\theta^2}\mathcal{L}(\theta)\bigg|_{\theta^*} \geq 0\]

     The second derivative is the ``gradient of the gradient'' - the rate of change of the rate of change. It should be positive because this indicates that the slope is increasing as we move away from $\theta^*$: we are on an upward movement in any direction. A move in any direction would make the slope become larger (more positive or less negative).
\end{enumerate}

Geometrically, these conditions ensure the function is locally \textbf{bowl-shaped} at the minimum.

\subsection{High-Dimensional Case}
\label{sec:high-d-minima}

\begin{enumerate}
    \item \textbf{First derivative: Gradient = 0}

    \begin{rigour}[Gradient]
        The \textbf{gradient} is the derivative along every parameter dimension. If $\theta$ is $p$-dimensional, then the gradient of $\mathcal{L}$ with respect to $\theta$ is a $p$-dimensional vector:
    \[
    \nabla \mathcal{L}(\theta) = \left( \frac{\partial \mathcal{L}}{\partial \theta_1}, \frac{\partial \mathcal{L}}{\partial \theta_2}, \ldots, \frac{\partial \mathcal{L}}{\partial \theta_p} \right)
    \]
    This vector represents the rate of change of $\mathcal{L}$ with respect to each component of $\theta$. The gradient points in the direction of steepest ascent.
    \end{rigour}

    At a local minimum, the gradient must be zero:
    \[\nabla \mathcal{L}(\theta)\big|_{\theta^*} = \nabla \mathcal{L}(\theta^*) = \mathbf{0}\]
    \textit{If the gradient were non-zero, we could move a little bit in the negative gradient direction and obtain a smaller value.}

    \item \textbf{Second derivative: Hessian must be ``positive-like''}

    \begin{rigour}[Hessian Matrix]
        The \textbf{Hessian} is a square matrix of second-order partial derivatives of a scalar-valued function. It describes the local curvature of the function's surface.

        For a function $f(\mathbf{x})$ where $\mathbf{x}$ is a vector of variables:
        \[
        H_{ij} = \frac{\partial^2 f(\mathbf{x})}{\partial x_i \partial x_j}
        \]

        where $H_{ij}$ represents the second partial derivative of $f$ with respect to $x_i$ and $x_j$.

        The Hessian matrix provides important information about the \textbf{local behaviour} of the function:
        \begin{itemize}
            \item \textbf{Positive-definite} Hessian $\Rightarrow$ local minimum
            \item \textbf{Negative-definite} Hessian $\Rightarrow$ local maximum
            \item \textbf{Indefinite} Hessian $\Rightarrow$ saddle point
        \end{itemize}
    \end{rigour}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{saddle point.png}
        \caption{A saddle point is not a local minimum: the Hessian is indefinite. While the function curves upward in the $x$-direction, it curves downward in the $y$-direction; moving along $y$ would reduce the loss.}
        \label{fig:saddle-point-1}
    \end{figure}

    For a local minimum, we require a positive (semi)definite Hessian - meaning a move in any direction should be going uphill:
    \[H^* = \nabla^2 \mathcal{L}(\theta)\big|_{\theta^*} = \nabla^2 \mathcal{L}(\theta^*) \succeq 0\]

\end{enumerate}

\subsection{Quadratic Forms and Definiteness}

To understand what ``positive definite'' means precisely, we need the concept of quadratic forms.

\begin{rigour}[Quadratic Form]
    The \textbf{quadratic form} $\mathbf{x}^T A \mathbf{x}$ represents a scalar value obtained by multiplying a vector $\mathbf{x}$ by a matrix $A$ and then by $\mathbf{x}^T$ (the transpose). This is called ``quadratic'' because it involves the square of the variables in $\mathbf{x}$.

    \begin{itemize}
        \item \textbf{Vector $\mathbf{x}$:} A column vector of variables.
        \item \textbf{Matrix $A$:} A symmetric matrix whose coefficients determine the interactions between variables.
    \end{itemize}

    The structure $\mathbf{x}^T A \mathbf{x}$ arises from matrix multiplication compatibility: $\mathbf{x}^T$ is a row vector, $A$ is a matrix, and $\mathbf{x}$ is a column vector, yielding a scalar. The result summarises how the variables interact according to the coefficients in $A$.
\end{rigour}

\begin{rigour}[Matrix Definiteness]
    \textbf{Positive Definite:} A symmetric matrix $A$ is positive definite if for any non-zero vector $\mathbf{x}$:
    \[
    \mathbf{x}^T A \mathbf{x} > 0 \quad \text{for all } \mathbf{x} \neq \mathbf{0}
    \]

    \textbf{Positive Semidefinite:} A symmetric matrix $A$ is positive semidefinite if:
    \[
    \mathbf{x}^T A \mathbf{x} \geq 0 \quad \text{for all } \mathbf{x}
    \]

    \textbf{Negative Definite:} A symmetric matrix $A$ is negative definite if:
    \[
    \mathbf{x}^T A \mathbf{x} < 0 \quad \text{for all } \mathbf{x} \neq \mathbf{0}
    \]

    \textbf{Indefinite:} A symmetric matrix $A$ is indefinite if there exist vectors $\mathbf{x}_1$ and $\mathbf{x}_2$ such that:
    \[
    \mathbf{x}_1^T A \mathbf{x}_1 > 0 \quad \text{and} \quad \mathbf{x}_2^T A \mathbf{x}_2 < 0
    \]
    That is, the quadratic form can take both positive and negative values depending on direction.
\end{rigour}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{saddlepoint.png}
    \caption{Another view of a saddle point: $H$ is indefinite. While the surface curves upward in one direction, any move along the perpendicular direction would further reduce the loss function.}
    \label{fig:saddle-point-2}
\end{figure}

\begin{keybox}[Local Minimum Conditions]
For a point $\theta^*$ to be a local minimum:
\begin{enumerate}
    \item $\nabla \mathcal{L}(\theta^*) = \mathbf{0}$ (gradient is zero)
    \item $\nabla^2 \mathcal{L}(\theta^*) \succeq 0$ (Hessian is positive semidefinite)
\end{enumerate}
Geometrically: the function is locally \textbf{bowl-shaped} at the minimum.
\end{keybox}

%=====================================================
\section{Taylor Expansion and the Logic of Local Minima}
%=====================================================

\begin{redbox}[Why This Matters]
    Taylor series provides the formal proof that positive definiteness of the Hessian characterises a minimum:
    \begin{itemize}
        \item Taylor series allows us to locally approximate a function
        \item Its second-order term is the Hessian
        \item At a critical point, the first-order term (gradient) is zero
        \item For sufficiently small perturbations, higher-order terms become negligible
    \end{itemize}
    This leaves us with just the second-order/quadratic term. Requiring the function to increase in all directions ($\geq 0$) then yields the positive definiteness condition.
\end{redbox}

The intuition: for $\theta$ to be a minimum, any small move in any direction must result in a higher loss. We formalise this using Taylor expansion.

\subsection{Taylor Expansion of a Function}

We take a Taylor expansion around a possible minimum $\theta$.

\begin{rigour}[Taylor Expansion]
    The Taylor expansion allows us to \textbf{locally approximate} a function $f(x + \epsilon)$ around a point $x$ using the function's value and derivatives at $x$.
\end{rigour}

The first few terms of the Taylor expansion of the loss function with a small perturbation $\epsilon$ in any direction:
\[
\mathcal{L}(\theta + \epsilon) \approx \mathcal{L}(\theta) + \nabla \mathcal{L}(\theta) \cdot \epsilon + \frac{1}{2} \epsilon^T \nabla^2 \mathcal{L}(\theta) \, \epsilon + \cdots
\]

\begin{keybox}[Hessian in Taylor Expansion]
    Crucially, $\nabla^2 \mathcal{L}(\cdot) = H$ (the Hessian is the matrix of second-order partial derivatives).

    So we can write:
    \[\mathcal{L}(\theta + \epsilon) \approx \mathcal{L}(\theta) + \nabla \mathcal{L}(\theta) \cdot \epsilon + \frac{1}{2} \epsilon^T H \epsilon + \cdots\]
\end{keybox}

\subsection{At a Critical Point}

At a critical point (possible minimum or maximum), the gradient is zero:
\[\nabla\mathcal{L}(\theta) = \mathbf{0}\]

This means the first-order term vanishes, leaving:
\[\mathcal{L}(\theta + \epsilon) \approx \mathcal{L}(\theta) + \frac{1}{2} \epsilon^T H \epsilon + \cdots\]

For sufficiently small $\epsilon$, we can neglect higher-order terms, leaving just the quadratic term.

\subsection{Condition for a Minimum}

For $\theta$ to be a minimum, moving a small distance away in any direction ($\epsilon$) should result in a higher loss:
\[\mathcal{L}(\theta + \epsilon) \geq \mathcal{L}(\theta)\]

Substituting our approximation:
\[\frac{1}{2} \epsilon^T H \epsilon \geq 0\]

Since $\frac{1}{2} > 0$, this simplifies to:
\[\epsilon^T H \epsilon \geq 0\]

This condition must hold for all non-zero vectors $\epsilon$, which is precisely the definition of \textbf{positive semidefiniteness}.

\subsection{Positive Definiteness as Characterisation}

The above derivation shows that \textbf{positive semidefiniteness of the Hessian at a point $\theta$ is a necessary condition for that point to be a local minimum}.

For a \textit{strict} local minimum (where nearby points have strictly higher function values), we require \textit{positive definiteness}: $\epsilon^T H \epsilon > 0$ for all $\epsilon \neq \mathbf{0}$.

%=====================================================
\section{Convexity}
%=====================================================

Convexity determines when a \textbf{local} minimum is also a \textbf{global} minimum.

\begin{keybox}[Convexity and Global Optimality]
\begin{enumerate}
    \item Positive (semi)definite $H$ at a point $\Rightarrow$ local minimum
    \item Convex function $\Rightarrow$ any local minimum is also a global minimum
\end{enumerate}
\end{keybox}

\subsection{Formal Definition}

\begin{rigour}[Convex Function]
A function $f$ is \textbf{convex} if for all $x$, $y$ in its domain and all $\lambda \in [0,1]$:
\[
f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda) f(y)
\]

This states that the function value at any convex combination of $x$ and $y$ is less than or equal to the convex combination of the function values.

Geometrically: the line segment connecting any two points on the curve lies above or on the curve. Any two points above the curve can be connected by a straight line that does not cross the curve.
\end{rigour}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{bowl shape.png}
    \caption{Left: convex function (line segment lies above curve). Right: non-convex function (the function crosses the line segment).}
    \label{fig:convexity}
\end{figure}

This ``bowl'' shape ensures that any point within the bowl cannot be higher than points on the line segment connecting any two points on the graph.

\subsection{Why Convexity Matters}

For loss functions in machine learning, the bowl shape ensures that \textbf{if you find a minimum, it is the lowest point in the entire ``bowl''} - the global minimum.

Convexity is crucial because:
\begin{itemize}
    \item It guarantees that a local minimum is also a global minimum
    \item In high-dimensional optimisation (common in ML), finding the global minimum is computationally challenging
    \item Convex functions make this feasible: once a local minimum is found, no further search is necessary
\end{itemize}

This is why convex loss functions are highly desirable for tasks like linear regression, logistic regression, and support vector machines.

\subsection{Local vs Global Minima: The Role of PSD Hessians}

\begin{rigour}[Characterising Minima via the Hessian]
\begin{enumerate}
    \item \textbf{Local Minimum:} Occurs at any point where the Hessian matrix is positive semidefinite (PSD) - the quadratic form $\mathbf{z}^T \nabla^2 f(x) \mathbf{z} \geq 0$ holds for any non-zero vector $\mathbf{z}$ \textbf{at that point}.

    \item \textbf{Global Minimum (via Convexity):} For a function to be convex (and thus have any local minimum be global), the Hessian must be PSD \textbf{everywhere} in the domain.

    \item \textbf{Unique Global Minimum:} Requires positive \textbf{definiteness} (PD) everywhere, giving strict convexity.
\end{enumerate}
\end{rigour}

\begin{redbox}[Non-Convex Functions Can Have Global Minima]
A function need not be convex to have a global minimum. Consider a surface with two valleys where one is slightly lower than the other. This function has a unique global minimum but is not convex (since the Hessian is not PSD everywhere).

The key insight is that convexity provides a \textbf{guarantee}: if you find any local minimum, you know it is global. Without convexity, you might find a local minimum but have no way to verify whether a better solution exists elsewhere.
\end{redbox}

\subsection{Finding Global Minima of Non-Convex Functions}

For non-convex functions, several strategies can help find the global minimum:

\begin{itemize}
    \item It \textit{may} be possible to find the global minimum of a non-convex function, but there are no guarantees.

    \item Consider a surface with two deep valleys where one is slightly lower. Depending on your starting point, gradient-based methods could converge to either valley, and both would appear to be good solutions.

    \item \textbf{Random restarts}: Run your (local) optimisation algorithm multiple times from different starting points, record the objective value at each minimum, then select the lowest as an estimate of the global minimum.

    \item Under certain weaker assumptions (e.g., that the objective is smooth), random restarts with sufficiently many trials can provide probabilistic guarantees of approaching the global minimum.
\end{itemize}

\subsection{OLS as a Concrete Example}

Let us examine Ordinary Least Squares to see convexity in action.

\subsubsection{OLS Loss Function}
\[
\mathcal{L}(\beta) = \frac{1}{2}(X\beta - y)^T(X\beta - y)
\]

Expanding:
\[\mathcal{L}(\beta) = \frac{1}{2} (\beta^T X^T X\beta - \beta^T X^T y - y^T X\beta + y^T y)\]

Since $(X\beta - y)^T = \beta^T X^T - y^T$, the middle terms $\beta^T X^T y$ and $y^T X\beta$ are equal scalars and can be combined.

\subsubsection{Gradient of OLS Loss}
\[
\nabla\mathcal{L}(\beta) = X^T X\beta - X^T y
\]

(We drop constant factors that do not affect the location of minima.)

\subsubsection{Hessian of OLS Loss}
\[
\nabla^2\mathcal{L}(\beta) = X^T X
\]

This is the covariance-like matrix of the features.

\subsubsection{Convexity of OLS}

For a minimum, we need the Hessian to be positive semidefinite. For OLS:

\textbf{If $X$ has full column rank, then $X^T X$ is positive semidefinite} (in fact, positive definite).

\textit{Proof sketch:} For any vector $\mathbf{z}$, we have $\mathbf{z}^T X^T X \mathbf{z} = \|X\mathbf{z}\|^2 \geq 0$, with equality only when $X\mathbf{z} = \mathbf{0}$. If $X$ has full column rank, then $X\mathbf{z} = \mathbf{0}$ implies $\mathbf{z} = \mathbf{0}$, so $\mathbf{z}^T X^T X \mathbf{z} > 0$ for all $\mathbf{z} \neq \mathbf{0}$.

Therefore, given that $X$ is full rank, \textbf{OLS is a convex problem}; any local minimum is the global minimum.

\begin{keybox}[Why We Like OLS]
With certain assumptions in place (full column rank of $X$), OLS provides a convex loss function. Finding the minimum through gradient descent (or analytically) guarantees we have found the unique global optimum.
\end{keybox}

\begin{tcolorbox}
\textbf{When $X$ is not of full rank:}

If there is \textbf{linear dependence} among columns, there may not be a unique global solution. Linear dependence means there exist combinations of columns leading to redundancy, allowing infinitely many solutions that minimise the loss function.

This scenario arises in \textbf{overfitting}: despite infinitely many models perfectly interpolating the training data, selecting the most appropriate model requires additional criteria or regularisation to ensure good generalisation.
\end{tcolorbox}

\begin{tcolorbox}
    \textbf{Summary: Convexity, PSD, and Minima}

    \textbf{Positive Semi-Definiteness and Convexity:} If a function's Hessian is PSD at every point, the function is convex. The function ``curves upward'' or is flat in all directions.

    \textbf{Convexity and Minima:} Convexity ensures any local minimum is global, eliminating isolated ``valleys'' common in non-convex functions.

    \textbf{Uniqueness:} For strictly convex functions (Hessian PD everywhere), there is exactly one unique global minimum. If the Hessian is merely PSD, the minimum may not be unique, but all minima form a convex set and are global.

    \textbf{Practical Implication:} Knowing a problem is convex simplifies optimisation - any minimum found is guaranteed to be global.
\end{tcolorbox}

%=====================================================
\section{First-Order Methods}
%=====================================================

\begin{redbox}[When to Use First-Order Methods]
    When closed-form solutions are unavailable, we turn to first-order optimisation methods. For instance:
    \begin{itemize}
        \item If $X$ is not of full rank, leading to an underdetermined system
        \item When the function is complex and difficult to evaluate directly (e.g., neural networks)
    \end{itemize}

    These \textit{numerical} methods, which rely primarily on gradient information, offer a way to iteratively search for a solution even without a straightforward \textit{analytical} solution.
\end{redbox}

First-order methods leverage only the first derivative (the \textbf{gradient}) to find minima. The gradient gives the direction of steepest ascent; by moving opposite to it, the algorithm seeks to reduce the function's value.

\subsection{General Procedure}

\begin{enumerate}
    \item \textbf{Start with an initial guess} ($\theta_0$): random or based on some heuristic.

    \item \textbf{Update your guess}:
    \[\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \eta_t \cdot \mathbf{d}_t\]
    where:
    \begin{itemize}
        \item $\eta_t$ = \textbf{step size / learning rate} (a positive scalar)
        \item $\mathbf{d}_t$ = \textbf{descent direction} (a vector indicating which way to update)
    \end{itemize}

    \item \textbf{Repeat until convergence} (many criteria exist: e.g., when the change in loss falls below a threshold, or when gradient magnitude is small).
\end{enumerate}

Note: We have decomposed the update step $\epsilon$ from the Taylor expansion into $\eta$ (magnitude) and $\mathbf{d}$ (direction).

\subsection{Descent Direction}

\subsubsection{Deriving the Descent Direction via Taylor Expansion}

Consider the Taylor expansion of $\mathcal{L}(\theta)$ at point $\theta$ with a small move $\eta \mathbf{d}$:
\begin{align*}
    \mathcal{L}(\theta + \eta \mathbf{d}) &\approx \mathcal{L}(\theta) + \nabla \mathcal{L}(\theta) \cdot (\eta \mathbf{d}) + \cdots\\
    &\approx \mathcal{L}(\theta) + \eta \cdot \nabla \mathcal{L}(\theta) \cdot \mathbf{d} + \cdots
\end{align*}

The change in $\mathcal{L}$ is primarily determined by $\eta \cdot \nabla \mathcal{L}(\theta) \cdot \mathbf{d}$, where:
\begin{itemize}
    \item $\eta$ is a scalar controlling step size (we want this large enough for progress)
    \item $\nabla \mathcal{L}(\theta) \cdot \mathbf{d}$ is a dot product determining direction (we want to minimise this)
\end{itemize}

\subsubsection{Optimal Direction}

To minimise the dot product $\nabla \mathcal{L}(\theta) \cdot \mathbf{d}$, we choose $\mathbf{d}$ opposite to $\nabla \mathcal{L}(\theta)$:

\begin{keybox}[Descent Direction]
\[\mathbf{d} = -\nabla\mathcal{L}(\theta)\]

The dot product is minimised when the two vectors point in opposite directions (the cosine of the angle is $-1$). Thus, choosing $\mathbf{d} = -\nabla \mathcal{L}(\theta)$ ensures we move in the direction that decreases $\mathcal{L}$ most steeply.
\end{keybox}

\subsubsection{Gradient Descent Update Rule}

\begin{keybox}[Gradient Descent]
\[\theta_{t+1} \leftarrow \theta_t - \eta_t \nabla \mathcal{L}(\theta_t)\]

By choosing direction $\mathbf{d} = -\nabla \mathcal{L}(\theta)$, gradient descent leverages local slope information to iteratively move towards the minimum.
\end{keybox}

\subsection{Step Size / Learning Rate}

Choosing the learning rate $\eta$ is more nuanced than choosing the direction.

\textbf{Too large $\Rightarrow$ failure to converge:}
\begin{itemize}
    \item Updates may overshoot the minimum, potentially causing divergence
    \item Even without divergence, oscillations around the minimum can prevent settling at the optimal point
\end{itemize}

\textbf{Too small $\Rightarrow$ slow convergence / getting stuck:}
\begin{itemize}
    \item Many more iterations required, becoming computationally expensive
    \item Risk of getting stuck in local minima or plateau regions (especially for non-convex problems)
\end{itemize}

\subsubsection{Example 1a: $f(x) = x^2$}

\begin{itemize}
    \item Convex function, so any local optimum is global
    \item Minimum at $x=0$
\end{itemize}

Running gradient descent from $x=4$ with learning rate $\eta=0.2$:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{iteration_1.png}
    \caption{Gradient descent on $f(x)=x^2$, iteration 1: starting at $x=4$ with $\eta=0.2$. The algorithm computes the gradient and takes a step towards the minimum.}
    \label{fig:gd-iter1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{iteration_4.png}
    \caption{Iteration 4: continued progress towards the minimum at $x=0$.}
    \label{fig:gd-iter4}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{iteration 10.png}
    \caption{Iteration 10: the algorithm has nearly converged to the minimum. The learning rate is small enough to avoid overshooting.}
    \label{fig:gd-iter10}
\end{figure}

\subsubsection{Example 1b: $f(x) = x^2$ with Large Learning Rate}

Same function, but with $\eta = 1.0$:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{iteration0b.png}
    \caption{Large learning rate ($\eta=1.0$), iteration 0: starting position.}
    \label{fig:gd-large-iter0}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{iteration1b.png}
    \caption{Iteration 1: the algorithm overshoots to the opposite side of the minimum.}
    \label{fig:gd-large-iter1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{iteration2b.png}
    \caption{Iteration 2: oscillating back. With $\eta=1.0$, the algorithm will never converge - it ping-pongs between $x=4$ and $x=-4$ indefinitely.}
    \label{fig:gd-large-iter2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{learning curve.png}
    \caption{Learning curves showing loss vs iteration. We want \textbf{smooth descent}. With a high learning rate (orange), the loss plateaus as the algorithm oscillates without making progress.}
    \label{fig:learning-curve}
\end{figure}

\subsubsection{Example 2a: Non-Convex Cubic}

Consider $f(x) = x + 2x^2 + 0.4x^3$:
\begin{itemize}
    \item Not convex
    \item Global minimum: $x = -\infty$ (where $f(-\infty) = -\infty$)
    \item But there is a local minimum
\end{itemize}

Running gradient descent from $x=2$ with $\eta = 0.25$:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{20.png}
    \caption{Cubic function, early iterations: starting at $x=2$.}
    \label{fig:cubic-1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{21.png}
    \caption{The algorithm descends into the local valley.}
    \label{fig:cubic-2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{image.png}
    \caption{Approaching the local minimum.}
    \label{fig:cubic-3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{23.png}
    \caption{Converged to local minimum. The algorithm does \textbf{not} find the global minimum (at $-\infty$) because the learning rate is too small to escape the local valley.}
    \label{fig:cubic-4}
\end{figure}

\subsubsection{Example 2b: Same Cubic, Different Starting Point}

Same function and learning rate, but starting at $x=-4$:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{2b0.png}
    \caption{Starting at $x=-4$: on the left side of the local minimum.}
    \label{fig:cubic-2b-1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{2b1.png}
    \caption{The gradient points left (towards $-\infty$), so the algorithm moves that direction.}
    \label{fig:cubic-2b-2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{2b2.png}
    \caption{The algorithm continues towards $-\infty$, finding the global minimum. With the same $\eta$, the starting point determined which minimum was found.}
    \label{fig:cubic-2b-3}
\end{figure}

\begin{keybox}[Non-Convexity and Initialisation]
For non-convex functions, the starting point critically determines what the algorithm converges to. This motivates \textbf{randomised restarts}: running gradient descent from multiple random starting points to increase the chance of finding the global minimum.
\end{keybox}

\subsection{Choosing a Constant Learning Rate}

If the gradient is sufficiently smooth - specifically, ``Lipschitz smooth'' with constant $L$:
\[
\left\| \nabla\mathcal{L}(\theta^*) - \nabla\mathcal{L}(\theta) \right\| \leq L \left\| \theta^* - \theta \right\|
\]

Then if we choose $\eta \leq \frac{2}{L}$, gradient descent is guaranteed to converge.

Intuitively:
\begin{itemize}
    \item \textbf{Smoother function} (smaller $L$) $\Rightarrow$ larger steps permissible
    \item \textbf{More rapidly varying function} (larger $L$) $\Rightarrow$ smaller steps required
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{smooth.png}
    \caption{Smooth vs non-smooth functions: smoother functions allow larger step sizes without overshooting. The Lipschitz constant $L$ quantifies this smoothness.}
    \label{fig:smoothness}
\end{figure}

\subsection{Choosing a Variable Learning Rate}

\subsubsection{Exact Line Search (Steepest Descent)}

At each iteration, find the step size that minimises the loss along the search direction:
\[\eta^* = \arg \min_{\eta>0} \mathcal{L}(\theta + \eta \mathbf{d})\]

For certain problem classes (e.g., quadratic functions), this has a closed-form solution:
\[\eta^* = - \frac{\mathbf{d}^T \textcolor{blue}{\nabla\mathcal{L}(\theta_t)}}{\mathbf{d}^T \textcolor{red}{\nabla^2 \mathcal{L}(\theta_t)} \mathbf{d}}\]

where:
\begin{itemize}
    \item \textcolor{blue}{$\nabla\mathcal{L}(\theta_t)$}: Gradient
    \item \textcolor{red}{$\nabla^2 \mathcal{L}(\theta_t)$}: Hessian
\end{itemize}

\begin{keybox}[Optimal Step Size Intuition]
The optimal step size is related to the \textbf{ratio of the gradient to the Hessian}:
\begin{itemize}
    \item The \textbf{gradient} provides direction and magnitude of steepest ascent
    \item The \textbf{Hessian} captures the curvature of the loss surface
\end{itemize}
This adapts the learning rate based on local curvature: larger steps in flatter regions, smaller steps in steeper/more curved regions.
\end{keybox}

For OLS specifically:
\[\eta^* = - \frac{\mathbf{d}^T (X^T X\beta - X^T y)}{\mathbf{d}^T X^T X \mathbf{d}}\]

\subsubsection{Inexact Line Search}

Exact line search can be computationally expensive. Inexact methods find a ``good enough'' step size.

\textbf{Armijo Backtracking:}
\begin{itemize}
    \item Start with a relatively large step size $\eta$
    \item Iteratively scale back by factor $c$ (where $0 < c < 1$)
    \item Stop when a sufficient decrease condition is met:
    \[\mathcal{L}(\theta + \eta \mathbf{d}) \leq \mathcal{L}(\theta) + c \cdot \eta \nabla \mathcal{L}(\theta)^T \mathbf{d}\]
\end{itemize}

This balances decrease in function value with computational efficiency.

\subsubsection{Convergence Rates and Condition Number}

\begin{tcolorbox}
The \textbf{condition number} $\kappa$ measures how the output can change for small input changes:
\[\kappa = \frac{\sigma_{\max}}{\sigma_{\min}}\]
where $\sigma_{\max}$ and $\sigma_{\min}$ are the maximum and minimum singular values of the Hessian.

The condition number indicates how ``round'' the loss surface bowl is:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{cond1000.png}
    \caption{High condition number ($\kappa = 1000$): the bowl is elongated, causing inefficient zigzagging paths. Low condition number ($\kappa \approx 1$): the bowl is circular, allowing direct paths to the minimum.}
    \label{fig:condition-number}
\end{figure}

\begin{itemize}
    \item \textbf{High $\kappa$}: ill-conditioned problem; different directions have vastly different curvatures, causing oscillations and slow convergence
    \item \textbf{Low $\kappa$}: well-conditioned; curvature is similar in all directions, leading to stable, fast convergence
\end{itemize}

For steepest descent on OLS, the \textbf{convergence rate} is:
\[\left(\frac{\kappa-1}{\kappa+1}\right)^2\]

This tells us the error reduction factor per iteration:
\begin{itemize}
    \item $\kappa \approx 1$: convergence rate $\approx 0$ (very fast convergence)
    \item $\kappa = 100$: convergence rate $\approx 0.96$ (slow - only 4\% error reduction per iteration)
\end{itemize}
\end{tcolorbox}

\subsection{Momentum}

Momentum incorporates \textbf{past gradient information} to inform the current update direction. Inspired by physical momentum: a ball rolling downhill accumulates speed.

Benefits:
\begin{enumerate}
    \item Helps navigate through \textbf{flat regions} of the loss landscape
    \item Overcomes \textbf{oscillations} in steep valleys
    \item Dampens sensitivity to noisy or temporarily misleading gradients
\end{enumerate}

\subsubsection{Momentum Update Rules}

\begin{enumerate}
    \item \textbf{Momentum accumulation (autoregressive):}
    \[m_t = \beta m_{t-1} + \nabla \mathcal{L}(\theta_{t-1})\]
    where $\beta \in [0,1)$ is the momentum coefficient. If $\beta = 0$, this reduces to pure gradient descent.

    \item \textbf{Parameter update:}
    \[\theta_t = \theta_{t-1} - \eta m_t\]
\end{enumerate}

\subsubsection{Understanding Momentum Accumulation}

Expanding the momentum term recursively:
\begin{align*}
    m_t &= \beta m_{t-1} + \nabla\mathcal{L}(\theta_{t-1})\\
    &= \beta^2 m_{t-2} + \beta \nabla \mathcal{L}(\theta_{t-2}) + \nabla \mathcal{L}(\theta_{t-1}) \\
    &= \sum_{i=0}^{t-1} \beta^i \nabla \mathcal{L}(\theta_{t-i-1})
\end{align*}

The momentum is an \textbf{exponentially weighted moving average} of past gradients, with recent gradients weighted more heavily.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{momentum.png}
    \caption{Momentum ``smooths out'' the optimisation path by averaging over past gradients, reducing oscillations and accelerating convergence in consistent directions.}
    \label{fig:momentum}
\end{figure}

\subsection{Nesterov Accelerated Momentum (NAG)}

NAG improves on standard momentum by \textbf{anticipating the future position} of parameters before computing the gradient.

With standard momentum, we compute the gradient at the current position then add momentum. This can lead to overshooting. NAG instead:
\begin{enumerate}
    \item \textbf{Look ahead:} $\hat{\theta}_{t+1} = \theta_t + \beta m_t$
    \item \textbf{Compute gradient at the anticipated position:}
    \[m_{t+1} = \beta m_t - \eta_t \nabla\mathcal{L}(\textcolor{red}{\theta_t + \beta m_t})\]
    The gradient is computed as if we had already taken the momentum step.
    \item \textbf{Update parameters:} $\theta_{t+1} = \theta_t + m_{t+1}$
\end{enumerate}

This incorporates curvature information from ``ahead,'' often leading to better convergence.

\subsection{Summary: First-Order Methods}

First-order methods use gradient information in various ways:
\begin{itemize}
    \item Gradients at \textbf{current} location (basic gradient descent)
    \begin{itemize}
        \item With \textbf{constant} learning rate
        \item With \textbf{variable} learning rate (exact or inexact line search)
    \end{itemize}
    \item Gradients at \textbf{past} locations (momentum)
    \item Gradients at \textbf{anticipated future} locations (Nesterov momentum)
\end{itemize}

\begin{tcolorbox}
    \textbf{Exact vs Inexact Line Search}

    \textbf{Exact Line Search:} Finds the step size that exactly minimises the objective along the search direction. Can be expensive for complex functions.

    \textbf{Inexact Line Search:} Finds a step size that sufficiently reduces the objective but does not minimise exactly. Generally less expensive as it only requires meeting certain criteria.
\end{tcolorbox}

But first-order methods use only gradient information. There is unused second-order (Hessian) structure that critically affects performance (recall condition numbers and optimal learning rates). This motivates second-order methods.

%=====================================================
\section{Second-Order Methods}
%=====================================================

\subsection{Newton's Method}

Newton's method uses the \textbf{Hessian} to account for curvature, dramatically speeding up convergence.

\begin{keybox}[Newton's Method]
\[\theta_{t+1} = \theta_t - H_t^{-1} \nabla \mathcal{L}(\theta_t)\]

Or with optional line search:
\[\theta_{t+1} = \theta_t - \eta_t (\nabla^2\mathcal{L}(\theta_t))^{-1} \nabla \mathcal{L}(\theta_t)\]

The Hessian informs both \textbf{direction} and \textbf{step size}.
\end{keybox}

Key properties:
\begin{itemize}
    \item Uses both first and second derivatives
    \item Faster convergence, especially near the optimum
    \item Performance contingent on ability to compute and invert the Hessian
\end{itemize}

\subsubsection{Algorithm}

\begin{enumerate}
    \item \textbf{Initialise} $\theta$ with an initial guess
    \item \textbf{Compute gradient} $\nabla \mathcal{L}(\theta_t)$
    \item \textbf{Compute Hessian} $\nabla^2\mathcal{L}(\theta_t)$
    \item \textbf{Solve for Newton direction} $\mathbf{d}_t$:
    \[\mathbf{d}_t = -(\nabla^2 \mathcal{L}(\theta_t))^{-1} \nabla \mathcal{L}(\theta_t)\]
    \item \textbf{Optional line search} for $\eta$
    \item \textbf{Update:} $\theta_{t+1} = \theta_t + \eta \mathbf{d}_t$
    \item \textbf{Repeat} until convergence
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{newton1.png}
    \caption{Newton's method iteration 1: the quadratic approximation (dashed) guides the step towards the minimum more directly than gradient descent.}
    \label{fig:newton1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{newton2.png}
    \caption{Newton's method iteration 2: for quadratic functions, Newton's method finds the exact minimum in one step.}
    \label{fig:newton2}
\end{figure}

\subsubsection{Derivation via Taylor Series}

\textbf{Step 1: Taylor expansion including Hessian}

\[\mathcal{L}(\theta_t + \epsilon) \approx \mathcal{L}(\theta_t) + \nabla \mathcal{L}(\theta_t) \cdot \epsilon + \frac{1}{2} \epsilon^T \nabla^2\mathcal{L}(\theta_t) \epsilon\]

Here $\epsilon$ is the step vector from $\theta_t$ towards the minimum (not an error term).

\textbf{Step 2: Minimise the quadratic approximation}

Take the gradient of the approximation with respect to $\epsilon$:
\[\nabla_\epsilon \mathcal{L}_{\text{approx}} = \nabla \mathcal{L}(\theta_t) + \nabla^2 \mathcal{L}(\theta_t) \epsilon\]

Set to zero:
\[\mathbf{0} = \nabla \mathcal{L}(\theta_t) + \nabla^2 \mathcal{L}(\theta_t) \epsilon\]

\textbf{Step 3: Solve for $\epsilon$}

\[\epsilon = -(\nabla^2 \mathcal{L}(\theta_t))^{-1} \nabla \mathcal{L}(\theta_t)\]

This gives the Newton update rule:
\begin{keybox}[Newton Update]
\[\theta_{t+1} = \theta_t - (\nabla^2 \mathcal{L}(\theta_t))^{-1} \nabla \mathcal{L}(\theta_t)\]
\end{keybox}

The Hessian informs both direction and step size by approximating how the gradient changes as parameters adjust.

\subsubsection{Newton's Method Applied to OLS}

For OLS with loss $\mathcal{L}(\beta) = \|X\beta - y\|^2$:
\begin{itemize}
    \item Gradient: $\nabla \mathcal{L}(\beta) = 2X^T(X\beta - y)$
    \item Hessian: $\nabla^2 \mathcal{L}(\beta) = 2X^T X$ (constant - no $\beta$ dependence)
\end{itemize}

Since the Hessian is constant, Newton's method converges in \textbf{one step}:

\begin{align*}
    \beta_1 &= \beta_0 - (X^T X)^{-1}(X^T X\beta_0 - X^T y)\\
    &= \beta_0 - (X^T X)^{-1} X^T X\beta_0 + (X^T X)^{-1} X^T y\\
    &= \beta_0 - \beta_0 + (X^T X)^{-1} X^T y\\
    &= (X^T X)^{-1} X^T y
\end{align*}

\begin{keybox}[Newton's Method and OLS]
\[\beta_1 = (X^T X)^{-1} X^T y\]

This is the familiar OLS closed-form solution! For OLS, Newton's method finds the exact solution in one iteration, regardless of starting point.
\end{keybox}

\subsubsection{Difficulties with Newton's Method}

While efficient for simple problems like OLS, Newton's method faces challenges:
\begin{itemize}
    \item \textbf{Computational cost:} Computing and inverting the Hessian is $O(p^3)$ for $p$ parameters
    \item \textbf{Memory requirements:} Storing the $p \times p$ Hessian may be impractical for large models
    \item \textbf{Numerical stability:} Inversion is unstable if the Hessian is nearly singular (multicollinearity, more features than observations)
\end{itemize}

These difficulties motivate quasi-Newton methods.

\subsection{BFGS (A Quasi-Newton Method)}

The Broyden--Fletcher--Goldfarb--Shanno (BFGS) algorithm approximates the Hessian iteratively rather than computing it directly.

\begin{keybox}[BFGS Key Idea]
Rather than compute $H$ exactly, build up approximations as optimisation proceeds. We know from gradient descent that the Hessian is not strictly necessary for convergence - it just improves efficiency. A rough approximation is often good enough.
\end{keybox}

\subsubsection{Algorithm}

\textbf{Initialisation:} $B_0 = I_p$ (identity matrix - assumes the surface is initially ``round'')

\textbf{BFGS Update:}
\[B_{t+1} = B_t + \frac{y_t y_t^T}{y_t^T s_t} - \frac{(B_t s_t)(B_t s_t)^T}{s_t^T B_t s_t}\]

where:
\begin{itemize}
    \item $y_t = \nabla \mathcal{L}(\theta_t) - \nabla \mathcal{L}(\theta_{t-1})$: gradient difference
    \item $s_t = \theta_t - \theta_{t-1}$: parameter difference
\end{itemize}

This update ensures $B_{t+1} s_t = y_t$ (the secant equation), meaning the approximate Hessian correctly predicts the observed gradient change.

\subsubsection{Technical Conditions}

For BFGS to work well:
\begin{enumerate}
    \item Line search should satisfy Wolfe conditions (ensuring sufficient decrease and positive definiteness of the approximation)
    \item The function should be twice continuously differentiable with positive definite Hessian near the optimum
\end{enumerate}

Under these conditions, BFGS approximates $\nabla^2 \mathcal{L}(\theta)$ well enough for efficient convergence.

\subsubsection{Efficiency Improvements}

\begin{itemize}
    \item \textbf{Limited-memory BFGS (L-BFGS):} Stores only a limited number of recent updates, reducing memory from $O(p^2)$ to $O(mp)$ where $m$ is small (typically 5--20). Suitable for high-dimensional problems.
    \item \textbf{Scaling:} Adjusting the initial approximation $B_0$ based on early iterations can improve convergence.
\end{itemize}

\begin{tcolorbox}
    For problems small enough to hold in memory, BFGS should be the default optimisation algorithm. It provides near-Newton efficiency without requiring explicit Hessian computation.
\end{tcolorbox}

\subsection{Summary: Second-Order Methods}

\textbf{Second-order methods exploit curvature information:}
\begin{itemize}
    \item \textbf{Curvature information} allows intelligent step size and direction adjustments
    \item \textbf{Adaptive steps:} larger steps in flat regions, smaller steps in curved regions - more direct paths to the minimum
\end{itemize}

\begin{redbox}[Why Line Search is Still Sometimes Used]
Even though second-order methods inherently inform step size via curvature, line search can still be beneficial:
\begin{itemize}
    \item The quadratic approximation may be inaccurate far from the optimum
    \item Line search provides a safety mechanism against poor steps
    \item For non-convex functions, the Hessian may not be positive definite everywhere
\end{itemize}
\end{redbox}

\textbf{Better approximation to the loss function:}
\begin{itemize}
    \item The Taylor series to second order gives a \textbf{quadratic approximation}
    \item This is usually closer to the true loss than a linear (first-order) approximation
    \item Enables more accurate prediction of how a step will affect the loss
\end{itemize}

\textbf{Faster convergence:}
\begin{itemize}
    \item Fewer iterations due to better step selection
    \item Better at navigating plateaus and saddle points
\end{itemize}

\textbf{Trade-offs:}
\begin{itemize}
    \item Computational cost of Hessian computation/inversion
    \item Memory requirements for storing the Hessian
    \item Quasi-Newton methods (BFGS) provide a practical middle ground
\end{itemize}

\begin{tcolorbox}
    \textbf{When to Use What}

    \textbf{Pure Newton:} Excellent when Hessian is cheap to compute and invert (small problems, constant Hessian like OLS)

    \textbf{BFGS:} General-purpose for medium-sized problems; benefits of second-order without explicit Hessian

    \textbf{First-order (gradient descent):} When problems are too large for second-order methods, or when only gradients are available (e.g., neural networks trained with backpropagation)
\end{tcolorbox}

%=====================================================
\section{Stochastic Gradient Descent (SGD)}
%=====================================================

\begin{redbox}[Why Return to First-Order?]
After discussing the advantages of second-order methods, we return to first-order methods. Why? Because with small data batches, we cannot estimate $H$ well enough. The noise in stochastic gradient estimates makes second-order information unreliable.
\end{redbox}

\begin{tcolorbox}
    \textbf{Motivation for SGD}

    SGD enables rapid updates to model parameters, facilitating faster convergence than methods requiring the entire dataset for each update. It is the foundation of neural network training.

    By updating parameters based on gradients from data subsets, SGD handles large-scale datasets effectively and provides a practical approximation to minimising the true (population) risk.
\end{tcolorbox}

\subsection{The Structure in Our Loss Functions}

SGD exploits the fact that empirical risk is a \textbf{sum over individual losses}:

\[\mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \ell(y_i, f(x_i; \theta))\]

Therefore, the gradient is also a sum:
\[\nabla \mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta} \ell(y_i, f(x_i; \theta))\]

\subsection{Relationship to Population Risk}

Empirical risk approximates population risk:
\[\mathbb{E}[\mathcal{L}(\theta)] = \int \ell(y, f(x;\theta)) p(x,y) \, dx \, dy\]

Key insight: if $(x_i, y_i)$ are i.i.d.\ samples from $p(x,y)$, then:
\[\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n \nabla_\theta \ell(y_i, f(x_i; \theta))\right] = \mathbb{E}\left[\frac{1}{B} \sum_{b=1}^B \nabla_\theta \ell(y_b, f(x_b; \theta))\right]\]

for any batch size $B \leq n$. The expected value of a mini-batch gradient equals the true gradient!

\begin{keybox}[SGD Intuition]
Stochastic gradient descent uses a subset of data to estimate the gradient. On average, this estimate equals the population gradient - so on average, SGD moves in the right direction.
\end{keybox}

\subsection{SGD Algorithm}

\begin{enumerate}
    \item Take a small sub-sample (mini-batch) of size $B$ from the data
    \item \textbf{Estimate gradient on that sub-sample:}
    \[\nabla \mathcal{L}_B(\theta) = \frac{1}{B} \sum_{b=1}^{B} \nabla_{\theta} \ell(y_b, f(x_b; \theta))\]
    \item \textbf{Update model parameters:}
    \[\theta_{t+1} = \theta_t - \eta_t \nabla \mathcal{L}_B(\theta_t)\]
    \item \textbf{Repeat until convergence}
\end{enumerate}

\subsection{Practical Considerations}

\textbf{Choice of batch size $B$:}
\begin{itemize}
    \item Small batches: more updates per epoch, higher variance in gradient estimates
    \item Large batches: fewer updates, lower variance, but higher memory usage
    \item Optimal choice is empirical and problem-dependent
\end{itemize}

\textbf{Sampling:} Typically sample without replacement within an epoch. After all samples are used, shuffle and repeat.

\textbf{Epochs:} One epoch = one pass over the entire dataset. Training usually involves multiple epochs.

\begin{redbox}[SGD Hyperparameters]
\begin{enumerate}
    \item Number of epochs
    \item Mini-batch size $B$
    \item Learning rate $\eta$ (and schedule)
\end{enumerate}
These require tuning for each problem.
\end{redbox}

\subsection{SGD Applied to OLS}

For OLS, the gradient for a single observation $(x, y)$ is:
\[\nabla \ell(y, x\beta) = x^T(x\beta - y)\]

With batch size $B=1$ (online/incremental learning):
\[\beta_{t+1} = \beta_t - \eta_t \cdot x_t^T (x_t \beta_t - y_t)\]

This is the \textbf{Least Mean Squares (LMS)} algorithm, a classic adaptive filter.

\begin{keybox}[Why ``Least Mean Squares''?]
It is called Least \textit{Mean} Squares because although individual steps may not move in the optimal direction (due to noise from using single samples), \textit{on average} the algorithm moves towards the minimum.
\end{keybox}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{LMS.png}
    \caption{LMS/SGD path: unlike gradient descent, individual steps may not always decrease the loss, but on average the algorithm progresses towards the minimum.}
    \label{fig:lms}
\end{figure}

\subsection{Choosing a Learning Rate for SGD}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{learningrate.png}
    \caption{Learning rate effects: too large causes oscillation/divergence; too small causes slow convergence or getting stuck.}
    \label{fig:lr-sgd}
\end{figure}

\subsubsection{Learning Rate Schedules}

Adjust $\eta$ over time: start high for rapid progress, decrease for fine-tuning near convergence.

Common schedules:
\begin{itemize}
    \item \textbf{Time-based decay:} $\eta_t = \eta_0 / (1 + kt)$ or $\eta_t = \eta_0 e^{-kt}$
    \item \textbf{Step decay:} Decrease by a factor at specified intervals
    \item \textbf{Performance-based:} Decrease when improvement stalls
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{learningschedules.png}
    \caption{Learning rate schedules: various strategies for decreasing the learning rate over training iterations.}
    \label{fig:lr-schedules}
\end{figure}

\subsubsection{Robbins--Monro Conditions}

For guaranteed convergence of stochastic approximation:

\begin{rigour}[Robbins--Monro Conditions]
\begin{enumerate}
    \item Learning rate approaches zero:
    \[\lim_{t \to \infty} \eta_t = 0\]

    \item Learning rates are square-summable but not summable:
    \[\sum_{t=1}^{\infty} \eta_t = \infty \quad \text{and} \quad \sum_{t=1}^{\infty} \eta_t^2 < \infty\]
\end{enumerate}

Equivalently: $\frac{\sum_t \eta_t^2}{\sum_t \eta_t} \to 0$
\end{rigour}

Interpretation:
\begin{itemize}
    \item Learning rate must decrease to zero for convergence
    \item But not too quickly - must explore the parameter space sufficiently ($\sum \eta_t = \infty$)
    \item Sum of squares converging controls variance and ensures stability
\end{itemize}

A common choice satisfying these conditions: $\eta_t = c/t$ for some constant $c$.

\subsection{Advanced SGD Techniques}

\textbf{Iterate Averaging / Stochastic Weight Averaging:}
\begin{itemize}
    \item Average parameters from multiple iterations to reduce variance
    \item Stochastic Weight Averaging (SWA) averages over the later training phase, often finding wider optima with better generalisation
\end{itemize}

\textbf{Incorporating Second-Order Information:}
\begin{itemize}
    \item Estimate the Hessian on larger batches, then use it
    \item In practice, noise in stochastic gradients makes this less effective
    \item Computing/inverting Hessians remains expensive
\end{itemize}

\textbf{Preconditioning:}
Modify gradients to account for ill-conditioning and anisotropic loss surfaces.

\textbf{Adaptive Learning Rates:}
\begin{itemize}
    \item \textbf{AdaGrad:} Per-parameter learning rates based on historical gradient magnitudes; good for sparse data
    \item \textbf{RMSProp:} Uses exponential moving average of squared gradients; addresses AdaGrad's aggressive decay
    \item \textbf{Adam:} Combines momentum (first moment) with adaptive rates (second moment); widely used default
\end{itemize}

%=====================================================
\section{Summary}
%=====================================================

\begin{keybox}[Optimisation Methods Summary]
\textbf{First-Order Methods (Gradient Descent, SGD):}
\begin{itemize}
    \item Simple and easy to implement
    \item May be slower due to using only local gradient information
    \item Find solutions to OLS but require many iterations
    \item SGD enables training on large datasets
\end{itemize}

\textbf{Second-Order Methods (Newton, BFGS):}
\begin{itemize}
    \item Account for curvature: faster convergence
    \item Newton's method solves OLS in one iteration
    \item BFGS provides second-order benefits without explicit Hessian
    \item Computationally expensive for large problems
\end{itemize}
\end{keybox}

\begin{tcolorbox}
    \textbf{Further Resources}

    For visualisations of optimisation algorithms in action, see: \url{https://github.com/jiupinjia/Visualize-Optimization-Algorithms}
\end{tcolorbox}

