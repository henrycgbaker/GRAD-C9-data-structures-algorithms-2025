

\thispagestyle{empty}
\begin{tabular}{p{15.5cm}}
{\large \bf DS\&A Lecture Notes 2024 \\ Henry Baker} \\
\hline
\\
\end{tabular}

\vspace*{0.3cm}
\begin{center}
	{\Large \bf Data Structures \& Algorithms: Wk 6\\ More Algorithm Analysis}
	\vspace{2mm}

\end{center}
\vspace{0.4cm}

This week builds on the foundations of Big O notation from Week 5, presenting formal properties of asymptotic analysis and applying them through four detailed case studies. The key pedagogical goal is to develop the skill of \emph{algorithmic thinking}: recognising structure in problems and exploiting it to improve efficiency.

\section{Properties of Big O}

Recall from Week 5 that Big O notation captures the upper bound on an algorithm's growth rate. These properties allow us to manipulate and combine Big O expressions algebraically, which is essential for analysing complex algorithms built from simpler components.

\begin{rigour}[Formal Properties of Big O Notation]
Let $f$, $f_1$, $f_2$, $g$, $g_1$, $g_2$, and $h$ be functions from $\mathbb{N} \to \mathbb{R}^+$. The following properties hold:

\textbf{1. Reflexivity:} $f$ is $O(f)$.

\textit{Every function is an upper bound for itself. Formally, we can choose $c = 1$ and any $n_0 \geq 1$ in the Big O definition.}

\textbf{2. Constants are absorbed:} If $f$ is $O(g)$ and $c > 0$, then $cf$ is $O(g)$.

\textit{Multiplying a function by a positive constant doesn't change its complexity class. This formalises why we ``drop constants'' in Big O analysis. If $f(n) \leq c_1 \cdot g(n)$ for large $n$, then $cf(n) \leq (c \cdot c_1) \cdot g(n)$, which is still $O(g)$.}

\textbf{3. Products combine multiplicatively:} If $f_1$ is $O(g_1)$ and $f_2$ is $O(g_2)$, then $f_1 \cdot f_2$ is $O(g_1 \cdot g_2)$.

\textit{This is crucial for analysing nested loops: if an outer loop runs $O(n)$ times and each iteration does $O(n)$ work, the total is $O(n \cdot n) = O(n^2)$.}

\textbf{4. Sums take the maximum:} If $f_1$ is $O(g_1)$ and $f_2$ is $O(g_2)$, then $f_1 + f_2$ is $O(\max\{g_1, g_2\})$.

\textit{When adding complexities, only the dominant term matters. This is why $O(n^2 + n) = O(n^2)$-the quadratic term dominates.}

\textbf{5. Transitivity:} If $f$ is $O(g)$ and $g$ is $O(h)$, then $f$ is $O(h)$.

\textit{Upper bounds compose: if $f$ grows no faster than $g$, and $g$ grows no faster than $h$, then $f$ grows no faster than $h$. This allows us to chain bounds through intermediate functions.}
\end{rigour}

These properties are not just theoretical-they are the tools we use repeatedly when analysing algorithms. The product rule handles nested structures; the sum rule handles sequential code blocks; transitivity lets us substitute bounds we've already established.


\section{Algorithm Analysis Guidelines}

Before diving into case studies, here is a general framework for improving algorithms:

\begin{keybox}[The Iterative Improvement Process]
\begin{enumerate}
    \item \textbf{Start simple:} Implement the naive brute-force solution first
    \item \textbf{Analyse weaknesses:} Identify \emph{why} it is inefficient-what work is being repeated? What structure is being ignored?
    \item \textbf{Exploit structure:} Find properties of the problem that allow shortcuts
    \item \textbf{Iterate:} Repeat until performance meets requirements
\end{enumerate}
\end{keybox}

\textbf{Questions to guide analysis:}
\begin{itemize}
    \item What are the specifics of the problem? What constraints exist?
    \item How can I make the problem easier for myself?
    \item Is there exploitable structure? (The same patterns recur across many problems)
    \item What data do I actually need to keep? Can I work with a subset?
    \item How can I efficiently search through the solution space?
    \item Does the data need to be sorted? Can I create structure that makes later operations faster?
    \item Does this resemble a problem I've seen before with a known efficient solution?
\end{itemize}

The following four case studies demonstrate this process in action.


\section{Case Study 1: Fibonacci Sequence}

The Fibonacci sequence is defined recursively:
\[
F(n) = F(n-1) + F(n-2), \quad F(0) = 0, \quad F(1) = 1
\]

The first few terms are: $0, 1, 1, 2, 3, 5, 8, 13, 21, 34, \ldots$

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fibonacci.png}
    \caption{The Fibonacci sequence: each term is the sum of the two preceding terms. This simple recurrence relation appears throughout mathematics, computer science, and nature.}
    \label{fig:fibonacci-sequence}
\end{figure}

\subsection{Naive Approach: Direct Recursion}

The most straightforward implementation directly translates the mathematical definition into code:

\begin{algorithm}[H]
  \SetAlgoLined
  \SetKwFunction{FibOne}{fib1}

  \SetKwProg{Fn}{def}{\string:}{}
  \Fn{\FibOne{$n$}}{
    \If{$n \leq 1$}{
      \KwRet $n$\;
    }
    \KwRet \FibOne{$n - 2$} $+$ \FibOne{$n - 1$}\;
  }

  \caption{Naive Recursive Fibonacci}
\end{algorithm}

\textbf{How it works:}
\begin{itemize}
    \item Base cases: $F(0) = 0$ and $F(1) = 1$ are returned directly
    \item Recursive case: for $n \geq 2$, compute both predecessors and sum them
    \item This is a direct translation of the mathematical definition (see functional programming principles in Week 4)
\end{itemize}

\subsubsection{Analysing Recursion via Recursion Trees}

To understand the time complexity, we visualise the function calls as a \textbf{recursion tree}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{recursion tree.png}
    \caption{Recursion tree for computing $F(n)$. Each node represents a function call; children represent the recursive calls made. The tree grows exponentially wide, and the same values (e.g., $F(2)$, $F(3)$) are computed many times at different positions.}
    \label{fig:recursion-tree}
\end{figure}

\textbf{Analysing the tree structure:}
\begin{itemize}
    \item At level $i$ (counting from 0 at the root), there are up to $2^i$ nodes
    \item The tree has depth approximately $n$ (from level 0 to level $n-1$)
    \begin{itemize}
        \item Level 0: the initial call $F(n)$
        \item Level $n-1$: the deepest calls reaching the base cases
    \end{itemize}
    \item Total nodes: up to $2^n - 1$ (a complete binary tree)
    \item Each node does constant work: one addition, one comparison
\end{itemize}

\begin{rigour}[Complexity of Naive Fibonacci]
\textbf{Time complexity: $O(2^n)$}

The recursion tree has up to $2^n - 1$ nodes, each performing $O(1)$ work. Therefore, total time is $O(2^n)$-exponential in the input.

\textbf{Space complexity: $O(n)$}

Space is determined by the maximum depth of the call stack at any moment, which equals the tree's height. The deepest path has $n$ levels, so at most $n$ stack frames exist simultaneously. Each frame stores local variables and the return address-constant space per frame.
\end{rigour}

\begin{redbox}[The Hidden Inefficiency]
The fundamental problem with this approach is \textbf{redundant computation}: the same Fibonacci values are calculated many times.

For example, when computing $F(5)$:
\begin{itemize}
    \item $F(3)$ is computed twice
    \item $F(2)$ is computed three times
    \item $F(1)$ is computed five times
\end{itemize}

We are paying exponential time to compute values we already know. The recursive structure \emph{forgets} what it has already computed.
\end{redbox}


\subsection{Improvement 1: Memoisation (Caching Values)}

The insight: \textbf{store computed values so we only calculate each $F(k)$ once}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fib2.png}
    \caption{Memoisation visualised: instead of recomputing values, we store them in an array. Each Fibonacci number is computed exactly once and retrieved from the cache thereafter. The tree effectively collapses to a linear chain.}
    \label{fig:fib-memoisation}
\end{figure}

\begin{algorithm}[H]
  \SetAlgoLined
  \SetKwFunction{FibTwo}{fib2}

  \SetKwProg{Fn}{def}{\string:}{}
  \Fn{\FibTwo{$n$}}{
    \If{$n \leq 1$}{
      \KwRet $n$\;
    }
    $arr = [0] * (n + 1)$\;
    $arr[1] = 1$\;
    \For{$i$ in range $(2, n+1)$}{
      $arr[i] = arr[i - 2] + arr[i - 1]$\;
    }
    \KwRet $arr[n]$\;
  }

  \caption{Iterative Fibonacci with Memoisation}
\end{algorithm}

\textbf{How it works:}
\begin{itemize}
    \item Create an array of size $n+1$, initialised to zeros
    \item Set the base case: $arr[1] = 1$ (and $arr[0] = 0$ is already set)
    \item Iterate from $i = 2$ to $n$, computing each Fibonacci number from the previous two
    \item Each $F(k)$ is computed exactly once and stored for future reference
\end{itemize}

This approach is called \textbf{dynamic programming}-solving a problem by breaking it into overlapping subproblems and storing their solutions.

\begin{rigour}[Complexity of Memoised Fibonacci]
\textbf{Time complexity: $O(n)$}

The loop executes exactly $n - 1$ times. Each iteration performs:
\begin{itemize}
    \item Two array lookups: $O(1)$ each (random access)
    \item One addition: $O(1)$
    \item One array assignment: $O(1)$
\end{itemize}
Total: $O(n)$ iterations $\times$ $O(1)$ per iteration $= O(n)$.

\textbf{Space complexity: $O(n)$}

We allocate an array of size $n + 1$ to store all intermediate values.
\end{rigour}

We have reduced time complexity from exponential to linear-a dramatic improvement! For $n = 50$, the naive approach would require approximately $2^{50} \approx 10^{15}$ operations, while the memoised version needs only about 50.

\begin{redbox}[Room for Further Improvement]
We're storing the entire array of $n+1$ values, but look closely at the algorithm: to compute $F(i)$, we only ever access $F(i-1)$ and $F(i-2)$. We never look back further than two positions.

\textbf{We're using $O(n)$ space to store values we'll never need again.}
\end{redbox}


\subsection{Improvement 2: Constant Space}

Since we only need the last two values at any point, we can reduce space from $O(n)$ to $O(1)$:

\begin{algorithm}[H]
  \SetAlgoLined
  \SetKwFunction{FibThree}{fib3}

  \SetKwProg{Fn}{def}{\string:}{}
  \Fn{\FibThree{$n$}}{
    \If{$n \leq 1$}{
      \KwRet $n$\;
    }
    $a, b = 0, 1$\;
    \For{$i$ in range $(2, n+1$)}{
      $c = a + b$\;
      $a = b$\;
      $b = c$\;
    }
    \KwRet $b$\;
  }

  \caption{Space-Optimised Iterative Fibonacci}
\end{algorithm}

\textbf{How it works:}
\begin{itemize}
    \item Maintain only three variables: $a = F(i-2)$, $b = F(i-1)$, $c = F(i)$
    \item After computing $c = a + b$, shift the window: $a \leftarrow b$, $b \leftarrow c$
    \item At iteration $i$: $a$ holds $F(i-2)$, $b$ holds $F(i-1)$, and we compute $c = F(i)$
\end{itemize}

\begin{rigour}[Complexity of Space-Optimised Fibonacci]
\textbf{Time complexity: $O(n)$}

Same as before: one loop of $n - 1$ iterations, each doing $O(1)$ work.

\textbf{Space complexity: $O(1)$}

We use only three variables ($a$, $b$, $c$) regardless of input size-constant space.
\end{rigour}

\begin{keybox}[Fibonacci Optimisation Journey]
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Approach} & \textbf{Time} & \textbf{Space} \\
\midrule
Naive recursion & $O(2^n)$ & $O(n)$ \\
Memoisation & $O(n)$ & $O(n)$ \\
Space-optimised & $O(n)$ & $O(1)$ \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Key insights:}
\begin{enumerate}
    \item \textbf{Avoid redundant work:} Memoisation eliminates repeated computation
    \item \textbf{Keep only what you need:} Recognising that only the last two values matter reduces space from $O(n)$ to $O(1)$
\end{enumerate}

This pattern-identify repeated work, cache results, then minimise cache size-recurs throughout algorithm design.
\end{keybox}


\section{Case Study 2: Binary Search}

\textbf{Problem:} Given a \textbf{sorted} array and a target value, find the index of the target (or determine it's not present).

\textbf{Example:} Find element 33 in a sorted array.

\subsection{Naive Approach: Linear Search}

The simplest approach: examine every element until we find the target.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{array.png}
    \caption{A sorted array. Linear search would check each element from left to right until finding the target-potentially examining all $n$ elements.}
    \label{fig:sorted-array}
\end{figure}

\textbf{Analysis:} This takes $O(n)$ time in the worst case (target is last or absent). It completely ignores the fact that the array is sorted!

\begin{redbox}[Wasted Structure]
The array is \textbf{sorted}-this is valuable structure we're not exploiting. When we look at an element and it's too small, we know the target must be to the right. When it's too big, the target must be to the left.

Linear search ignores this, treating a sorted array no differently than an unsorted one.
\end{redbox}


\subsection{Binary Search: Divide and Conquer}

\textbf{Key insight:} Compare the target against the \emph{middle} element. This immediately eliminates half the search space.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{binary search.png}
    \caption{Binary search setup: identify the lowest index (\texttt{lo}), highest index (\texttt{hi}), and middle index (\texttt{mid}). We compare the target against the element at \texttt{mid}.}
    \label{fig:binary-search-setup}
\end{figure}

\textbf{The algorithm:} Compare the target against the middle element:
\begin{itemize}
    \item If the target is smaller $\rightarrow$ search the left half
    \item If the target is larger $\rightarrow$ search the right half
    \item If equal $\rightarrow$ found!
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{bin search 2.png}
    \caption{First comparison: target 33 is greater than middle element 27, so we eliminate the left half and search only the right portion.}
    \label{fig:bin-search-step1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{bin search 3.png}
    \caption{Second comparison: update \texttt{lo} to just past the old middle. The search space is now half its original size.}
    \label{fig:bin-search-step2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{bin search 4.png}
    \caption{Find the new middle of the reduced search space. Each iteration halves the remaining elements.}
    \label{fig:bin-search-step3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{bin search 5.png}
    \caption{Third comparison: target 33 is less than middle element 43, so we eliminate the right half and search only the left portion.}
    \label{fig:bin-search-step4}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{bin search 6.png}
    \caption{Final state: all three pointers (\texttt{lo}, \texttt{mid}, \texttt{hi}) converge on the same element. If this element equals the target, we've found it; otherwise, the target is not in the array.}
    \label{fig:bin-search-final}
\end{figure}

\subsubsection{Algorithm}

\begin{algorithm}[H]
  \SetAlgoLined
  \SetKwProg{Fn}{def}{\string:}{}
  \SetKwFunction{BinarySearch}{binarySearch}

  \Fn{\BinarySearch{$A, x$}}{
    $lo = 1$\;
    $hi = n$\;
    \While{$lo \leq hi$}{
      $mid = \text{floor}((lo + hi) / 2)$\;
      \eIf{$x < A[mid]$}{
        $hi = mid - 1$\;
      }{
        \eIf{$x > A[mid]$}{
          $lo = mid + 1$\;
        }{
          \KwRet $mid$\;
        }
      }
    }
    \KwRet $-1$\;
  }

  \caption{Binary Search Algorithm}
\end{algorithm}

\textbf{Parameters:}
\begin{itemize}
    \item $A$: the sorted array to search
    \item $x$: the target value
    \item Returns: index of $x$ in $A$, or $-1$ if not found
\end{itemize}

\subsubsection{Correctness}

\textbf{Loop invariant:} If $x$ is in $A$, then $x$ is in $A[lo:hi]$ (the current search range).

\textbf{Termination:}
\begin{itemize}
    \item The quantity $(hi - lo)$ strictly decreases each iteration (we always shrink the range)
    \item Eventually $hi < lo$, and the loop exits
\end{itemize}

\textbf{Correctness:}
\begin{itemize}
    \item If we find $A[mid] = x$, we return the correct index
    \item If the loop exits with $hi < lo$, the search space is empty, so $x \notin A$
\end{itemize}

\subsubsection{Efficiency}

\begin{rigour}[Binary Search Complexity]
\textbf{Time complexity: $O(\log n)$}

After $k$ iterations, the remaining search space has size at most:
\[
(hi - lo + 1) \leq \frac{n}{2^k}
\]

The algorithm terminates when this becomes less than 1, i.e., when:
\[
\frac{n}{2^k} < 1 \implies 2^k > n \implies k > \log_2 n
\]

Therefore, the maximum number of iterations is $\lfloor \log_2 n \rfloor + 1 = O(\log n)$.

Each iteration does $O(1)$ work (one comparison, one arithmetic operation, one array access).

\textbf{Space complexity: $O(1)$}

We only maintain a constant number of variables: \texttt{lo}, \texttt{hi}, \texttt{mid}.
\end{rigour}

\begin{keybox}[Binary Search: Key Result]
\textbf{Binary search achieves $O(\log n)$ time complexity}-dramatically faster than linear search's $O(n)$.

\textbf{The power of logarithms:} To search 1 billion elements:
\begin{itemize}
    \item Linear search: up to 1,000,000,000 comparisons
    \item Binary search: at most $\log_2(10^9) \approx 30$ comparisons
\end{itemize}

This is a factor of 33 million improvement!
\end{keybox}

\subsubsection{Applications of Binary Search}

Binary search is a \textbf{canonical building block} that appears throughout computer science:
\begin{itemize}
    \item \textbf{Dictionary lookup:} Finding a word in a physical dictionary (open to the middle, decide which half)
    \item \textbf{Debugging code:} Is the bug before or after line $K$? Repeat with binary elimination
    \item \textbf{Resource allocation:} Exponential backoff in networking-try a value, if too high/low, adjust
    \item \textbf{Database indexing:} B-trees use binary search within nodes
    \item \textbf{Game playing:} Twenty questions is essentially binary search over possibilities
\end{itemize}

\begin{redbox}[Binary Search Requires Sorted Data]
Binary search \textbf{only works on sorted data}. If the input is unsorted, you must either:
\begin{enumerate}
    \item Sort it first (typically $O(n \log n)$), then binary search
    \item Use linear search ($O(n)$)
\end{enumerate}

For a single search, linear search ($O(n)$) beats sort-then-binary-search ($O(n \log n + \log n)$). But for \emph{many} searches on the same data, sorting once and searching many times is far more efficient.
\end{redbox}


\section{Case Study 3: Sorting}

Sorting is one of the most fundamental algorithmic problems. Unlike the previous examples where we were \emph{exploiting} existing structure, here we are \emph{creating} structure: transforming an unordered list into an ordered one.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{merge1.png}
    \caption{The sorting problem: given an unordered array (top), produce a sorted array (bottom) with elements arranged from least to greatest.}
    \label{fig:sorting-problem}
\end{figure}

\subsection{Naive Approach: BogoSort}

The simplest conceivable ``algorithm'': guess and check.

\textbf{Algorithm concept:}
\begin{enumerate}
    \item Randomly permute the array
    \item Check if it's sorted
    \item If not, repeat
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{merge2.png}
    \caption{BogoSort: randomly shuffle the array and check if it's sorted. This is extremely unlikely to produce the correct order quickly.}
    \label{fig:bogosort}
\end{figure}

\textbf{Correctness:} There are exactly $n!$ possible permutations of $n$ elements. One of them is the sorted order. Eventually (with probability 1), we'll stumble upon it.

\begin{rigour}[BogoSort Complexity]
\textbf{Time complexity: $O(n!)$ expected}

\begin{itemize}
    \item There are $n!$ possible permutations
    \item Each random shuffle has probability $\frac{1}{n!}$ of being correct
    \item Expected number of shuffles: $n!$
    \item Checking if sorted takes $O(n)$ (one pass through the array)
    \item Total expected time: $O(n \cdot n!) = O(n!)$ (since $n!$ dominates)
\end{itemize}

\textbf{Space complexity: $O(1)$}

In-place shuffling requires only constant extra space.
\end{rigour}

\begin{redbox}[Factorial Time is Catastrophically Slow]
To appreciate how bad $O(n!)$ is, consider Stirling's approximation:
\[
n! \approx \sqrt{2\pi n} \left(\frac{n}{e}\right)^n
\]

This shows $n!$ is $O(n^n)$-combining \emph{both} polynomial and exponential growth!

For just $n = 20$ elements: $20! \approx 2.4 \times 10^{18}$ operations.

Even at 1 billion operations per second, this would take about 77 years. For $n = 25$, it would take longer than the age of the universe.
\end{redbox}

\textbf{Why is BogoSort so terrible?} It treats sorting as pure guesswork, ignoring all structure. It doesn't consider:
\begin{itemize}
    \item What properties a sorted list has
    \item How individual swaps might move us toward a solution
    \item How to systematically build up the correct order
\end{itemize}


\subsection{MergeSort: Divide and Conquer}

MergeSort exploits a key insight: \textbf{merging two sorted lists is fast ($O(n)$)}. The strategy is to recursively divide the problem until we have trivially sorted sublists (single elements), then merge them back together.

\begin{tcolorbox}
\subsubsection*{Background: Merging Sorted Lists is $O(n)$}

Given two sorted lists $A = [a_1, \ldots, a_n]$ and $B = [b_1, \ldots, b_m]$, we can merge them into a single sorted list in $O(n + m)$ time.

\begin{lstlisting}
Merge(A, B):
    i = 0, j = 0
    C = [0] * (n + m)
    while (i < n and j < m):
        if (A[i] <= B[j]):
            C[i+j] = A[i]
            i = i + 1
        else:
            C[i+j] = B[j]
            j = j + 1
    # Copy remaining elements from whichever list is not exhausted
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{merge list.png}
    \caption{Merging two sorted lists: maintain pointers to the current element of each list. At each step, compare the two current elements and append the smaller to the output. The key insight is that we only ever compare ``current'' elements-we never need to look back.}
    \label{fig:merge-operation}
\end{figure}

\textbf{Why it's $O(n + m)$:}
\begin{itemize}
    \item We process each element exactly once
    \item Each comparison moves one element to the output
    \item Each operation (comparison, copy, increment) is $O(1)$
\end{itemize}

\textbf{This is only possible because the lists are sorted!} If they weren't, we'd need to search for the minimum element each time, making it $O((n+m)^2)$.
\end{tcolorbox}

\subsubsection{MergeSort Algorithm}

MergeSort works in two phases:
\begin{enumerate}
    \item \textbf{Divide:} Recursively split the array into halves until reaching single-element arrays (which are trivially sorted)
    \item \textbf{Conquer:} Merge the sorted subarrays back together, building up larger sorted arrays
\end{enumerate}

\textbf{Step-by-step example:}

\begin{enumerate}
    \item \textbf{Initial split into pairs:} Compare adjacent pairs and order them.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge3.png}
        \caption{First pair: compare M and E. Since E $<$ M alphabetically, swap to get (E, M).}
        \label{fig:merge-step1}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge4.png}
        \caption{Result of first pair comparison: E and M are now in sorted order.}
        \label{fig:merge-step2}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge5.png}
        \caption{Second pair: compare R and G. Since G $<$ R, swap to get (G, R).}
        \label{fig:merge-step3}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge6.png}
        \caption{Result: both pairs in the first half are now sorted.}
        \label{fig:merge-step4}
    \end{figure}

    \item \textbf{Merge sorted pairs into sorted quadruples:}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge7.png}
        \caption{Merge (E, M) and (G, R) into (E, G, M, R). We only compare current elements of each list-this is why merging is efficient.}
        \label{fig:merge-step5}
    \end{figure}

    \item \textbf{Continue with the other half:}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge8.png}
        \caption{Sort the next pair: compare S and E to get (E, S).}
        \label{fig:merge-step6}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge 9.png}
        \caption{Sort another pair: compare O and R to get (O, R).}
        \label{fig:merge-step7}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge10.png}
        \caption{Merge (E, S) and (O, R) into (E, O, R, S).}
        \label{fig:merge-step8}
    \end{figure}

    \item \textbf{Merge sorted quadruples:}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge11.png}
        \caption{Merge (E, G, M, R) and (E, O, R, S) into (E, E, G, M, O, R, R, S)-the first half is now fully sorted.}
        \label{fig:merge-step9}
    \end{figure}

    \item \textbf{Process the second half similarly:}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge12.png}
        \caption{Begin sorting the second half of the original array.}
        \label{fig:merge-step10}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge13.png}
        \caption{Sort pairs in the second half.}
        \label{fig:merge-step11}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge14.png}
        \caption{Merge sorted pairs into quadruples.}
        \label{fig:merge-step12}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge15.png}
        \caption{Continue merging in the second half.}
        \label{fig:merge-step13}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{merge16.png}
        \caption{Merge quadruples in the second half.}
        \label{fig:merge-step14}
    \end{figure}

    \item \textbf{Continue building larger sorted sublists:}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{image.png}
        \caption{Both halves are now sorted. One final merge remains.}
        \label{fig:merge-step15}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{image17.png}
        \caption{Performing the final merge operation.}
        \label{fig:merge-step16}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{image18.png}
        \caption{Final result: the entire array is now sorted.}
        \label{fig:merge-step17}
    \end{figure}
\end{enumerate}

\subsubsection{Correctness}

\begin{itemize}
    \item \textbf{Base case:} Single-element arrays are trivially sorted
    \item \textbf{Inductive step:} If two subarrays are sorted, merging them produces a sorted result
    \item \textbf{Termination:} Each recursive call works on a strictly smaller array; eventually we reach single elements
\end{itemize}

\subsubsection{Efficiency}

\begin{rigour}[MergeSort Complexity]
\textbf{Time complexity: $O(n \log n)$}

\textbf{Analysis via recursion levels:}
\begin{itemize}
    \item At level $k$ (counting from 0 at the leaves), we have $2^k$ subarrays
    \item Each subarray at level $k$ has size $n / 2^k$
    \item Merging all subarrays at level $k$ processes all $n$ elements exactly once: $O(n)$ work per level
    \item Number of levels: $\log_2 n$ (we halve the problem size each time)
    \item Total work: $O(n) \times O(\log n) = O(n \log n)$
\end{itemize}

\textbf{Alternative view-recurrence relation:}
\[
T(n) = 2T(n/2) + O(n)
\]
(Two subproblems of half size, plus linear merge time.) This recurrence solves to $T(n) = O(n \log n)$.

\textbf{Space complexity: $O(n)$}

Merging requires a temporary array to store the merged result.
\end{rigour}

\begin{keybox}[Sorting Comparison]
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Algorithm} & \textbf{Time} & \textbf{Space} \\
\midrule
BogoSort & $O(n!)$ & $O(1)$ \\
MergeSort & $O(n \log n)$ & $O(n)$ \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Key insight:} MergeSort exploits the structure of sorted sublists. Rather than random guessing, it systematically builds up sorted portions and efficiently combines them.

\textbf{Note:} $O(n \log n)$ is asymptotically optimal for comparison-based sorting-no algorithm that compares elements pairwise can do better in the worst case. This is proven via information-theoretic arguments.
\end{keybox}


\section{Case Study 4: Sieve of Eratosthenes}

\textbf{Problem:} Find all prime numbers up to $n$.

A prime number $p > 1$ has no divisors other than 1 and itself.

\subsection{Naive Approach: Trial Division}

For each number $k$ from 2 to $n$, check if any number from 2 to $k-1$ divides it evenly.

\textbf{Analysis:}
\begin{itemize}
    \item For each of $n$ numbers, we perform up to $n$ divisions
    \item Total: $O(n^2)$ operations
\end{itemize}

\textbf{Slight improvement:} We only need to check divisors up to $\sqrt{k}$, since if $k = a \times b$ with $a \leq b$, then $a \leq \sqrt{k}$. This reduces to $O(n\sqrt{n})$, but we can do much better.


\subsection{Sieve of Eratosthenes}

The Sieve of Eratosthenes exploits the structure of prime numbers: \textbf{every composite number is a multiple of some smaller prime}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{polarcoord.png}
    \caption{Prime numbers plotted in polar coordinates with radius $= p$ and angle $= p$ radians. The emerging spiral patterns reveal deep structure in the distribution of primes. The Sieve of Eratosthenes exploits a different structural property: multiples of primes.}
    \label{fig:prime-structure}
\end{figure}

\subsubsection{Algorithm}

\begin{enumerate}
    \item Create a list of all integers from 2 to $n$, initially all unmarked (potentially prime)
    \item Start with the smallest unmarked number $p = 2$
    \item Mark all multiples of $p$ (starting from $p^2$) as composite
    \begin{itemize}
        \item We start at $p^2$ because smaller multiples ($2p, 3p, \ldots, (p-1)p$) were already marked by smaller primes
    \end{itemize}
    \item Find the next unmarked number; set it as the new $p$
    \item Repeat until $p > \sqrt{n}$ (all remaining unmarked numbers are prime)
\end{enumerate}

\textbf{Why only check up to $\sqrt{n}$?} If a number $m \leq n$ is composite and its smallest prime factor is $p$, then $p \leq \sqrt{m} \leq \sqrt{n}$. So all composites are marked by the time we've processed all primes up to $\sqrt{n}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{sieve.png}
    \caption{The Sieve of Eratosthenes in action (see \texttt{https://upload.wikimedia.org/wikipedia/commons/9/94/Animation\_Sieve\_of\_Eratosth.gif} for an animated version). Starting from 2, we mark all its multiples. Then find 3 (unmarked), mark its multiples. Then 5, 7, etc. Numbers remaining unmarked are prime.}
    \label{fig:sieve-animation}
\end{figure}

\textbf{Structural insight:} As $p$ increases, we mark fewer and fewer new numbers. Multiples of 2 eliminate half the candidates; multiples of 3 eliminate a third of what remains; and so on. We're leveraging the fact that the definition of primality encodes information about multiples.

\subsubsection{Correctness}

A number remains unmarked if and only if it has no prime factors less than itself-i.e., if and only if it is prime.

\begin{itemize}
    \item If $m$ is composite, it has a prime factor $p < m$, and $m$ is marked when we process $p$
    \item If $m$ is prime, no smaller prime divides it, so it is never marked
\end{itemize}

\subsubsection{Efficiency}

\begin{rigour}[Sieve of Eratosthenes Complexity]
\textbf{Time complexity: $O(n \log \log n)$}

This is a subtle analysis. The number of operations is:
\[
\sum_{p \leq n, \, p \text{ prime}} \frac{n}{p} = n \sum_{p \leq n} \frac{1}{p}
\]

By Mertens' theorem, $\sum_{p \leq n} \frac{1}{p} \approx \log \log n$, giving $O(n \log \log n)$.

For practical purposes, $\log \log n$ grows extremely slowly-for $n = 10^9$, $\log \log n \approx 3$. So the algorithm is effectively linear.

\textbf{Space complexity: $O(n)$}

We maintain a boolean array of size $n$.
\end{rigour}

\begin{keybox}[Sieve of Eratosthenes: Key Result]
The Sieve of Eratosthenes finds all primes up to $n$ in $O(n \log \log n)$ time-nearly linear.

\textbf{Key insight:} Rather than testing each number independently, we exploit the multiplicative structure of integers. Marking multiples is far cheaper than testing primality directly.

This is a paradigm shift: instead of asking ``is $m$ prime?'' for each $m$, we ask ``what numbers does the prime $p$ eliminate?''
\end{keybox}

\begin{redbox}[Complexity Notation]
The complexity $O(n \log \log n)$ is sometimes loosely stated as $O(n \log n)$ in introductory treatments. While technically $\log \log n = o(\log n)$, the difference is small for practical input sizes, so $O(n \log n)$ serves as a conservative upper bound.

For rigorous analysis, the $O(n \log \log n)$ bound is correct and follows from the distribution of prime numbers (specifically, the Prime Number Theorem and Mertens' second theorem).
\end{redbox}


\section{Summary: The Art of Algorithm Design}

The four case studies illustrate a common theme: \textbf{efficiency comes from exploiting structure}.

\begin{keybox}[Algorithm Design Principles]
\begin{enumerate}
    \item \textbf{Identify redundant work:} Are we computing the same thing multiple times? (Fibonacci)
    \item \textbf{Exploit ordering:} Does sorted data allow faster operations? (Binary search)
    \item \textbf{Create useful structure:} Can we organise data to make later operations cheaper? (MergeSort)
    \item \textbf{Use problem-specific properties:} What mathematical structure can we leverage? (Sieve)
\end{enumerate}

\textbf{The common pattern:}
\begin{center}
Naive approach $\xrightarrow{\text{identify weakness}}$ Insight about structure $\xrightarrow{\text{exploit it}}$ Efficient algorithm
\end{center}
\end{keybox}

\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Problem} & \textbf{Naive} & \textbf{Improved} & \textbf{Key Insight} \\
\midrule
Fibonacci & $O(2^n)$ & $O(n)$ & Cache computed values \\
Search & $O(n)$ & $O(\log n)$ & Halve search space each step \\
Sort & $O(n!)$ & $O(n \log n)$ & Merge sorted sublists \\
Primes & $O(n^2)$ & $O(n \log \log n)$ & Mark multiples, not test divisors \\
\bottomrule
\end{tabular}
\end{center}

Each improvement represents not just a faster algorithm, but a deeper understanding of the problem's structure.

